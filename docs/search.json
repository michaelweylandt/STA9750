[
  {
    "objectID": "tips.html",
    "href": "tips.html",
    "title": "STA 9750: Tips and Tricks",
    "section": "",
    "text": "When you are writing quarto documents in RStudio, do not use the “Visual” editor mode. It will seem tempting at first, but it will lead to difficult and hard to debug errors down the road. You should disable it in your RStudio settings (if possible) and include editor: source in the header of quarto documents to keep it from opening automatically.\n git supports a huge variety of programming workflow and was originally designed to allow large teams of programmers to work together. This flexibility leads to a few “sharp edges”, however.\nYou will have the best experience with git in this course if:\n\n You commit early and often. git commits are essentially free (40 bytes - less than a text!) and give you the ability to roll back changes if you get into a bad situation.\n You do all your work on a single computer. Keeping files and git synchronized across multiple machines is possible, but trickier. If, for whatever reason, you need to use multiple computers, reach out to course staff in office hours for advice.\nIn particular, even though it may be tempting to edit files directly through the GitHub web interface, resist! You will get into situations where your local copy and the copy on GitHub are out of sync and you will have issues pushing your work onto GitHub.\n(It is possible to fix this, but you’ll have to learn a lot more about how git works, far beyond what is expected of you in this course.)\n You git push at the end of every work session. This is the stage when things are most likely to go wrong, so you want to be pushing early and often, giving you the best chance to spot and fix an issues early.\n You work in a “regular” file system. Synchronization and back up tools like DropBox or OneDrive perform magic behind the scenes that doesn’t always play well with git. You don’t need to use these tools with git since git (used properly and in conjunction with GitHub) will give you automatic history and backups.\n You avoid anti-virus issues. If you are on a Windows machine, tell your anti-virus software to ignore the directory containing your work for this course. Both git and quarto create and delete lots of little files as they execute; certain anti-virus tools interpret this as evidence of a cyber attack and attempt to lock the file system, leading to a variety of subtle and hard to explain errors.\n You are careful about what files you add in git. You should generally only include your qmd files and the rendered outputs in the docs folder. The build_site script included in Mini-Project #00 is helpful, but not bullet-proof, here.\n\n Use the file names and directory structures specified in the course instructions. I use a variety of automated processing tools to coordinate this course and if you do not follow instructions closely, your assignments will not be processed. I will typically manually handle your submissions, but with a penalty applied to your grade. It is better for all parties if you avoid this.\nThe helper scripts below can be used to confirm whether your submissions are properly formatted.\nAvoid using file or directory names with spaces in them. Some software, particularly on Windows computers, does not handle spaces in directory or file names properly."
  },
  {
    "objectID": "tips.html#miscellaneous-rstudio-and-git-advice",
    "href": "tips.html#miscellaneous-rstudio-and-git-advice",
    "title": "STA 9750: Tips and Tricks",
    "section": "",
    "text": "When you are writing quarto documents in RStudio, do not use the “Visual” editor mode. It will seem tempting at first, but it will lead to difficult and hard to debug errors down the road. You should disable it in your RStudio settings (if possible) and include editor: source in the header of quarto documents to keep it from opening automatically.\n git supports a huge variety of programming workflow and was originally designed to allow large teams of programmers to work together. This flexibility leads to a few “sharp edges”, however.\nYou will have the best experience with git in this course if:\n\n You commit early and often. git commits are essentially free (40 bytes - less than a text!) and give you the ability to roll back changes if you get into a bad situation.\n You do all your work on a single computer. Keeping files and git synchronized across multiple machines is possible, but trickier. If, for whatever reason, you need to use multiple computers, reach out to course staff in office hours for advice.\nIn particular, even though it may be tempting to edit files directly through the GitHub web interface, resist! You will get into situations where your local copy and the copy on GitHub are out of sync and you will have issues pushing your work onto GitHub.\n(It is possible to fix this, but you’ll have to learn a lot more about how git works, far beyond what is expected of you in this course.)\n You git push at the end of every work session. This is the stage when things are most likely to go wrong, so you want to be pushing early and often, giving you the best chance to spot and fix an issues early.\n You work in a “regular” file system. Synchronization and back up tools like DropBox or OneDrive perform magic behind the scenes that doesn’t always play well with git. You don’t need to use these tools with git since git (used properly and in conjunction with GitHub) will give you automatic history and backups.\n You avoid anti-virus issues. If you are on a Windows machine, tell your anti-virus software to ignore the directory containing your work for this course. Both git and quarto create and delete lots of little files as they execute; certain anti-virus tools interpret this as evidence of a cyber attack and attempt to lock the file system, leading to a variety of subtle and hard to explain errors.\n You are careful about what files you add in git. You should generally only include your qmd files and the rendered outputs in the docs folder. The build_site script included in Mini-Project #00 is helpful, but not bullet-proof, here.\n\n Use the file names and directory structures specified in the course instructions. I use a variety of automated processing tools to coordinate this course and if you do not follow instructions closely, your assignments will not be processed. I will typically manually handle your submissions, but with a penalty applied to your grade. It is better for all parties if you avoid this.\nThe helper scripts below can be used to confirm whether your submissions are properly formatted.\nAvoid using file or directory names with spaces in them. Some software, particularly on Windows computers, does not handle spaces in directory or file names properly."
  },
  {
    "objectID": "tips.html#helper-scripts",
    "href": "tips.html#helper-scripts",
    "title": "STA 9750: Tips and Tricks",
    "section": "Helper Scripts",
    "text": "Helper Scripts\nThe following helper scripts can be used to help automate several students tasks in this course. To use any of them, run\n\nsource(\"https://michael-weylandt.com/STA9750/load_helpers.R\")\n\nand then run the name of the relevant script.\n\nMini-Project Submission\n\nSubmit Mini-Project via GitHub Issue\nThe following functon will create the GitHub issue necessary to submit a mini-project:\n\nmp_submission_create &lt;- function(N, github_id){\n  library(rvest)\n  library(glue)\n  library(tidyverse)\n  library(httr2)\n  library(gh)\n  \n  if(missing(N)){\n    N &lt;- menu(title=\"Which Mini-Project would you like to submit on GitHub?\", \n              choices=c(0, 1, 2, 3, 4))\n  }\n    \n  mp_url &lt;- glue(\"https://michael-weylandt.com/STA9750/miniprojects/mini0{N}.html\")\n  \n  mp_text &lt;- read_html(mp_url) |&gt; html_element(\"#submission-text\") |&gt; html_text()\n  \n  if(missing(github_id)){\n    github_id &lt;- readline(\"What is your GitHub ID? \")\n  }\n  \n  title &lt;- glue(\"{course_short} {github_id} MiniProject #0{N}\")\n  \n  body &lt;- mp_text |&gt; str_replace(\"&lt;GITHUB_ID&gt;\", github_id)\n  \n  r &lt;- request(\"https://github.com/\") |&gt;\n         req_url_path_append(\"michaelweylandt\") |&gt;\n         req_url_path_append(course_repo) |&gt;\n         req_url_path_append(\"issues/new\") |&gt;\n         req_url_query(title=title, \n                       body=body) \n  \n  browseURL(r$url)\n}\n\n\n\nVerify Mini-Project Submission\nThe following function will confirm that the submission issue is properly formatted.\n\nmp_submission_verify &lt;- function(N, github_id){\n  library(rvest)\n  library(glue)\n  library(tidyverse)\n  library(httr2)\n  library(gh)\n    \n  if(missing(N)){\n    N &lt;- menu(title=\"Which Mini-Project would you like to check was properly submitted on GitHub?\", \n              choices=c(0, 1, 2, 3, 4))\n  }\n  \n  mp_url &lt;- glue(\"https://michael-weylandt.com/STA9750/miniprojects/mini0{N}.html\")\n  \n  mp_text &lt;- read_html(mp_url) |&gt; html_element(\"#submission-text\") |&gt; html_text()\n  \n  if(missing(github_id)){\n    github_id &lt;- readline(\"What is your GitHub ID? \")\n  }\n  \n  title &lt;- glue(\"{course_short} {github_id} MiniProject #0{N}\")\n  \n  body &lt;- mp_text |&gt; str_replace(\"&lt;GITHUB_ID&gt;\", github_id) |&gt; str_squish()\n  \n  issues &lt;- gh(\"/repos/michaelweylandt/{repo}/issues?state=all\", \n               owner=github_id, \n               repo=course_repo)\n  \n  issue_names &lt;- vapply(issues, function(x) str_squish(x$title), \"\")\n  \n  name_match &lt;- which(issue_names == title)\n  \n  if(length(name_match) == 0){\n    cat(\"I could not find a unique issue with the title:\\n\", \n        \"    \", sQuote(title),\"\\n\",\n        \"The issues I found had the following titles:\\n\",\n        paste(c(\"\", issue_names), collapse=\"\\n - \"), \"\\n\",\n        \"If something on that list looks correct, please check\",\n        \"capitalization and punctuation.\\n\")\n      \n    case_insensitive_match &lt;- which(tolower(issue_names) == tolower(title))\n        \n    if(length(case_insensitive_match) &gt; 0){\n      cat(\"In particular, please check\", \n          sQuote(issue_names[case_insensitive_match]), \n          \"which appears to be the same, modulo capitalization.\\n\")\n    }\n         \n    stop(\"MINIPROJECT NOT SUBMITTED CORRECTLY.\")\n  } else if(length(name_match) &gt; 1){\n    cat(\"I found multiple issues with the title:\\n\", \n        \"    \", sQuote(title),\"\\n\",\n        \"Please change the names of issues so that there is only\", \n        \"issue with the desired name. (Note that closing an issue is\",\n        \"not sufficient.)\\n\")\n          \n    stop(\"MINIPROJECT NOT SUBMITTED CORRECTLY.\")\n  }\n  \n  issue &lt;- issues[[name_match]]\n  \n  issue_num &lt;- issue$number\n  issue_url &lt;- issue$html_url\n  issue_body &lt;- issue$body |&gt; str_squish()\n  \n  cat(\"Identified Issue #\", issue_num, \"at\", issue_url, \"as possible candidate.\\n\")\n  \n  if(issue$state != \"open\"){\n    cat(\"Issue does not appear to be in 'open' status. Please\",\n        \"confirm the issue is open and try again.\\n\")\n    stop(\"MINIPROJECT NOT SUBMITTED CORRECTLY.\")\n  }\n  \n  if(issue_body != body){\n    cat(\"Issue does not appear to have the correct body text. This is not\",\n        \"necessarily an issue, but you may want to confirm all relevant\",\n        \"information is included.\\n\")\n  }\n  \n  # This isn't quite general enough, but I think it covers everything\n  # we'll see in this course. \n  URL_REGEX &lt;- \"http[A-Za-z0-9:/\\\\-\\\\.]*\"\n    \n  urls &lt;- str_match_all(paste(issue_body, \"\\n Thanks!\\n\", sep=\"\"), \n                        URL_REGEX)[[1]]\n  expected_url &lt;- str_extract(body, URL_REGEX)\n  \n  if(length(urls) &gt; 1){\n    cat(\"The following URLs were found in the issue text:\",\n        paste(c(\"\", urls), collapse=\"\\n - \"), \n        \"Only one URL was expected; please remove any extras.\\n\")\n    stop(\"MINIPROJECT NOT SUBMITTED CORRECTLY.\")\n  }\n \n  submitted_url &lt;- urls[1]\n  \n  if(submitted_url != expected_url){\n    cat(\"The submitted URL does not match the Mini-Project instructions.\\n\", \n        \"Expected:\\n - \", expected_url, \"\\n\", \n        \"Submitted:\\n - \", submitted_url, \"\\n\", \n        \"Please correct any differences and try again.\\n\")\n        stop(\"MINIPROJECT NOT SUBMITTED CORRECTLY.\")\n  }\n  \n  resp &lt;- request(submitted_url) |&gt; \n      req_error(is_error = \\(r) FALSE) |&gt;\n      req_perform()\n  \n  if(resp_is_error(resp)){\n    cat(\"Something appears to be incorrect at the URL: \", \n        submitted_url, \n        \"\\n Please confirm that it is working as expected and try again.\")\n    browseURL(submitted_url)\n    stop(\"MINIPROJECT NOT SUBMITTED SUCCESSFULLY.\")\n  }\n  \n  if(N == 0){\n    raw_url &lt;- glue(\"https://raw.githubusercontent.com/{github_id}/{course_repo}/refs/heads/main/index.qmd\")\n  } else {\n    raw_url &lt;- glue(\"https://raw.githubusercontent.com/{github_id}/{course_repo}/refs/heads/main/mp0{N}.qmd\")\n  }\n  \n  resp_raw &lt;- request(raw_url) |&gt; \n      req_error(is_error = \\(r) FALSE) |&gt;\n      req_perform()\n  \n  if(resp_is_error(resp_raw)){\n      ## Try again with 'master' instead of 'main' branch\n      raw_url &lt;- str_replace(raw_url, \"main\", \"master\")\n      resp_raw &lt;- request(raw_url) |&gt; \n      req_error(is_error = \\(r) FALSE) |&gt;\n      req_perform()\n  }\n  \n  if(resp_is_error(resp_raw)){\n      cat(\"I cannot find the source qmd document at\", raw_url, \".\\n\",\n          \"Please confirm it was correctly submitted and try again.\\n\",\n          \"This document is needed for automated code quality checks.\\n\")\n      \n    stop(\"MINIPROJECT NOT SUBMITTED SUCCESSFULLY.\")\n  }\n  \n  cat(\"Congratulations! Your mini-project appears to have been submitted correctly!\\n\")\n  invisible(TRUE)\n}\n\n\n\n\nPeer Feedback\n\nCreate Peer Feedback Comment\nAs of January 2025, GitHub does not support pre-filling issue comments via URL so I am currently unable to pre-load the peer feedback template for you. For now, you should be able to copy the template on the mini-project overview page. (Note the “copy” button in the top right corner of the code-formatted block.)\n\n\nFind Peer Feedback Assigned to Me\n\nmp_feedback_locate &lt;- function(N, github_id){\n    library(rvest)\n    library(glue)\n    library(tidyverse)\n    library(httr2)\n    library(gh)\n    \n    if(missing(N)){\n        N &lt;- menu(title=\"Which Mini-Project's Peer Feedback cycle is it currently?\", \n                  choices=c(0, 1, 2, 3, 4))\n    }\n    \n    if(missing(github_id)){\n        github_id &lt;- readline(\"What is your GitHub ID? \")\n    }\n    \n    issues &lt;- gh(\"/repos/michaelweylandt/{repo}/issues?state=all\", \n                 repo=course_repo)\n    \n    pf_urls &lt;- issues |&gt; \n        keep(~ str_detect(.x$title, glue(\"#0{N}\"))) |&gt;\n        map(~data.frame(html=.x$html_url, comments=.x$comments_url)) |&gt;\n        keep(~any(str_detect(map_chr(gh(.x$comments), \"body\"), github_id))) |&gt;\n        map(\"html\") |&gt;\n        list_c()\n    \n    cat(glue(\"I have found several MP#0{N} issues that may be assigned to you.\\n\"),\n             \"Please review the following:\\n\")\n    for(pf in pf_urls){\n        cat(\" - \", pf, \"\\n\")\n    }\n    \n    to_browser &lt;- menu(title=\"Would you like me to open these in your web browser?\", \n                       choices=c(\"Yes\", \"No\"))\n    \n    if(to_browser == 1){\n        for(pf in pf_urls){\n            browseURL(pf)\n        }\n    }\n    invisible(TRUE)\n}\n\n\n\nVerify Peer Feedback Properly Formatted\n\nmp_feedback_verify &lt;- function(N, github_id, peer_id){\n  library(rvest)\n  library(glue)\n  library(tidyverse)\n  library(httr2)\n  library(gh)\n    \n  if(missing(N)){\n    N &lt;- menu(title=\"Which Mini-Project's Peer Feedback would you like to check was properly submitted on GitHub?\", \n              choices=c(0, 1, 2, 3, 4))\n  }\n  \n  if(missing(github_id)){\n    github_id &lt;- readline(\"What is your GitHub ID? \")\n  }\n\n  if(missing(peer_id)){\n    peer_id &lt;- readline(\"What is your Peer's GitHub ID? \")\n  }\n \n  template_url &lt;- \"https://michael-weylandt.com/STA9750/miniprojects.html\"\n  \n  template_text &lt;- read_html(template_url) |&gt; \n    html_element(\"#peer-feedback-template\") |&gt; \n    html_text() |&gt;\n    str_trim() |&gt;\n    str_replace_all(\"NN\", \"(.*)\") |&gt;\n    str_replace_all(\"TEXT TEXT TEXT\", \"(.*)\")\n  \n  title &lt;- glue(\"{course_short} {peer_id} MiniProject #0{N}\")\n\n  issues &lt;- gh(\"/repos/michaelweylandt/{repo}/issues?state=all\", \n               repo=course_repo)\n  \n  issue_names &lt;- vapply(issues, function(x) str_squish(x$title), \"\")\n  \n  name_match &lt;- which(issue_names == title)\n  \n  if(length(name_match) != 1){\n      cat(\"I could not find a unique issue with the title:\\n\", \n          \"    \", sQuote(title),\"\\n\",\n          \"I'm afraid I can't verify whether the peer feedback was submitted properly\\n\", \n          \"but the issue likely lies with the submittor, not with your feedback.\")\n          stop(\"PEER FEEDBACK NOT VERIFIED.\")\n  } \n  \n  issue &lt;- issues[[name_match]]\n  \n  issue_url &lt;- issue$html_url\n  issue_comment_url &lt;- issue$comments_url\n  \n  comments &lt;- gh(issue_comment_url)\n  \n  commenters &lt;- vapply(comments, function(x) x$user$login, \"\")\n  \n  comment_num &lt;- which(commenters == github_id)\n  \n  if(length(comment_num) != 1){\n    cat(\"I cannot identify your comment on the issue at\", \n       sQuote(issue_url), \". Please verify that this is the correct link.\\n\")\n       \n    if(length(comment_num) == 0){\n      cat(\"You do not appear to have commented on this issue.\")\n    } else {\n      cat(\"You have left multiple comments on this issue.\",\n          \"Please consolidate your feedback\")\n    }\n    stop(\"PEER FEEDBACK NOT VERIFIED.\")\n  }\n  \n  comment_body &lt;- comments[[comment_num]]$body\n  \n  cat(\"I have found your comment, with the following text:\\n-----\\n\")\n  \n  cat(comment_body)\n  \n  cat(\"\\n-----\\n\")\n  \n  comment_body  &lt;- comment_body |&gt; str_squish()\n  template_text &lt;- template_text |&gt; str_squish()\n  \n  matches &lt;- str_match_all(comment_body, template_text)[[1]][-1]\n  \n  if(anyNA(matches)){\n    cat(\"I couldn't match the template to your comment. Please modify and try again.\")\n    stop(\"PEER FEEDBACK NOT VERIFIED.\")\n  }\n  \n  cat(glue(\"Congratulations! Your peer feedback on {peer_id}'s MP #0{N} appears properly formatted.\\nThank you for your contributions to {course_short}!\\n\"))\n  \n  invisible(TRUE)\n  \n}\n\n\n\n\nCount Words\nThe following script can be used to count words on a Quarto-generated web page.\n\ncount_words &lt;- function(url){\n    library(rvest)\n    library(stringi)\n    \n    # Note that this includes code inside an inline block, but omits\n    # full-sized code blocks\n    read_html(url) |&gt;\n        html_elements(\"main p\") |&gt;\n        html_text() |&gt;\n        stri_count_words() |&gt;\n        sum()\n}\n\n\n\nLint Submission\nThe following will automatically run the lintr package on a submitted qmd document.\n\nlint_submission &lt;- function(N, peer_id){\n  library(rvest)\n  library(glue)\n  library(tidyverse)\n  library(httr2)\n  library(lintr)\n\n  if(missing(N)){\n    N &lt;- menu(title=\"Which Mini-Project submission would you like to lint?\", \n              choices=c(1, 2, 3, 4))\n  }\n    \n  if(missing(peer_id)){\n    peer_id &lt;- readline(\"What is your Peer's GitHub ID? \")\n  }\n    \n  raw_url &lt;- glue(\"https://raw.githubusercontent.com/{peer_id}/{course_repo}/refs/heads/main/mp0{N}.qmd\")\n  \n  cat(\"Attempting to access qmd source at\", raw_url, \".\\n\")\n  \n  resp &lt;- request(raw_url) |&gt; \n      req_error(is_error = \\(r) FALSE) |&gt;\n      req_perform()\n  \n  if(resp_is_error(resp)){\n      cat(\"I could not access the raw qmd document. Attempting to open in browser...\\n\")\n      browseURL(resp)\n      return(FALSE)\n  } else {\n      cat(\"I was able to access the raw qmd document. Beginning to lint.\\n\")\n      \n      tf &lt;- tempfile(pattern=glue(\"mp0{N}_{peer_id}_lint_document.qmd\"))\n      \n      writeLines(resp_body_string(resp), tf)\n      \n      lintr::lint(tf)\n  }\n}"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "STA 9750 - Final Project",
    "section": "",
    "text": "In lieu of exams, STA 9750 has an end-of-semester project, worth 40% of your final grade. This project is intended to showcase the data analysis technologies covered in this course, including - but not limited to:\nThe project will be graded out of a total 400 points, divided as follows:\nProjects can be completed in groups of 3-4 students.1 All group members are responsible for all portions of the work and will receive the same grade, except on the individual evaluation.\nGroup Membership: By 2025-03-05 at 11:45pm ET, email the instructor with a list of group members, cc-ing all group members. Once the original email is sent, other group members must reply acknowledging their intention to work with this group. After this date, group membership may only be changed for extraordinary circumstances."
  },
  {
    "objectID": "project.html#footnotes",
    "href": "project.html#footnotes",
    "title": "STA 9750 - Final Project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf desired, students can work in pairs or even individually. That “team” is still responsible for a minimum of three specific questions, so you will have to do extra work if you have a team of fewer than 3 people.↩︎\nMore properly, you would want to use Zip Code Tabulation Areas (ZCTAs) for this sort of analysis. The distinction is subtle, but while all ZCTAs have geographic extents, not all zip codes do. For example, there are dedicated zip codes for the IRS and the Department of Defense that have no associated geographic boundaries. Most open data sources will omit this distinction, but if you see it, you should be aware of it.↩︎\nIf students choose to take on multiple specific questions (perhaps because they were in a small group or if a classmate had to drop the course), they may submit multiple individual reports (one per question). If doing so, please modify the GitHub message template to link all reports.↩︎"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA 9750 - Course Syllabus",
    "section": "",
    "text": "All syllabus and course schedule provisions subject to change with suitable advance notice."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "STA 9750 - Course Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTheoretically, this may result in scores equivalent to an A in an un-curved course receiving a lower grade in this course. In practice, the instructor will design course assessments to induce a range of scores and does not anticipate “down-curving” happening.↩︎\nThough ubiquitous, email is a remarkably ‘flaky’ service, providing the sender no way to guarantee their message arrives untampered and providing the recipient no way to guarantee the providence of a message received. (This is not quite true: there are tools for more secure email but they are somewhat more difficult to use and are not supported at CUNY.) Brightspace is integrated with CUNY’s Identity Verification Services and allows students to guarantee correct submission. Note that Brightspace does not, by default, send students an email confirming submission, but I believe this is an option that can be enabled on the student’s end.↩︎\nFor this course, an average student is a student who enters the course with:\n\nBasic computer literacy, including use of the file system, plain text files and editors, etc.;\nA small amount of programming experience, not necessarily in R; and\nFluency with statistics and data analysis at the level of (at least) STA 9708, ideally STA 9700;\n\nand is earning a B-range grade. If you have less background or are aiming for a higher grade, you should expect to commit proportionally more time to this course.\nIf you lack the prerequisite background listed above or simply wish to review it before the semester begins in earnest, please reach out to the instructor and I will be more than happy to provide supplementary readings.↩︎\nThe CUNY Graduate Center has a useful summary of these expectations. Baruch courses follow the same standards. See also CUNY Central Policy.↩︎"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Table Tests",
    "section": "",
    "text": "radius &lt;- 5\n\n\nTest list-table\n\n\n\nRight\nLeft\nCentered\n\n\n\n\nBananas\n12 – 5\nbuilt-in wrapper\n\n\nNew Row\n\nItem\nItem"
  },
  {
    "objectID": "slides/slides12.html#mini-project-04",
    "href": "slides/slides12.html#mini-project-04",
    "title": "STA 9750 - Week 12",
    "section": "STA 9750 Mini-Project #04",
    "text": "STA 9750 Mini-Project #04\nMP#04 online now\n\nDue 2024-12-04 (\\(\\approx\\) 3 weeks - 2 remaining)\nTopic: financial modeling\n\nComparison of two retirement plans\nHistorical data + Monte Carlo (“bootstrapping”)\n\nFormat:\n\nDecision Analytics - Play the role of financial advisor\nGitHub post AND Brightspace submission"
  },
  {
    "objectID": "slides/slides12.html#week-12-pre-assignment",
    "href": "slides/slides12.html#week-12-pre-assignment",
    "title": "STA 9750 - Week 12",
    "section": "Week 12 Pre-Assignment",
    "text": "Week 12 Pre-Assignment\nDue at midnight tonight - take a moment to do it now if you haven’t already!"
  },
  {
    "objectID": "slides/slides12.html#pre-assignments",
    "href": "slides/slides12.html#pre-assignments",
    "title": "STA 9750 - Week 12",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext (and final!) pre-assignment is December 4th\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "slides/slides12.html#next-semester-topics",
    "href": "slides/slides12.html#next-semester-topics",
    "title": "STA 9750 - Week 12",
    "section": "Next Semester Topics",
    "text": "Next Semester Topics\n\nNYC Open Data\nSports (Baseball?)\nSpotify / Music\nHealthcare / Pharmaceutical (might be tricky…)\nVideo Games\nQuant Finance / Time Series?\nBaruch Demographics\nJob Market\nReal Estate"
  },
  {
    "objectID": "slides/slides12.html#grading",
    "href": "slides/slides12.html#grading",
    "title": "STA 9750 - Week 12",
    "section": "Grading",
    "text": "Grading\nReturned:\n\nMP#02 grade\nMP#02 meta-review grade\nVideos now on Vocat\n\nWe owe you:\n\nMid-Term Check-In Feedback"
  },
  {
    "objectID": "slides/slides12.html#grading---ex-post-adjustments",
    "href": "slides/slides12.html#grading---ex-post-adjustments",
    "title": "STA 9750 - Week 12",
    "section": "Grading - Ex Post Adjustments",
    "text": "Grading - Ex Post Adjustments\nFYI: At the end of the course, I curve individual peer grades.\nExample: If grader \\(X\\) is on average, 5 points lower, I re-center all their grades, raising the gradees by an average of 1.25.\nTry to be consistent over the semester so I can calibrate this correctly."
  },
  {
    "objectID": "slides/slides12.html#github-notifications",
    "href": "slides/slides12.html#github-notifications",
    "title": "STA 9750 - Week 12",
    "section": "GitHub Notifications",
    "text": "GitHub Notifications\nMake sure you check GitHub notifications, via email or at https://github.com/notifications to make sure you get all peer feedback assignments.\n\nI tag you in other folks’ repo when you are supposed to review\nPeople tagged in your repo are evaluating you"
  },
  {
    "objectID": "slides/slides12.html#course-support",
    "href": "slides/slides12.html#course-support",
    "title": "STA 9750 - Week 12",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(38\\) minute average response time)\n\n\nChange: MW Thursday Zoom OH now 4:00pm to 5:00pm"
  },
  {
    "objectID": "slides/slides12.html#upcoming",
    "href": "slides/slides12.html#upcoming",
    "title": "STA 9750 - Week 12",
    "section": "Upcoming",
    "text": "Upcoming\nNov 27 - Thanksgiving Holiday (No Class on Nov 28)\n\nCheck-In Peer Feedback (Vocat)"
  },
  {
    "objectID": "slides/slides12.html#comments",
    "href": "slides/slides12.html#comments",
    "title": "STA 9750 - Week 12",
    "section": "Comments",
    "text": "Comments\nBad - Trivial:\n\n# Set x to 3\nx &lt;- 3\n\nBad - Opaque:\n\n# Follow instructions\nx &lt;- 3\n\nBad - Redundant / Explaining Code\n\n# Fit a model\nmod &lt;- model(x, y)\n\n# Build a query\nquery_build() |&gt; query_add() |&gt; query_formulate()"
  },
  {
    "objectID": "slides/slides12.html#comments-1",
    "href": "slides/slides12.html#comments-1",
    "title": "STA 9750 - Week 12",
    "section": "Comments",
    "text": "Comments\nGood - Purpose (“Business Logic”):\n\n# Regulation XX.YY requires us to apply a risk multiplier to output\n# As of 2024-11-01, CFO says risk multiplier is 3\n# Cf. Email to risk group, subject \"New Risk Multiplier\"\nRISK_MULTIPLIER &lt;- 3\n\nGood - Higher Level Structure (Example from googledrive package):\n\n# https://github.com/gaborcsardi/rencfaq#with-base-r\nwrite_utf8 &lt;- function(text, path = NULL) {\n  # sometimes we use writeLines() basically to print something for a snapshot\n  if (is.null(path)) {\n    return(base::writeLines(text))\n  }\n\n  # step 1: ensure our text is utf8 encoded\n  utf8 &lt;- enc2utf8(text)\n  upath &lt;- enc2utf8(path)\n\n  # step 2: create a connection with 'native' encoding\n  # this signals to R that translation before writing\n  # to the connection should be skipped\n  con &lt;- file(upath, open = \"w+\", encoding = \"native.enc\")\n  withr::defer(close(con))\n\n  # step 3: write to the connection with 'useBytes = TRUE',\n  # telling R to skip translation to the native encoding\n  base::writeLines(utf8, con = con, useBytes = TRUE)\n}"
  },
  {
    "objectID": "slides/slides12.html#comments-2",
    "href": "slides/slides12.html#comments-2",
    "title": "STA 9750 - Week 12",
    "section": "Comments",
    "text": "Comments\nMore Advice on StackOverflow"
  },
  {
    "objectID": "slides/slides12.html#faq-unicode-resources",
    "href": "slides/slides12.html#faq-unicode-resources",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Unicode Resources",
    "text": "FAQ: Unicode Resources\n\nUnicode Tables: unicodeplus.com/\nTaco Emoji History\nTaco Emoji Controversy"
  },
  {
    "objectID": "slides/slides12.html#faq-regular-expression-tools",
    "href": "slides/slides12.html#faq-regular-expression-tools",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Regular Expression Tools",
    "text": "FAQ: Regular Expression Tools\n\nTesting Regular Expressions Interactively: regex101.com/\nAlternative regexr.com/\nAutomated Regular Expression Builder: regex-generator\nAI Regexp Builder: hregexgo.com/"
  },
  {
    "objectID": "slides/slides12.html#faq-substrings-and-string-splitting",
    "href": "slides/slides12.html#faq-substrings-and-string-splitting",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Substrings and String Splitting",
    "text": "FAQ: Substrings and String Splitting\n\nfruits &lt;- c(\"apples and oranges and pears and bananas\", \n            \"pineapples and mangos and guavas\")\n\nstringr::str_split(fruits, \" and \")\n\n[[1]]\n[1] \"apples\"  \"oranges\" \"pears\"   \"bananas\"\n\n[[2]]\n[1] \"pineapples\" \"mangos\"     \"guavas\"    \n\n\n\nstringr::str_split_fixed(fruits, \"and\", n=2)\n\n     [,1]          [,2]                            \n[1,] \"apples \"     \" oranges and pears and bananas\"\n[2,] \"pineapples \" \" mangos and guavas\""
  },
  {
    "objectID": "slides/slides12.html#sub-strings-splitting",
    "href": "slides/slides12.html#sub-strings-splitting",
    "title": "STA 9750 - Week 12",
    "section": "Sub-Strings / Splitting",
    "text": "Sub-Strings / Splitting\n\nx &lt;- \"Baruch College, CUNY\"\nstringr::str_sub(x, end=6) # Includes endpoints\n\n[1] \"Baruch\"\n\n\n\nstringr::str_sub(x, start=-4) # Count from end\n\n[1] \"CUNY\"\n\n\n\nx &lt;- c(\"Baruch College, CUNY\", \"Brooklyn College, CUNY\")\nstringr::str_sub(x, end=-7) # Drop last _6_\n\n[1] \"Baruch College\"   \"Brooklyn College\""
  },
  {
    "objectID": "slides/slides12.html#faq-start-and-end-anchors",
    "href": "slides/slides12.html#faq-start-and-end-anchors",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Start and End Anchors",
    "text": "FAQ: Start and End Anchors\n\nWhen to use the ^ and $ anchors?\n\nStart and end of a line.\n\nVery useful for structured text (computer log outputs)\nIn data analysis, a bit less useful\n\nApplied to output of str_split"
  },
  {
    "objectID": "slides/slides12.html#faq-exclusion-detection",
    "href": "slides/slides12.html#faq-exclusion-detection",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Exclusion + Detection",
    "text": "FAQ: Exclusion + Detection\n\nx &lt;- c(\"10 blue fish\", \"three wet goats\")\nstringr::str_detect(x, \"[^0123456789]\")\n\n[1] TRUE TRUE\n\n\nstr_detect has a negate option:\n\nstringr::str_detect(x, \"[0-9]\", negate=TRUE)\n\n[1] FALSE  TRUE"
  },
  {
    "objectID": "slides/slides12.html#faq-str_detect-vs-str_match-vs-str_extract",
    "href": "slides/slides12.html#faq-str_detect-vs-str_match-vs-str_extract",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: str_detect vs str_match vs str_extract",
    "text": "FAQ: str_detect vs str_match vs str_extract\n\nstr_detect is there a ‘fit’?\nstr_extract extract the whole ‘fit’\nstr_match extract specific groups\n\n\nx &lt;- \"Baruch College, CUNY is a wonderful place to work!\"\nstringr::str_detect(x, \"(.*), CUNY\")\n\n[1] TRUE\n\nstringr::str_extract(x, \"(.*), CUNY\")\n\n[1] \"Baruch College, CUNY\"\n\nstringr::str_match(x, \"(.*), CUNY\")\n\n     [,1]                   [,2]            \n[1,] \"Baruch College, CUNY\" \"Baruch College\""
  },
  {
    "objectID": "slides/slides12.html#faq-subset-selection-indexing",
    "href": "slides/slides12.html#faq-subset-selection-indexing",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Subset Selection + Indexing",
    "text": "FAQ: Subset Selection + Indexing\nstr_match(group=) is useful for complex data extraction.\n\nx &lt;- c(\"Michael Weylandt teaches STA9750\", \"KRR teaches STA9891\")\npattern &lt;- c(\"(.*) teaches (.*)\")\nstringr::str_extract(x, pattern, group=1)\n\n[1] \"Michael Weylandt\" \"KRR\"             \n\nstringr::str_extract(x, pattern, group=2)\n\n[1] \"STA9750\" \"STA9891\"\n\n\n(Not sure what negatives do here…)\nAlso allows named groups:\n\nx &lt;- c(\"Michael Weylandt teaches STA9750 on Thursday\", \"KRR teaches STA9891 on Wednesday\")\npattern &lt;- c(\"(?&lt;instructor&gt;.*) teaches (?&lt;course&gt;.*) on (?&lt;weekday&gt;.*)\")\nstringr::str_match(x, pattern) |&gt; as.data.frame()\n\n                                            V1       instructor  course\n1 Michael Weylandt teaches STA9750 on Thursday Michael Weylandt STA9750\n2             KRR teaches STA9891 on Wednesday              KRR STA9891\n    weekday\n1  Thursday\n2 Wednesday"
  },
  {
    "objectID": "slides/slides12.html#faq-homoglyphs",
    "href": "slides/slides12.html#faq-homoglyphs",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Homoglyphs",
    "text": "FAQ: Homoglyphs\n\nx &lt;- c(\"Η\", \"H\")\ntolower(x)\n\n[1] \"η\" \"h\"\n\n\nWhy?\n\n\nuni_info &lt;- Vectorize(function(x) Unicode::u_char_name(utf8ToInt(x)), \"x\")\nuni_info(x)\n\n                         Η                          H \n\"GREEK CAPITAL LETTER ETA\"   \"LATIN CAPITAL LETTER H\" \n\n\n\n\nParticularly nasty with dashes - lean on [[:punct::]] where possible.\n\nx &lt;- c(\"Em Dash —\", \"En Dash –\", \"Hyphen ‐\")\nstringr::str_remove(x, \"[[:punct:]]\") # Works\n\n[1] \"Em Dash \" \"En Dash \" \"Hyphen \" \n\nstringr::str_remove(x, \"-\")  # Keyboard minus = Fail\n\n[1] \"Em Dash —\" \"En Dash –\" \"Hyphen ‐\""
  },
  {
    "objectID": "slides/slides12.html#faq-symbol-quantifiers",
    "href": "slides/slides12.html#faq-symbol-quantifiers",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: ? Symbol (Quantifiers)",
    "text": "FAQ: ? Symbol (Quantifiers)\nQuantifiers (multiple matches):\n\n.{a, b}: anywhere from a to b copies (inclusive)\n.{, b}: no more than b copies\n.{a,}: at least a copies\n.?: zero-or-one, same as .{0,1}\n.*: zero-or-more, same as .{0,}\n.+: one-or-more, same as {1,}"
  },
  {
    "objectID": "slides/slides12.html#faq-stringr-vs-grep-grepl",
    "href": "slides/slides12.html#faq-stringr-vs-grep-grepl",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: stringr vs grep / grepl",
    "text": "FAQ: stringr vs grep / grepl\nUltimately the same functionality, but stringr has a more consistent interface.\nConversion table online"
  },
  {
    "objectID": "slides/slides12.html#faq-working-columnwise",
    "href": "slides/slides12.html#faq-working-columnwise",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Working Columnwise",
    "text": "FAQ: Working Columnwise\nAll stringr functions work well in dplyr pipelines (“vectorized”):\n\nlibrary(dplyr); library(stringr)\ndf &lt;- data.frame(lower_letters = letters)\ndf |&gt; mutate(upper_letters = str_to_upper(lower_letters))\n\n   lower_letters upper_letters\n1              a             A\n2              b             B\n3              c             C\n4              d             D\n5              e             E\n6              f             F\n7              g             G\n8              h             H\n9              i             I\n10             j             J\n11             k             K\n12             l             L\n13             m             M\n14             n             N\n15             o             O\n16             p             P\n17             q             Q\n18             r             R\n19             s             S\n20             t             T\n21             u             U\n22             v             V\n23             w             W\n24             x             X\n25             y             Y\n26             z             Z"
  },
  {
    "objectID": "slides/slides12.html#faq-how-to-convert-to-utf-8",
    "href": "slides/slides12.html#faq-how-to-convert-to-utf-8",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: How to Convert to UTF-8",
    "text": "FAQ: How to Convert to UTF-8\nIf you know the source encoding:\n\ninconv(STRING, from=\"latin1\", to=\"UTF-8\")\n\nIf you don’t know the source, …."
  },
  {
    "objectID": "slides/slides12.html#agenda",
    "href": "slides/slides12.html#agenda",
    "title": "STA 9750 - Week 12",
    "section": "Agenda",
    "text": "Agenda\n\nUnicode Discussion\nRegex Discussion\nRegex Exercises\nCompletion of Cocktail Exercise\nTime Permitting: More Scraping\nTime Permitting: Statistical Inference"
  },
  {
    "objectID": "slides/slides12.html#breakout-rooms",
    "href": "slides/slides12.html#breakout-rooms",
    "title": "STA 9750 - Week 12",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nOrder\nTeam\n\nOrder\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "slides/slides07.html#mini-project-01",
    "href": "slides/slides07.html#mini-project-01",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\nGrades returned this afternoon.\nReview regrade policy and late work policy if you have questions."
  },
  {
    "objectID": "slides/slides07.html#mini-project-02",
    "href": "slides/slides07.html#mini-project-02",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nMP#02 - Identifying Environmentally Responsible US Public Transit Systems\nDue 2025-03-26 at 11:45pm ET\n\nGitHub post (used for peer feedback) AND Brightspace\nStart early to avoid Git issues\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness\nUse what you learned from MP#01\nTurn in something - 20% ‘free points’ are nothing to scoff at"
  },
  {
    "objectID": "slides/slides07.html#upcoming-mini-projects",
    "href": "slides/slides07.html#upcoming-mini-projects",
    "title": "STA 9750 - Week 7",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#03: Political Analysis\nMP#04: Retirement Forecasting"
  },
  {
    "objectID": "slides/slides07.html#course-project",
    "href": "slides/slides07.html#course-project",
    "title": "STA 9750 - Week 7",
    "section": "Course Project",
    "text": "Course Project\nProposal feedback soon (need to do 2 more before releasing)"
  },
  {
    "objectID": "slides/slides07.html#pre-assignments",
    "href": "slides/slides07.html#pre-assignments",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is 2025-03-26 at 11:45pm ET:\n\nIncludes an optional (but really great) lecture on data visualization from Di Cook (Monash B School (AUS))\n\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "slides/slides07.html#course-support",
    "href": "slides/slides07.html#course-support",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 2x / week\n\nMW Office Hours on Tuesdays + Thursday for rest of semester\nNo OH during Spring Break\n\n\nAsynchronous\n\nPiazza (\\(&lt;30\\) minute average response time)\n\n\nThanks to DS for helping peers on Piazza!"
  },
  {
    "objectID": "slides/slides07.html#upcoming-week",
    "href": "slides/slides07.html#upcoming-week",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nDue Wednesday at 11:45pm:\n\nPre-Assignment #08 (Brightspace)\n\nInteractive Tools for Data Visualization\n\nMP #02 on GitHub AND Brightspace"
  },
  {
    "objectID": "slides/slides07.html#additional-resources",
    "href": "slides/slides07.html#additional-resources",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nC. Wilke. Fundamentals of Data Visualization\nK. Healy. Data Visualization\nH. Wickham ggplot2: Elegant Visualizations for Data Analysis\n\n\n\nB. Yu and R. Barter Veridical Data Science"
  },
  {
    "objectID": "slides/slides07.html#faq-ggplot2-vs-tableau",
    "href": "slides/slides07.html#faq-ggplot2-vs-tableau",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: ggplot2 vs Tableau",
    "text": "FAQ: ggplot2 vs Tableau\n\nTableau\n\n$$$\nIT department automatically integrates with data sources\nEasy, if it does what you want\n\nggplot2\n\nFree\nCan use arbitrary data sources, with effort\nFlexible / customizable"
  },
  {
    "objectID": "slides/slides07.html#faq-ggplot2-vs-matplotlib",
    "href": "slides/slides07.html#faq-ggplot2-vs-matplotlib",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: ggplot2 vs matplotlib",
    "text": "FAQ: ggplot2 vs matplotlib\n\nggplot2\n\nData visualizations\nEnforces “good practice” via gg\n\nmatplotlib\n\nScientific visualizations\nMore flexible for good or for ill\nInspired by Matlab plotting\n\n\nClosest Python analogue to ggplot2 is seaborn"
  },
  {
    "objectID": "slides/slides07.html#faq-why-use-instead-of",
    "href": "slides/slides07.html#faq-why-use-instead-of",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Why use + instead of |>",
    "text": "FAQ: Why use + instead of |&gt;\n\nggplot2 is older than |&gt;\nPer H. Wickham: if ggplot3 ever gets made, will use |&gt;\nUnlikely to change: too much code depends on it"
  },
  {
    "objectID": "slides/slides07.html#faq-performance",
    "href": "slides/slides07.html#faq-performance",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Performance",
    "text": "FAQ: Performance\n\nI tried an interactive plot with \\(n=132,000\\) points, but it brought my computer to a halt. [Ed. Paraphrased]\n\nThat’s a lot of plots!!\nggplot2 is itself pretty fast, but it depends on (possibly slow) graphics backends\n\nDifferent file types implement graphics differently.\nYou should also think about overplotting / pre-processing"
  },
  {
    "objectID": "slides/slides07.html#faq-overplotting",
    "href": "slides/slides07.html#faq-overplotting",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Overplotting",
    "text": "FAQ: Overplotting\nLarge data sets can lead to overplotting:\n\nPoints “on top of” each other\nCan also occur with “designed” experiments / rounded data\n\nWays to address:\n\ngeom_jitter\ngeom_hex"
  },
  {
    "objectID": "slides/slides07.html#faq-overplotting-1",
    "href": "slides/slides07.html#faq-overplotting-1",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Overplotting",
    "text": "FAQ: Overplotting\nJitter: add a bit of random noise so points don’t step on each other\n\nlibrary(ggplot2); library(patchwork)\np &lt;- ggplot(mpg, aes(cyl, hwy))\np1 &lt;- p + geom_point() + ggtitle(\"geom_point\")\np2 &lt;- p + geom_jitter() + ggtitle(\"geom_jitter\")\np1 + p2"
  },
  {
    "objectID": "slides/slides07.html#faq-hexagonal-binning",
    "href": "slides/slides07.html#faq-hexagonal-binning",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Hexagonal Binning",
    "text": "FAQ: Hexagonal Binning\nLittle “heatmaps” of counts. Hexagons to avoid weird rounding artifacts\n\nlibrary(ggplot2); library(patchwork)\np &lt;- ggplot(diamonds, aes(carat, price))\np1 &lt;- p + geom_point() + ggtitle(\"geom_point\")\np2 &lt;- p + geom_hex() + ggtitle(\"geom_hex\")\np1 + p2"
  },
  {
    "objectID": "slides/slides07.html#faq-inside-vs.-outside-aes",
    "href": "slides/slides07.html#faq-inside-vs.-outside-aes",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Inside vs. Outside aes()",
    "text": "FAQ: Inside vs. Outside aes()\naes maps data to values. Outside of aes, set constant value\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, \n       aes(x=bill_length_mm, y=bill_depth_mm, color=species))+ geom_point()"
  },
  {
    "objectID": "slides/slides07.html#faq-inside-vs.-outside-aes-1",
    "href": "slides/slides07.html#faq-inside-vs.-outside-aes-1",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Inside vs. Outside aes()",
    "text": "FAQ: Inside vs. Outside aes()\naes maps data to values. Outside of aes, set constant value\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, \n       aes(x=bill_length_mm, y=bill_depth_mm))+ geom_point(color=\"blue\")"
  },
  {
    "objectID": "slides/slides07.html#faq-global-vs-geom_-specific-aes",
    "href": "slides/slides07.html#faq-global-vs-geom_-specific-aes",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Global vs geom_ specific aes()",
    "text": "FAQ: Global vs geom_ specific aes()\n\nElements set in ggplot() apply to entire plot\nElements set in specific geom apply there only\n\nOverride globals\n\n\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, \n       aes(x=bill_length_mm, y=bill_depth_mm, color=species))+\n    geom_smooth() + \n    geom_point(color=\"blue\")"
  },
  {
    "objectID": "slides/slides07.html#faq-how-to-choose-plot-types",
    "href": "slides/slides07.html#faq-how-to-choose-plot-types",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: How to choose plot types",
    "text": "FAQ: How to choose plot types\nTwo “modes”\n\nExploratory data analysis. Quick, rapid iteration, for your eyes only\n\nLet the data tell you a story\nLow pre-processing: scatter plots, lines, histograms\n\n“Publication quality”. Polished,\n\nYou tell the reader a story\nMore processing, more modeling: trends, line segments, ribbons"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes",
    "href": "slides/slides07.html#faq-color-palettes",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\nThree types of color palettes:\n\nSequential: ordered from 0 to “high”\n\nExample: rain forecast in different areas\n\nDiverging: ordered from -X to +X with meaningful 0 in the middle\n\nExample: political leaning\n\nQualitative: no ordering\n\n\nWhen mapping quantitative variables to palettes (sequential/diverging), two approaches:\n\nBinned: \\([0, 1)\\) light green, \\([1, 3)\\) medium green; \\([3, 5]\\) dark green\nContinuous"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes-1",
    "href": "slides/slides07.html#faq-color-palettes-1",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_distiller(type=\"seq\") # Continuous"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes-2",
    "href": "slides/slides07.html#faq-color-palettes-2",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_fermenter(type=\"seq\") # Binned"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes-3",
    "href": "slides/slides07.html#faq-color-palettes-3",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_fermenter(type=\"seq\") # Binned + Sequential"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes-4",
    "href": "slides/slides07.html#faq-color-palettes-4",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() +\n    scale_color_fermenter(type=\"qual\") # Binned + Qualitative"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes-5",
    "href": "slides/slides07.html#faq-color-palettes-5",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_fermenter(type=\"div\") # Binned + Diverging"
  },
  {
    "objectID": "slides/slides07.html#faq-how-to-hard-code-colors",
    "href": "slides/slides07.html#faq-how-to-hard-code-colors",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: How to “hard-code” colors",
    "text": "FAQ: How to “hard-code” colors\n\nlibrary(dplyr)\ndata &lt;- data.frame(x = rnorm(5), \n                   y = rnorm(5), \n                   group = c(\"a\", \"a\", \"b\", \"b\", \"b\"))\n\ndata |&gt; \n    group_by(group) |&gt;\n    mutate(n_count = n()) |&gt;\n    ungroup() |&gt;\n    mutate(color = ifelse(n_count == max(n_count), \"red\", \"black\")) |&gt;\n    ggplot(aes(x=x, y=y, shape=group, color=color)) + \n    geom_point() + \n    scale_color_identity()"
  },
  {
    "objectID": "slides/slides07.html#faq-how-to-customize-themes",
    "href": "slides/slides07.html#faq-how-to-customize-themes",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: How to Customize Themes",
    "text": "FAQ: How to Customize Themes\nBuilt-in themes + ggthemes package:\n\nlibrary(ggplot2); library(ggthemes); \nlibrary(palmerpenguins); library(ggpmisc)\np &lt;- ggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    facet_wrap(~species)"
  },
  {
    "objectID": "slides/slides07.html#faq-themes",
    "href": "slides/slides07.html#faq-themes",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nDefault theme (ggplot2::theme_grey()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-1",
    "href": "slides/slides07.html#faq-themes-1",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nBlack and White theme (ggplot2::theme_bw()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-2",
    "href": "slides/slides07.html#faq-themes-2",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nMinimal theme (ggplot2::theme_minimal()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-3",
    "href": "slides/slides07.html#faq-themes-3",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nLight theme (ggplot2::theme_light()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-4",
    "href": "slides/slides07.html#faq-themes-4",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nDark theme (ggplot2::theme_dark()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-5",
    "href": "slides/slides07.html#faq-themes-5",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nExcel theme (ggthemes::theme_excel()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-6",
    "href": "slides/slides07.html#faq-themes-6",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nGoogle Docs theme (ggthemes::theme_gdocs()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-7",
    "href": "slides/slides07.html#faq-themes-7",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nThe Economist theme (ggthemes::theme_economist()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-8",
    "href": "slides/slides07.html#faq-themes-8",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nThe Economist theme (ggthemes::theme_economist()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-9",
    "href": "slides/slides07.html#faq-themes-9",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nSolarized theme (ggthemes::theme_solarized()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-10",
    "href": "slides/slides07.html#faq-themes-10",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nSolarized2 theme (ggthemes::theme_solarized_2()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-11",
    "href": "slides/slides07.html#faq-themes-11",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nStata theme (ggthemes::theme_stata()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-12",
    "href": "slides/slides07.html#faq-themes-12",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nTufte theme (ggthemes::theme_tufte()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-13",
    "href": "slides/slides07.html#faq-themes-13",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nWall Street Journal theme (ggthemes::theme_wsj()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-14",
    "href": "slides/slides07.html#faq-themes-14",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nMany more online:\n\nThemePark for movie themes"
  },
  {
    "objectID": "slides/slides07.html#faq-order-of-layers",
    "href": "slides/slides07.html#faq-order-of-layers",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Order of Layers",
    "text": "FAQ: Order of Layers\nOrder of layers technically matters, but the effect is small\n\np1 &lt;- ggplot(penguins, aes(x=bill_length_mm, y=flipper_length_mm)) +\n        geom_point(color=\"black\") + \n        geom_smooth(color=\"blue\", method=\"lm\") + ggtitle(\"Line on points\")\np2 &lt;- ggplot(penguins, aes(x=bill_length_mm, y=flipper_length_mm)) +\n        geom_smooth(color=\"blue\", method=\"lm\") + \n        geom_point(color=\"black\") + ggtitle(\"Points on line\")\np1 + p2"
  },
  {
    "objectID": "slides/slides07.html#faq-order-of-layers-1",
    "href": "slides/slides07.html#faq-order-of-layers-1",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Order of layers",
    "text": "FAQ: Order of layers\nOrder matters more with theme. Adding a theme_*() will override any theme() customization you did:\n\np1 &lt;- p + theme_bw() + theme(legend.position=\"bottom\")\np2 &lt;- p + theme(legend.position=\"bottom\") + theme_bw() \np1 + p2"
  },
  {
    "objectID": "slides/slides07.html#faq-stat_poly_lineeq-vs-geom_smooth",
    "href": "slides/slides07.html#faq-stat_poly_lineeq-vs-geom_smooth",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: stat_poly_{line,eq} vs geom_smooth",
    "text": "FAQ: stat_poly_{line,eq} vs geom_smooth\nBy default geom_smooth fits a generalized additive model (GAM)\nggpmisc::stat_poly_{line,eq} fit linear models, so they can expose more machinery.\n\nWhat is a GAM?\n\nTake 9890 with me (typically Spring semester) to find out!\nFree Course: “GAMs in R” from Noam Ross"
  },
  {
    "objectID": "slides/slides07.html#faq-titles-and-captions",
    "href": "slides/slides07.html#faq-titles-and-captions",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Titles and Captions",
    "text": "FAQ: Titles and Captions\n\nggplot() + \n    labs(title=\"Title\", subtitle=\"Subtitle\", caption=\"Caption\",\n         tag=\"Tag\", alt=\"Alt-Text\", alt_insight=\"Alt-Insight\")\n\n\n+ggtitle(\"text\") is just shorthand for +labs(title=\"text\")"
  },
  {
    "objectID": "slides/slides07.html#faq-relative-importance-of-aesthetics",
    "href": "slides/slides07.html#faq-relative-importance-of-aesthetics",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Relative Importance of Aesthetics",
    "text": "FAQ: Relative Importance of Aesthetics\nPerceptually:\n\nLocation &gt; Color &gt; Size &gt; Shape\n\nHumans are better at:\n\nLength &gt; Area &gt; Volume"
  },
  {
    "objectID": "slides/slides07.html#faq-when-to-use-facets",
    "href": "slides/slides07.html#faq-when-to-use-facets",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: When to Use Facets?",
    "text": "FAQ: When to Use Facets?\nFacets are group_by for plots. Useful for\n\nDistinguishing intra- vs inter-group trends\nAvoiding overplotting"
  },
  {
    "objectID": "slides/slides07.html#faq-simpsons-paradox",
    "href": "slides/slides07.html#faq-simpsons-paradox",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Simpson’s Paradox",
    "text": "FAQ: Simpson’s Paradox"
  },
  {
    "objectID": "slides/slides07.html#faq-simpsons-paradox-1",
    "href": "slides/slides07.html#faq-simpsons-paradox-1",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Simpson’s Paradox",
    "text": "FAQ: Simpson’s Paradox"
  },
  {
    "objectID": "slides/slides07.html#faq-twin-axes-plots",
    "href": "slides/slides07.html#faq-twin-axes-plots",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Twin Axes Plots",
    "text": "FAQ: Twin Axes Plots\n\nHow can I implement a dual (twin) axis plot in ggplot2?\n\nDisfavored. But if you must …\nsec.axis\nDoesn’t allow arbitrary secondary axes; allows transformed axes (e.g., Celsius and Fahrenheit)"
  },
  {
    "objectID": "slides/slides07.html#faq-embedding-images-in-ggplot",
    "href": "slides/slides07.html#faq-embedding-images-in-ggplot",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Embedding images in ggplot",
    "text": "FAQ: Embedding images in ggplot\nSee the ggimage or ggflags package for images as “points”:\n\n#devtools::install_github(\"jimjam-slam/ggflags\"); \nlibrary(ggflags)\nd &lt;- data.frame(x=rnorm(50), y=rnorm(50), \n                country=sample(c(\"ar\",\"fr\", \"nz\", \"gb\", \"es\", \"ca\"), 50, TRUE), \n                stringsAsFactors = FALSE)\nggplot(d, aes(x=x, y=y, country=country, size=x)) + \n  geom_flag() + \n  scale_country()"
  },
  {
    "objectID": "slides/slides07.html#faq-embedding-images",
    "href": "slides/slides07.html#faq-embedding-images",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: Embedding Images",
    "text": "FAQ: Embedding Images\nSee cowplot::draw_image() for image background:\n\nlibrary(cowplot)\np &lt;- ggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.7) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_half_open(12)\n\nlogo_file &lt;- system.file(\"extdata\", \"logo.png\", package = \"cowplot\")\nggdraw() +\n  draw_image(\n    logo_file, scale = .7\n  ) +\n  draw_plot(p)"
  },
  {
    "objectID": "slides/slides07.html#diving-deeper-with-ggplot2",
    "href": "slides/slides07.html#diving-deeper-with-ggplot2",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Diving Deeper with ggplot2",
    "text": "Diving Deeper with ggplot2\nData Sets:\n\ndiamonds from the ggplot2 package\ncdiac from the CVXR package\ngapminder from the gapminder package\n\nYou need to install CVXR and gapminder now.\nExercise: Lab #07"
  },
  {
    "objectID": "slides/slides07.html#diving-deeper-with-ggplot2-learning-goals",
    "href": "slides/slides07.html#diving-deeper-with-ggplot2-learning-goals",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Diving Deeper with ggplot2: Learning Goals",
    "text": "Diving Deeper with ggplot2: Learning Goals\nToday:\n\nFluency with basic geoms\nAnimation (Just a bit)\n\nNext Week:\n\nInteractive graphics\nDashboards\nSpatial Data (time allowing)"
  },
  {
    "objectID": "slides/slides07.html#breakout-rooms",
    "href": "slides/slides07.html#breakout-rooms",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nRoom\nTeam\n\nRoom\nTeam\n\n\n\n\n1\nVH + SG + DS + DL\n\n5\nGB + RJ + FS + MH\n\n\n2\nHZ + JLL + CA\n\n6\nEM + AK + GMdS + JL\n\n\n3\nMT + CW + VG + GS + CWo.\n\n7\nSJB + JC + SB + ZS\n\n\n4\nSD + GO + CFGF"
  },
  {
    "objectID": "slides/slides05.html#mini-project-01",
    "href": "slides/slides05.html#mini-project-01",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\nDue in 6 Days Four already submitted - fantastic!\n(Hopefully an excess of preparedness, not a last minute panic)\n\n\nSubmit early and submit often\n\nLess “last minute” tech support going forward\n\nUse Piazza and use your peers\n\nVery impressed by Detailed Analyses, Fancy Tables, Graphics\nYou don’t need fancy graphics yet, but I love to see above and beyond"
  },
  {
    "objectID": "slides/slides05.html#mini-project-01---peer-feedback",
    "href": "slides/slides05.html#mini-project-01---peer-feedback",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "STA 9750 Mini-Project #01 - Peer Feedback",
    "text": "STA 9750 Mini-Project #01 - Peer Feedback\nTo be assigned on GitHub in one week (March 5th):\n\n\\(\\approx 4\\) feedbacks each\nTake this seriously: around 20% of this assignment is “meta-review”\nGoal: rigorous constructive critique\n\nUse helper functions to find submissions assigned to you. Ask on Piazza if still having trouble."
  },
  {
    "objectID": "slides/slides05.html#mini-project-02",
    "href": "slides/slides05.html#mini-project-02",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nMP#02 released - Public Transit\n\nDue 2025-03-26 at 11:45pm ET\n\nGitHub post (used for peer feedback) AND Brightspace\nOne Month: don’t wait until the very end\n\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness, not necessarily correctness"
  },
  {
    "objectID": "slides/slides05.html#upcoming-mini-projects",
    "href": "slides/slides05.html#upcoming-mini-projects",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTopics\n\nMP#03: Spotify Analytics - 2025-04-23 at 11:45pm ET\nMP#04: Political Analysis - 2025-05-07 at 11:45pm ET"
  },
  {
    "objectID": "slides/slides05.html#pre-assignments",
    "href": "slides/slides05.html#pre-assignments",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is March 20th - few weeks off\n\nApologies for misleading question this week. I forgot to change it when giving the extension"
  },
  {
    "objectID": "slides/slides05.html#course-project",
    "href": "slides/slides05.html#course-project",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Course Project",
    "text": "Course Project\n2 teams already formed\n\nTeam 1: DL + DS + SG + VH\nTeam 2: VG\nMore to come\n\nAll team commitments due via email 2025-03-05 at 11:45pm ET"
  },
  {
    "objectID": "slides/slides05.html#course-support",
    "href": "slides/slides05.html#course-support",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 2x / week\n\nAsynchronous\n\nPiazza (&lt;30 minute average response time)\n\n\n\nLong list of helpful resources added on Piazza, esp. https://learnr.numbat.space/\n\n\nAsk questions! This course is demanding but rewarding.\nSocial contract: I push you hard, but I also provide lots of support."
  },
  {
    "objectID": "slides/slides05.html#upcoming-week",
    "href": "slides/slides05.html#upcoming-week",
    "title": "STA 9750 - Week 5 Update",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nNext Wednesday at 11:45pm:\n\nMP#01 peer feedback due\nTeam membership due\n\nNo class on October 3rd"
  },
  {
    "objectID": "slides/slides05.html#october-10---project-proposal-presentations",
    "href": "slides/slides05.html#october-10---project-proposal-presentations",
    "title": "STA 9750 - Week 5 Update",
    "section": "October 10 - Project Proposal Presentations",
    "text": "October 10 - Project Proposal Presentations\nOfficial Description\n\n6 minute presentation\nKey topics:\n\nAnimating Question\nTeam Roster\n\nAlso discuss: Possible specific questions, data sources, analytical plan, anticipated challenges\n\nPeer feedback mechanism TBD"
  },
  {
    "objectID": "slides/slides05.html#faq-subqueries",
    "href": "slides/slides05.html#faq-subqueries",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: Subqueries",
    "text": "FAQ: Subqueries\n\n[W]ill we be learning how to perform joins within a subquery?\n\nYou don’t need subqueries in R since it’s an imperative language. Just create a new variable to represent the result of the subquery and use that in the next command.\nSELECT first_name, last_name\nFROM collectors\nWHERE id IN (\n    SELECT collector_id\n    FROM sales\n);\ncollector_ids &lt;- sales |&gt; pull(collector_id)\ncollectors |&gt; filter(id %in% collector_ids) |&gt; select(first_name, last_name)"
  },
  {
    "objectID": "slides/slides05.html#faq-data-integrity",
    "href": "slides/slides05.html#faq-data-integrity",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: Data Integrity",
    "text": "FAQ: Data Integrity\n\n[H]ow can we ensure that the information [resulting from a join] is accurate and not repeated?\n\n\nIf you have a true unique ID, you’re usually safe\nPay attention to all warnings\nManually examine the result of any joins"
  },
  {
    "objectID": "slides/slides05.html#faq-performance-on-large-data-sets",
    "href": "slides/slides05.html#faq-performance-on-large-data-sets",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: Performance on Large Data Sets",
    "text": "FAQ: Performance on Large Data Sets\n\nWill joining large data sets […] affect performance?\n\nSomewhat - larger data sets are always slower.\nBigger danger is “bad joins” creating huge data automatically.\nNote that R is less “smart” than SQL, so won’t optimize execution order for you automatically."
  },
  {
    "objectID": "slides/slides05.html#faq-what-is-the-role-of-pivot_wider",
    "href": "slides/slides05.html#faq-what-is-the-role-of-pivot_wider",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: What is the Role of pivot_wider?",
    "text": "FAQ: What is the Role of pivot_wider?\n\nIs [pivot_wider] just for formatting?\n\n\nlibrary(dplyr); library(tidyr); library(palmerpenguins)\npenguins |&gt; drop_na() |&gt; \n    group_by(sex, species) |&gt; \n    summarize(weight = mean(body_mass_g)) |&gt;\n    pivot_wider(id_cols=species, \n                names_from=sex,\n                values_from=weight) |&gt;\n    mutate(gender_diff = male - female)\n\n# A tibble: 3 × 4\n  species   female  male gender_diff\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     3369. 4043.        675.\n2 Chinstrap  3527. 3939.        412.\n3 Gentoo     4680. 5485.        805."
  },
  {
    "objectID": "slides/slides05.html#faq-dplyr-joins-vs-sql-joins",
    "href": "slides/slides05.html#faq-dplyr-joins-vs-sql-joins",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: dplyr joins vs SQL joins",
    "text": "FAQ: dplyr joins vs SQL joins\n\nWhat is the difference between dplyr and SQL joins?\n\nNot too much - biggest difference is no INDEX or FOREIGN KEY in R so less guarantees of data integrity."
  },
  {
    "objectID": "slides/slides05.html#faq-when-to-use-anti_join",
    "href": "slides/slides05.html#faq-when-to-use-anti_join",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: When to use anti_join?",
    "text": "FAQ: When to use anti_join?\nRare: looking for unmatched rows. - Useful to find data integrity issues or ‘implicit’ missingness. - I use an anti_join to find students who haven’t submitted an assignment."
  },
  {
    "objectID": "slides/slides05.html#faq-many-to-many-warning",
    "href": "slides/slides05.html#faq-many-to-many-warning",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: many-to-many Warning",
    "text": "FAQ: many-to-many Warning\nTricky to address, but fortunately pretty rare.\n\nSQL explicitly forbids many-to-many\nUsually a sign that a “key” isn’t really unique\n\nCheck for duplicates in x and y tables\nCan occur with “fancy” joins (rolling, inequality)\n\nAdd additional join variables to break “duplication”"
  },
  {
    "objectID": "slides/slides05.html#faq-how-to-check-efficiency",
    "href": "slides/slides05.html#faq-how-to-check-efficiency",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: How to Check Efficiency?",
    "text": "FAQ: How to Check Efficiency?\nNo automatic way. Some rules of thumb:\n\nDon’t create large tables just to filter down\n\nfilter before join when possible\n\nfull_outer join is a bit dangerous\ncross_join is rarely the right answer"
  },
  {
    "objectID": "slides/slides05.html#faq-tidyr-vs-dplyr",
    "href": "slides/slides05.html#faq-tidyr-vs-dplyr",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: tidyr vs dplyr",
    "text": "FAQ: tidyr vs dplyr\n\nIs tidyr more efficient than dplyr?\n\nNope - different packages from the same developers.\nDesigned to work together elegantly."
  },
  {
    "objectID": "slides/slides05.html#faq-rare-joins",
    "href": "slides/slides05.html#faq-rare-joins",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: Rare Joins",
    "text": "FAQ: Rare Joins\n\nPlease explain what cross_join, filter joins, and nest_join are?\n\n\ncross_join: dangerous.\n\nCreates “all pairs” of rows. Useful for ‘design’ problems\n\nfilter joins (anti_, semi_):\n\nHunting down quietly missing data.\nFiltering to sub-samples\n\nnest_join: beyond this course.\n\nleft_join with extra structure to output."
  },
  {
    "objectID": "slides/slides05.html#faq-how-to-pick-a-join",
    "href": "slides/slides05.html#faq-how-to-pick-a-join",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "FAQ: How to Pick a Join",
    "text": "FAQ: How to Pick a Join\n\nHow do I decide which type of join is most approriate for a given analysis?\n\nTopic of today’s work."
  },
  {
    "objectID": "slides/slides05.html#other-tips",
    "href": "slides/slides05.html#other-tips",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Other Tips",
    "text": "Other Tips\n\nDisable RStudio’s visual Quarto editor. It’s more trouble than it’s worth. To stop it from opening by default, add editor: source in the header of your qmd files.\nQuarto depends on file structure for organizing content. The main directory (STA9750-2025-SPRING) should hold all of your input files. You should never directly put anything in the docs/ folder. That’s where generated output should live.\nWhen I leave &lt;GITHUB_NAME&gt; or similar in instructions, put in your GitHub ID. (And make sure to remove the &lt; and &gt; symbols)"
  },
  {
    "objectID": "slides/slides05.html#diving-deeper-with-dplyr---joins-and-pivots",
    "href": "slides/slides05.html#diving-deeper-with-dplyr---joins-and-pivots",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Diving Deeper with dplyr - Joins and Pivots",
    "text": "Diving Deeper with dplyr - Joins and Pivots\nData Set: nycflights13\nExercise: Lab #05"
  },
  {
    "objectID": "slides/slides10.html#mini-project-03",
    "href": "slides/slides10.html#mini-project-03",
    "title": "STA 9750 - Week 10",
    "section": "STA 9750 Mini-Project #03",
    "text": "STA 9750 Mini-Project #03\nNow online\nDue November 13th\n\nGitHub post (used for peer feedback) AND Brightspace\nThree Weeks: don’t wait until the very end\nShould be less demanding than MP #01 and MP#02\n\nLots of little files. No big files!\nMaps and election retrodiction\n\n\n\nPay attention to the rubric"
  },
  {
    "objectID": "slides/slides10.html#mini-project-03-1",
    "href": "slides/slides10.html#mini-project-03-1",
    "title": "STA 9750 - Week 10",
    "section": "STA 9750 Mini-Project #03",
    "text": "STA 9750 Mini-Project #03\nTips:\n\ngeom_sf by default puts a thin grey boundary around each geometry. Disable this by using geom_sf(color=NA).\nYou can plot a state in a certain color by plotting all of its districts in that same color. Alternatively, use st_combine to combine geometries.\nEach state gets \\(R+2\\) ECVs where \\(R\\) is the number of congressional districts in that state.\nst_simplify can “smooth out” boundaries to make things easier to plot"
  },
  {
    "objectID": "slides/slides10.html#upcoming-mini-projects",
    "href": "slides/slides10.html#upcoming-mini-projects",
    "title": "STA 9750 - Week 10",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#04: Something financial\n\nAny requests?"
  },
  {
    "objectID": "slides/slides10.html#pre-assignments",
    "href": "slides/slides10.html#pre-assignments",
    "title": "STA 9750 - Week 10",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is November 13th\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "slides/slides10.html#grading",
    "href": "slides/slides10.html#grading",
    "title": "STA 9750 - Week 10",
    "section": "Grading",
    "text": "Grading\nWe owe you:\n\nMP#02 final average\nMP#02 peer meta-review\n\nWe will owe you:\n\nMid-Term Check-In Feedback"
  },
  {
    "objectID": "slides/slides10.html#course-support",
    "href": "slides/slides10.html#course-support",
    "title": "STA 9750 - Week 10",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(38\\) minute average response time)\n\n\nChange: MW Thursday Zoom OH now 4:00pm to 5:00pm"
  },
  {
    "objectID": "slides/slides10.html#upcoming",
    "href": "slides/slides10.html#upcoming",
    "title": "STA 9750 - Week 10",
    "section": "Upcoming",
    "text": "Upcoming\nNext Week:\n\nMini-Project #03\nPre-Assignment\n\nNov 20:\n\nMP#03 Peer Feedback\nPre Assignment\n\nNov 27 - Thanksgiving Holiday (No Class on Nov 28)\n\nCheck-In Peer Feedback (Vocat)"
  },
  {
    "objectID": "slides/slides10.html#check-in-presentations",
    "href": "slides/slides10.html#check-in-presentations",
    "title": "STA 9750 - Week 10",
    "section": "Check-In Presentations",
    "text": "Check-In Presentations\nToday, we’re looking for:\n\n6 minutes\nLocking in on specific questions\nEngagement with existing literature\n\nMainly, we want to see that you will be able to succeed"
  },
  {
    "objectID": "slides/slides10.html#presentation-order",
    "href": "slides/slides10.html#presentation-order",
    "title": "STA 9750 - Week 10",
    "section": "Presentation Order",
    "text": "Presentation Order\n\n\n\nOrder\nTeam\n\nOrder\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "slides/slides03.html#mini-project-00",
    "href": "slides/slides03.html#mini-project-00",
    "title": "STA 9750 - Week 3 Update",
    "section": "STA 9750 Mini-Project #00",
    "text": "STA 9750 Mini-Project #00\n\nMP#00 submitted\n\nA few of you didn’t submit; I’ll follow up directly for VoE\n\nMP#00 peer feedback assignments released (check GitHub)\n\nGive some feedback to your peers\nGet ideas for improving your own site"
  },
  {
    "objectID": "slides/slides03.html#mini-project-01",
    "href": "slides/slides03.html#mini-project-01",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\n\nMP#01 released\nStart early\n\nNot too hard if everything is working (post-MP#00)\nTech support takes time\n\n\nFound on Course Site"
  },
  {
    "objectID": "slides/slides03.html#graduate-teaching-assistant-gta",
    "href": "slides/slides03.html#graduate-teaching-assistant-gta",
    "title": "STA 9750 - Week 3 Update",
    "section": "Graduate Teaching Assistant (GTA)",
    "text": "Graduate Teaching Assistant (GTA)\n\nCharles Ramirez\nTwice Weekly Office Hours (Zoom)\n\nTuesdays 4-5pm\nFridays 12-1pm\n\nWill also help coordinate peer feedback (GitHub), Piazza responses, etc.\nExcellent resource for course project advice!"
  },
  {
    "objectID": "slides/slides03.html#piazza-participation",
    "href": "slides/slides03.html#piazza-participation",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Piazza Participation",
    "text": "Piazza Participation\n\nAverage time to response: 15 minutes\n154 posts\n\nThanks to those of you who are helping classmates!"
  },
  {
    "objectID": "slides/slides03.html#course-project",
    "href": "slides/slides03.html#course-project",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Course Project",
    "text": "Course Project\n\nTeams: 3-4 classmates\nStages:\n\nProposal (in class presentation)\nMid-semester check-in (in class presentation)\nFinal: in class presentation, individual report, summary report\n\nStructure:\n\nShared “Overarching Question”\nIndividual “Specific Question” (one per teammate)\n\n\nFull description online"
  },
  {
    "objectID": "slides/slides03.html#upcoming-week",
    "href": "slides/slides03.html#upcoming-week",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nNext Wednesday at 11:45pm:\n\nNext Pre-Assignment\nMP#00 Peer Feedback due"
  },
  {
    "objectID": "slides/slides03.html#faq-vector-index-printout-rules",
    "href": "slides/slides03.html#faq-vector-index-printout-rules",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "FAQ: Vector Index Printout Rules",
    "text": "FAQ: Vector Index Printout Rules\nDefault vector printing:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nEach line gets a new index:\n\nsqrt(1:10)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\n\nMore complex objects have alternate print styles:\n\nmatrix(1:9, nrow=3, ncol=3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nPrint width is controlled by getOption(\"width\")."
  },
  {
    "objectID": "slides/slides03.html#faq-recycling-rules",
    "href": "slides/slides03.html#faq-recycling-rules",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "FAQ: Recycling Rules",
    "text": "FAQ: Recycling Rules\nAlignment by default:\n\nx &lt;- 1:3\ny &lt;- 4:6\nx + y\n\n[1] 5 7 9\n\n\nRecycling by default:\n\nx &lt;- 1\ny &lt;- 4:6\nx + y\n\n[1] 5 6 7\n\n\nRecycle warning when vectors don’t fit together cleanly:\n\nx &lt;- 1:2\ny &lt;- 4:6\nx + y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 5 7 7"
  },
  {
    "objectID": "slides/slides03.html#faq-recycling-warning",
    "href": "slides/slides03.html#faq-recycling-warning",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "FAQ: Recycling Warning",
    "text": "FAQ: Recycling Warning\n\nx &lt;- 1:2\ny &lt;- 4:6\nx + y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 5 7 7\n\n\nNot a problem per se, but often a sign that something has gone wrong.\n\nscalar + vector is usually safe\n2 vectors of same size is usually safe\nvectors of different size is usually a programming mistake"
  },
  {
    "objectID": "slides/slides03.html#faq-warnings-vs-errors",
    "href": "slides/slides03.html#faq-warnings-vs-errors",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "FAQ: Warnings vs Errors",
    "text": "FAQ: Warnings vs Errors\n\nWarnings: heuristics pointing at typical problem\n\nCode still executed without a problem\nTry to fix these unless you’re certain it’s not a problem\n\nErrors: code failed to execute\n\nYou have to fix these to run your code"
  },
  {
    "objectID": "slides/slides03.html#faq-changing-built-in-functions",
    "href": "slides/slides03.html#faq-changing-built-in-functions",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "FAQ: Changing built-in functions",
    "text": "FAQ: Changing built-in functions\nMost built-in functions can’t / shouldn’t be changed.\nSome allow alternate behavior via additional arguments:\n\nlog(10) # Default is natural (base e) logarithm\n\n[1] 2.302585\n\nlog(10, base=10)\n\n[1] 1\n\n\nIf you want different behavior, write your own function:\n\ncosd &lt;- function(x){\n    ## Cosine in degrees\n    cos(x * pi / 180)\n}\ncosd(90)\n\n[1] 6.123234e-17\n\n\nAlways try ?name to see documentation."
  },
  {
    "objectID": "slides/slides03.html#faq-git-workflow",
    "href": "slides/slides03.html#faq-git-workflow",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "FAQ: Git Workflow",
    "text": "FAQ: Git Workflow\nThree key commands:\n\ngit add: add some changes to a ‘box’\ngit commit: seal the ‘box’\ngit push: send the ‘box’ to GitHub\n\nGit pane in RStudio shows uncommited changes, not files.\nIf a file ‘vanishes’ after a commit, that’s good!"
  },
  {
    "objectID": "slides/slides09.html#mini-project-02",
    "href": "slides/slides09.html#mini-project-02",
    "title": "STA 9750 - Week 9",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nThank you for peer feedback! I had a lot of fun reading these."
  },
  {
    "objectID": "slides/slides09.html#mini-project-03",
    "href": "slides/slides09.html#mini-project-03",
    "title": "STA 9750 - Week 9",
    "section": "STA 9750 Mini-Project #03",
    "text": "STA 9750 Mini-Project #03\nNow online\nDue November 13th\n\nGitHub post (used for peer feedback) AND Brightspace\nThree Weeks: don’t wait until the very end\nShould be less demanding than MP #01 and MP#02\n\nLots of little files. No big files!\nMaps and election retrodiction\n\n\n\nPay attention to the rubric"
  },
  {
    "objectID": "slides/slides09.html#course-project",
    "href": "slides/slides09.html#course-project",
    "title": "STA 9750 - Week 9",
    "section": "STA 9750 Course Project",
    "text": "STA 9750 Course Project\nProposal feedback sent by email this afternoon. Please contact if not received.\nNext Week: Mid-Term Check-In Presentations\n\n6 minutes\nLocking in on specific questions\nEngagement with existing literature"
  },
  {
    "objectID": "slides/slides09.html#upcoming-mini-projects",
    "href": "slides/slides09.html#upcoming-mini-projects",
    "title": "STA 9750 - Week 9",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#04: Something financial\n\nAny requests?"
  },
  {
    "objectID": "slides/slides09.html#pre-assignments",
    "href": "slides/slides09.html#pre-assignments",
    "title": "STA 9750 - Week 9",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is November 13th\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "slides/slides09.html#course-support",
    "href": "slides/slides09.html#course-support",
    "title": "STA 9750 - Week 9",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(38\\) minute average response time)\n\n\nChange: MW Thursday Zoom OH will be 4:00pm to 5:00pm"
  },
  {
    "objectID": "slides/slides09.html#upcoming-week",
    "href": "slides/slides09.html#upcoming-week",
    "title": "STA 9750 - Week 9",
    "section": "Upcoming Week",
    "text": "Upcoming Week\n\nMid-Project Check-Ins"
  },
  {
    "objectID": "slides/slides09.html#faq-file-import",
    "href": "slides/slides09.html#faq-file-import",
    "title": "STA 9750 - Week 9",
    "section": "FAQ: File Import",
    "text": "FAQ: File Import\nNo FAQs today. (New topic - data import)"
  },
  {
    "objectID": "slides/slides09.html#warm-up",
    "href": "slides/slides09.html#warm-up",
    "title": "STA 9750 - Week 9",
    "section": "Warm-Up",
    "text": "Warm-Up\nFrom FiveThirtyEight"
  },
  {
    "objectID": "slides/slides09.html#breakout-rooms",
    "href": "slides/slides09.html#breakout-rooms",
    "title": "STA 9750 - Week 9",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nRoom\nTeam\n\nRoom\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "slides/slides09.html#warm-up-1",
    "href": "slides/slides09.html#warm-up-1",
    "title": "STA 9750 - Week 9",
    "section": "Warm-Up",
    "text": "Warm-Up\nData can be found at https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/candy-power-ranking/candy-data.csv\nRead into R (readr::read_csv) and make 3 plots:\n\nDo people like more sugary candy?\nDo people like more expensive candy?\nOpen-Ended"
  },
  {
    "objectID": "slides/slides09.html#getting-data-into-r",
    "href": "slides/slides09.html#getting-data-into-r",
    "title": "STA 9750 - Week 9",
    "section": "Getting Data into R",
    "text": "Getting Data into R\nTwo topics:\n\nHow internet data transfer actually works\nHow to handle non-rectangular data formats\n\n🎃End a bit early for Halloween🎃"
  },
  {
    "objectID": "slides/slides09.html#urls",
    "href": "slides/slides09.html#urls",
    "title": "STA 9750 - Week 9",
    "section": "URLs",
    "text": "URLs\nFrom abstrax.io"
  },
  {
    "objectID": "slides/slides09.html#json",
    "href": "slides/slides09.html#json",
    "title": "STA 9750 - Week 9",
    "section": "JSON",
    "text": "JSON\nJSON:\n\nShort for JavaScript Object Notation\nPopular plain-text representation for hierarchical data.\nCloser to Python objects (dicts of dicts of dicts) than R data.frames\nWidely used for Application Programming Interfaces (APIs)"
  },
  {
    "objectID": "slides/slides09.html#json-1",
    "href": "slides/slides09.html#json-1",
    "title": "STA 9750 - Week 9",
    "section": "JSON",
    "text": "JSON\nExample:\n{\n    \"data\": {\n        \"id\": 27992,\n        \"title\": \"A Sunday on La Grande Jatte — 1884\",\n        \"image_id\": \"1adf2696-8489-499b-cad2-821d7fde4b33\"\n    },\n    \"config\": {\n        \"iiif_url\": \"https://www.artic.edu/iiif/2\",\n    }\n}"
  },
  {
    "objectID": "slides/slides09.html#json-2",
    "href": "slides/slides09.html#json-2",
    "title": "STA 9750 - Week 9",
    "section": "JSON",
    "text": "JSON\nRead JSON in R with jsonlite package (alternatives exist)\n\nlibrary(jsonlite)\n# A JSON array of primitives\njson &lt;- '[\"Mario\", \"Peach\", null, \"Bowser\"]'\n\n# Simplifies into an atomic vector\nfromJSON(json)\n\n[1] \"Mario\"  \"Peach\"  NA       \"Bowser\""
  },
  {
    "objectID": "slides/slides09.html#json-3",
    "href": "slides/slides09.html#json-3",
    "title": "STA 9750 - Week 9",
    "section": "JSON",
    "text": "JSON\n\njson &lt;-\n'[\n  {\"Name\" : \"Mario\", \"Age\" : 32, \"Occupation\" : \"Plumber\"}, \n  {\"Name\" : \"Peach\", \"Age\" : 21, \"Occupation\" : \"Princess\"},\n  {},\n  {\"Name\" : \"Bowser\", \"Occupation\" : \"Koopa\"}\n]'\nmydf &lt;- fromJSON(json)\nmydf\n\n    Name Age Occupation\n1  Mario  32    Plumber\n2  Peach  21   Princess\n3   &lt;NA&gt;  NA       &lt;NA&gt;\n4 Bowser  NA      Koopa"
  },
  {
    "objectID": "slides/slides09.html#json-4",
    "href": "slides/slides09.html#json-4",
    "title": "STA 9750 - Week 9",
    "section": "JSON",
    "text": "JSON\n\nfromJSON(\"http://worldtimeapi.org/api/timezone/America/New_York\")[1:5]\n\n$utc_offset\n[1] \"-05:00\"\n\n$timezone\n[1] \"America/New_York\"\n\n$day_of_week\n[1] 0\n\n$day_of_year\n[1] 40\n\n$datetime\n[1] \"2025-02-09T10:17:23.031673-05:00\"\n\n\nCompare to browser access"
  },
  {
    "objectID": "slides/slides09.html#data-transfer-download.file",
    "href": "slides/slides09.html#data-transfer-download.file",
    "title": "STA 9750 - Week 9",
    "section": "Data Transfer: download.file",
    "text": "Data Transfer: download.file\n\nargs(download.file)\n\nfunction (url, destfile, method, quiet = FALSE, mode = \"w\", cacheOK = TRUE, \n    extra = getOption(\"download.file.extra\"), headers = NULL, \n    ...) \nNULL\n\n\nBasic file download capabilities:\n\nurl: source\ndestfile: where on your computer to store it\nmethod: what software to use in the background to download"
  },
  {
    "objectID": "slides/slides09.html#data-transfer-http",
    "href": "slides/slides09.html#data-transfer-http",
    "title": "STA 9750 - Week 9",
    "section": "Data Transfer: HTTP",
    "text": "Data Transfer: HTTP\nHTTP\n\nHyperText Transfer Protocol\nMost common (but not only) internet protocol\nAlso ftp, smtp, ssh, …\n\n“Low-level” mechanism of internet transfer\n\nMany R packages add a friendly UX\nhttr2 for low-level work (today)"
  },
  {
    "objectID": "slides/slides09.html#http",
    "href": "slides/slides09.html#http",
    "title": "STA 9750 - Week 9",
    "section": "HTTP",
    "text": "HTTP\n\nHTTP has two stages:\n\n\n\nRequest\n\nURL (Host + Path)\nMethod (VERB)\nHeaders\nContent\nCookies\n\n\n\n\n\nResponse\n\nStatus Code\nHeaders\nContent\n\n\n\n\nModern (easy) APIs put most of the behavior in the URL"
  },
  {
    "objectID": "slides/slides09.html#http-in-the-browser",
    "href": "slides/slides09.html#http-in-the-browser",
    "title": "STA 9750 - Week 9",
    "section": "HTTP in the Browser",
    "text": "HTTP in the Browser\nIn Firefox: Right-Click + Inspect\nIn Chrome: Right-Click + Developer Tools"
  },
  {
    "objectID": "slides/slides09.html#http-with-httr2",
    "href": "slides/slides09.html#http-with-httr2",
    "title": "STA 9750 - Week 9",
    "section": "HTTP with httr2",
    "text": "HTTP with httr2\nhttr2 (pronounced “hitter-2”) is low-level manipulation of HTTP.\n\nlibrary(httr2)\nrequest(example_url())\n\nPretty simple so far:\n\nexample_url() starts a tiny local web host\n127.0.0.1 is localhost\n\nLive Demo Time"
  },
  {
    "objectID": "slides/slides09.html#httr2-requests",
    "href": "slides/slides09.html#httr2-requests",
    "title": "STA 9750 - Week 9",
    "section": "httr2 Requests",
    "text": "httr2 Requests\nBuild a request:\n\nreq_method\nreq_body_*\nreq_cookies_set\nreq_auth_basic / req_oauth\n\nBehaviors:\n\nreq_cache\nreq_timeout\n\nExecution:\n\nreq_perform"
  },
  {
    "objectID": "slides/slides09.html#httr2-responses",
    "href": "slides/slides09.html#httr2-responses",
    "title": "STA 9750 - Week 9",
    "section": "httr2 Responses",
    "text": "httr2 Responses\nRequest status\n\nresp_status / resp_status_desc\n\nContent: - resp_header* - resp_body_*"
  },
  {
    "objectID": "slides/slides09.html#exercise",
    "href": "slides/slides09.html#exercise",
    "title": "STA 9750 - Week 9",
    "section": "Exercise",
    "text": "Exercise\nArt Institute of Chicago API"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 9750 - Basic Software Tools for Data Analysis",
    "section": "",
    "text": "Welcome to the course website for STA 9750 (Spring 2025)!\nSTA 9750 is an Introduction to R targeted at students in the MS in Business Analytics, MS in Statistics, and MS in Quantitative Methods programs.\nThis site hosts the unofficial Course Syllabus, Course Policies, and Course Learning Objectives.\nOfficial copies of these documents can be found on CUNY Brightspace. Course pre-assignments, labs, and mini-projects can also be found on this site.\nThis offering of STA 9750 will be taught in a mixture of the flipped-classroom and experiential-learning formats. Roughly, this means that most weeks, students will be asked to complete a small pre-assignment each week to introduce the core concept(s) covered in that week’s lecture. Each class period will be split between a brief lecture covering concepts in more detail and an extended lab activity designed to build familiarity and fluency with that week’s subject matter.\nThere are quite a few moving parts to this course, so this key dates file or the list of upcoming course activities below may be useful:\nA CSV file suitable for import into Google Calendar with all assignment deadlines can be found here.1\nInstructor: Michael Weylandt"
  },
  {
    "objectID": "objectives.html",
    "href": "objectives.html",
    "title": "STA 9750 - Course Learning Objectives",
    "section": "",
    "text": "This course provides an understanding of the principles and concepts of using computer tools for data analysis and visualization. Students will learn to use a scientific programming language (such as R) to import and export data from and into spreadsheets or other statistical software packages. They will gain experience in analyzing both quantitative and qualitative data, and statistical modelling techniques will be introduced. Written reports will prepare students for clear communication of their analysis in professional settings. This course is designed primarily for Masters’ students in statistics and quantitative methods and modeling (QMM), and those interested in carrying out sophisticated statistical analyses of data using statistical software."
  },
  {
    "objectID": "objectives.html#official-course-description",
    "href": "objectives.html#official-course-description",
    "title": "STA 9750 - Course Learning Objectives",
    "section": "",
    "text": "This course provides an understanding of the principles and concepts of using computer tools for data analysis and visualization. Students will learn to use a scientific programming language (such as R) to import and export data from and into spreadsheets or other statistical software packages. They will gain experience in analyzing both quantitative and qualitative data, and statistical modelling techniques will be introduced. Written reports will prepare students for clear communication of their analysis in professional settings. This course is designed primarily for Masters’ students in statistics and quantitative methods and modeling (QMM), and those interested in carrying out sophisticated statistical analyses of data using statistical software."
  },
  {
    "objectID": "objectives.html#course-learning-objectives",
    "href": "objectives.html#course-learning-objectives",
    "title": "STA 9750 - Course Learning Objectives",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nStudents successfully completing STA 9750 will be able to:\n\nEffectively communicate the results of data analyses.\nManipulate tabular data in R\nDevelop effective and compelling visualizations using standard statistical software\nManipulate ‘wild-caught’ data from web-based sources\nUse computational approaches to statistical inference\nDevelop novel analytical products to convey actionable insights.\n\nThe following course elements contribute to these goals:\n\nContribution of Course Elements to Learning Goals\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Goal 1\nLearning Goal 2\nLearning Goal 3\nLearning Goal 4\nLearning Goal 5\nLearning Goal 6\n\n\n\n\nMini Project #00\n✓\n\n\n\n\n\n\n\nMini Project #01\n✓\n✓\n\n\n\n\n\n\nMini Project #02\n✓\n✓\n✓\n\n\n\n\n\nMini Project #03\n✓\n✓\n✓\n✓\n\n\n\n\nMini Project #04\n✓\n✓\n✓\n✓\n✓\n\n\n\nCourse Project\n✓\n✓\n✓\n✓\n✓\n✓"
  },
  {
    "objectID": "objectives.html#program-learning-goals",
    "href": "objectives.html#program-learning-goals",
    "title": "STA 9750 - Course Learning Objectives",
    "section": "Program Learning Goals",
    "text": "Program Learning Goals\nThis course contributes to the program learning goals of several MS programs offered by the Zicklin School of Business.\n\nMS in Business Analytics\nThis course contributes to the following Program Learning Goals for the MS in Business Analytics:\n\nMSBA Program Learning Goals\n\n\n\n\n\n\n\nSTA 9750 Learning Goal\nMSBA Learning Goal\nDescription\n\n\n\n\n✓\nData Management\nStudents will be able to apply methods, tools, and software for acquiring, managing/storing, and accessing structured and unstructured data. Students will also demonstrate knowledge of the strategic uses of data.\n\n\n✓\nFoundational Statistical / Quantitative Skills\nStudents will be able to prepare data for statistical analysis, perform basic exploratory and descriptive analysis as well as employ foundational statistical techniques needed to analyze data.\n\n\n✓\nAdvanced Statistical/Quantitative Skills\nStudents will be able to build and interpret advanced predictive models. Students will be able to combine business rules and mathematical models to optimize business decisions from data.\n\n\n\nEthical Awareness\nStudents will be able to articulate an understanding of ethical issues in all phases of business analytics with particular emphasis on the new possibilities afforded by the emergence of big data.\n\n\n✓\nProfessional Communication\nStudents will be able to explain complex analytical models and their results orally and in writing to technical and non technical/lay audiences.\n\n\n✓\nKnowledge Integration\nStudents will be able to apply the three key types of analytics (descriptive, predictive, and prescriptive) in a business domain to add value to business decision-making.\n\n\n\n\n\nMS in Quantitative Methods & Modeling\nThis course contributes to the following Program Learning Goals for the MS in Quantitative Methods & Modeling:\n\nMSQMM Program Learning Goals\n\n\n\n\n\n\n\nSTA 9750 Learning Goal\nMSQMM Learning Goal\nDescription\n\n\n\n\n✓\nOperations Research & Mathematical Modeling\nStudents will be able to effectively model, evaluate, and solve quantitative (business) problems using quantitative modeling methods (e.g. deterministic and probabilistic operations research techniques).\n\n\n✓\nStatistics\nStudents will be able to correctly apply appropriate statistical methods when defining, solving, and analyzing problems.\n\n\n✓\nTechnology Competency\nStudents will be able to use current technological tools, including spreadsheets and specialized software, when solving problems.\n\n\n✓\nProfessional Communication\nStudents will be able to effectively communicate their problem solving methods and solutions to technical and non-technical audiences.\n\n\n\n\n\nMS in Statistics\nThis course contributes to the following Program Learning Goals for the MS in Statistics:\n\nMS Statistics Program Learning Goals\n\n\n\n\n\n\n\nSTA 9750 Learning Goal\nMS Stat Learning Goal\nDescription\n\n\n\n\n✓\nGeneral Statistical Competence\nStudents will be able to apply appropriate probability models and statistical techniques when analyzing problems from business and other fields.\n\n\n✓\nStatistical Practice\nStudents will become familiar with the standard tools of statistical practice for multiple regression, along with the tools of a subset of specialized statistical areas such as multivariate analysis, applied sampling, time series analysis, experimental design, data mining, categorical analysis, and/or stochastic processes.\n\n\n✓\nTechnology Competency\nStudents will learn to use one or more of the benchmark statistical software platforms, such as SAS or R."
  },
  {
    "objectID": "archive/AY-2024-FALL/resources.html",
    "href": "archive/AY-2024-FALL/resources.html",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "",
    "text": "R for Data Science (R4DS) is an excellent free textbook covering much of the material of this course.\nThe tidyverse packages used throughout this course have excellent documentation:\n\nreadr\ndplyr\ntidyr\nggplot2\nrvest\n\nThe quarto guide is particularly useful.\nStudents may also benefit from the Unofficial Solutions for R4DS, the Posit R Cheatsheets, Statistical Infernece via Data Science, and the book Elegrant Graphics for Data Analysis.\nThe book Happy Git with R is particularly useful for git usage. General git usage is also covered by the Git Book.\nThe book Veridical Data Science by Yu and Barter has lots of useful advice on applied data analytics that may help with the course project.\nStudents are encouraged to ask the instructor for additional resources as needed.\n\n\nSTA 9750 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nWritten and oral communication is an important element of this course.\nThe Baruch Writing Center offers free support to all Baruch students; students can meet with a professional writing consultant one-to-one (in person, in NVC 8-185, or online, by video, audio, and text-based chat) and in group workshops. Writing Center consultants will work collaboratively with you to deepen your writing and English language skills. At any step in the process, they’ll help you become a more independent, confident, and versatile writer.1\nBaruch’s Tools for Clear Speech program improves the pronunciation, fluency, and pragmatic abilities of English language learners and non-native English speakers at Baruch College. TfCS participants achieve more effective and intelligible communication, developing skills that empower them to succeed in their classrooms, careers, and beyond. TfCS offers a range of free face-to-face and online services with our professional Speech Consultants, including One-to-One Sessions, small-group Focused Skills Series sessions, large-group Overview Workshops, interview and career preparation, and weekly Conversation Hours.\n\n\n\nAll software used in this course is Free and Open-Source Software that can be installed on your personal machine without cost. Students will need to install, at a minimum,\n\nR\nrstudio Desktop Edition\nquarto\n\nThanks to the Binder project, we are also able to provide free virtual machines equipped with all course software pre-installed:\n\nRStudio\nCommand Line Access\n\nCUNY also provides a Windows-based RStudio virtual machine through Apporto.\nPlease note that these are transient instances and any work saved on these achines may be lost without warning."
  },
  {
    "objectID": "archive/AY-2024-FALL/resources.html#course-resources",
    "href": "archive/AY-2024-FALL/resources.html#course-resources",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "",
    "text": "R for Data Science (R4DS) is an excellent free textbook covering much of the material of this course.\nThe tidyverse packages used throughout this course have excellent documentation:\n\nreadr\ndplyr\ntidyr\nggplot2\nrvest\n\nThe quarto guide is particularly useful.\nStudents may also benefit from the Unofficial Solutions for R4DS, the Posit R Cheatsheets, Statistical Infernece via Data Science, and the book Elegrant Graphics for Data Analysis.\nThe book Happy Git with R is particularly useful for git usage. General git usage is also covered by the Git Book.\nThe book Veridical Data Science by Yu and Barter has lots of useful advice on applied data analytics that may help with the course project.\nStudents are encouraged to ask the instructor for additional resources as needed.\n\n\nSTA 9750 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nWritten and oral communication is an important element of this course.\nThe Baruch Writing Center offers free support to all Baruch students; students can meet with a professional writing consultant one-to-one (in person, in NVC 8-185, or online, by video, audio, and text-based chat) and in group workshops. Writing Center consultants will work collaboratively with you to deepen your writing and English language skills. At any step in the process, they’ll help you become a more independent, confident, and versatile writer.1\nBaruch’s Tools for Clear Speech program improves the pronunciation, fluency, and pragmatic abilities of English language learners and non-native English speakers at Baruch College. TfCS participants achieve more effective and intelligible communication, developing skills that empower them to succeed in their classrooms, careers, and beyond. TfCS offers a range of free face-to-face and online services with our professional Speech Consultants, including One-to-One Sessions, small-group Focused Skills Series sessions, large-group Overview Workshops, interview and career preparation, and weekly Conversation Hours.\n\n\n\nAll software used in this course is Free and Open-Source Software that can be installed on your personal machine without cost. Students will need to install, at a minimum,\n\nR\nrstudio Desktop Edition\nquarto\n\nThanks to the Binder project, we are also able to provide free virtual machines equipped with all course software pre-installed:\n\nRStudio\nCommand Line Access\n\nCUNY also provides a Windows-based RStudio virtual machine through Apporto.\nPlease note that these are transient instances and any work saved on these achines may be lost without warning."
  },
  {
    "objectID": "archive/AY-2024-FALL/resources.html#course-policies",
    "href": "archive/AY-2024-FALL/resources.html#course-policies",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic Integrity Policy\nI fully support CUNY’s Policy on Academic Integrity, which states, in part:\n\nAcademic dishonesty is prohibited in The City University of New York. Penalties for academic dishonesty include academic sanctions, such as failing or otherwise reduced grades, and/or disciplinary sanctions, including suspension or expulsion.\n\n\nAcademic integrity is at the core of a college or university education. Faculty assign essays, exams, quizzes, projects, and so on both to extend the learning done in the classroom and as a means of assessing that learning. When students violate the academic integrity policy (i.e., “cheat”), they are committing an act of theft that can cause real harm to themselves and others including, but not limited to, their classmates, their faculty, and the caregivers who may be funding their education. Academic dishonesty confers an unfair advantage over others, which undermines educational equity and fairness. Students who cheat place their college’s accreditation and their own future prospects in jeopardy.\n\nAcademic sanctions in this class will range from an F on the Assignment to an F in this Course. A report of suspected academic dishonesty will be sent to the Office of the Dean of Students.\nStudents are encouraged to contact the instructor with any questions or concerns related to matters of academic integrity.\n\n\nExternal Resources Use Policy\nFor the coding elements of this course, students are encouraged to use freely available online resources, including question-and-answer fora such as StackOverflow. You may also use AI-driven developer tools such as GitHub Co-Pilot. Paid services are not allowed. On each assignment, you will be asked to list external resources used on each assignment. You are ultimately responsible for the correctness of any submitted materials - ``the AI told me so’’ is not a valid defense.\nNote on ChatGPT and Related Large-Language Models: You may not use large-language models to complete any assignment in this course. Specifically, you may not use tools where you describe the course assignment in natural language and receive (pseudo-)code output. While these tools are powerful, and often surprisingly accurate, for this task, using them in this manner will undermine the learning objectives of this course.\nFor the written elements of this course (e.g. Project Final Report), standard academic expectations of attribution and citation are in place. This will be covered in more detail in the course project documents.\nStudents are highly encouraged to collaborate on homework assignments, but each student is required to individually and complete each assignment. If substantially identical assignments are submitted, the instructor may require each student to individually demonstrate their understanding of the material. Collaborators should be listed at the end of each submitted assignment along with a statement of contributions.\n\n\nUnexcused Abscence Policy\nAttendance is not required, but lecture recordings will not be provided. Students are responsible for the content of all sessions missed.\n\n\nLate Work Policy\nLate work will not be accepted except in extraordinary and unforeseeable circumstances. Students submitting late work should provide supporting documentation to the Office of the Dean of Students; ODS will provide the instructor with a letter authorizing late work submission as appropriate.\nAll assignment submission technology used in this course allows multiple submissions, so students are encouraged to submit early and often to avoid any technology troubles associated with late submission.\nNote that late work is allowed consistent with specific pre-arranged course accomodations as noted below."
  },
  {
    "objectID": "archive/AY-2024-FALL/resources.html#course-accomodations",
    "href": "archive/AY-2024-FALL/resources.html#course-accomodations",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "Course Accomodations",
    "text": "Course Accomodations\n\nDisability Services\nIt is CUNY policy to provide Accommodations and Academic Adjustments to students with disabilities.\nAny student who has a disability who may need accommodations in this class should register as early as possible with Student Disability Services. Your registration with Student Disability Services is confidential, and is not recorded on your Baruch Academic Record. SDS can be reached by email at disability.services@baruch.cuny.edu, by phone at 646-312-4590, or in person at NVC 2-272.\nPlease note that the instructor cannot provide accommodations unless requested by SDS.\n\n\nReligious Accomodations\nIt is CUNY policy to provide accommodations for students’ sincerely held religious beliefs. If a religious accommodation is requested, please contact the instructor at least two weeks in advance."
  },
  {
    "objectID": "archive/AY-2024-FALL/resources.html#care-resources-for-students1",
    "href": "archive/AY-2024-FALL/resources.html#care-resources-for-students1",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "Care Resources for Students2",
    "text": "Care Resources for Students2\nTake care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress.\nAll of us benefit from support during times of struggle. You are not alone. Asking for support sooner rather than later is often helpful.\nThis course is intended to be demanding, but not difficult. If you feel like you are struggling, please reach out sooner rather than later. Swimming long-distances in choppy waters builds strength: drowning doesn’t.\n\nMental Health Resources\nIf you or anyone you know experiences significant academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support.\nThe Baruch Counselling Center is here to help. You can visit them in person at 137 E 25th St, 9th floor or call them at 646-312-2155 during normal business hours; you can make an appointment online here. For more immediate support, please call NYC WELL (1-888-NYC-WELL or 1-888-692-9355).\nAsking for help is often difficult: consider reaching out to a friend, family, or a member of the faculty you trust for help getting connected to support that can help.\nIf you are worried about a friend or classmate, consider reaching out to the Baruch Campus Intervention Team.\n\n\nPhysical Health\nHealthy CUNY promotes well-being and a culture of health in order to foster the academic and life success of all CUNY students. They can connect you with a variety of campus- and community-based healthcare providers.\nBaruch Health Services provides students with a full range of clinical health services. Call 646-312-2040 or email StudentHealthCareCenter@baruch.cuny.edu to make an appointment.\n\n\nFood Security\nAll CUNY students have access to CUNY Food Pantries located throughout the five boroughs, thanks to the CUNY CARES program. CUNY CARES is also able to help qualifying students with SNAP (“Food Stamps”) enrollment.\nOn campus, you can also access the Bearcat Food Pantry.\n\nFinancial Security\nBaruch students experiencing heightened financial stress have access to Student Emergency Grants administered through the Office of the Dean of Students. Note that funds are also available for students experiencing immigration-related financial stress.\n\n\n\nImmigration Status\nCUNY Citizenship Now! provides confidential, high-quality immigration law services to all CUNY students.\nNote that Citizenship Now!’s primary Manhattan office is located in the Heights, not on the Baruch campus and that an appointment is strongly recommended. Call 646-664-9350 during standard business hours for more information or to make an appointment"
  },
  {
    "objectID": "archive/AY-2024-FALL/resources.html#footnotes",
    "href": "archive/AY-2024-FALL/resources.html#footnotes",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDescriptions of Baruch and CUNY resources adapted from program websites.↩︎\nLanguage adapted from Professor Ryan Tibshirani (UC Berkeley).↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/syllabus.html",
    "href": "archive/AY-2024-FALL/syllabus.html",
    "title": "STA 9750 - Course Syllabus",
    "section": "",
    "text": "Professor Michael Weylandt\nDepartment of Information Systems & Statistics\nZicklin School of Business\nBaruch College, CUNY"
  },
  {
    "objectID": "archive/AY-2024-FALL/syllabus.html#footnotes",
    "href": "archive/AY-2024-FALL/syllabus.html#footnotes",
    "title": "STA 9750 - Course Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTheoretically, this may result in scores equivalent to an A in an un-curved course receiving a lower grade in this course. In practice, the instructor will design course assessments to induce a range of scores and does not anticipate “down-curving” happening.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#mini-project-01",
    "href": "archive/AY-2024-FALL/slides/slides06.html#mini-project-01",
    "title": "STA 9750 - Week 6 Update",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\n👏 Thank you to everyone who took part in peer feedback! 👏\nCharles and I have marked all grades - expect on Brightspace soon.\n(Tech issues on our end…)\n\nAlso - let’s thank Charles for going through every post manually with reminders 👏"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#mini-project-01-1",
    "href": "archive/AY-2024-FALL/slides/slides06.html#mini-project-01-1",
    "title": "STA 9750 - Week 6 Update",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\nSome popular tips:\n\nThe gt makes very nice tables\nQuarto allows “code folding”: useful for hiding long boring code blocks\n\n\n\nShow the code\n1 + 1\n\n\n[1] 2\n\n\nYou can set this globally. You also want to keep echo: true."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#mini-project-02",
    "href": "archive/AY-2024-FALL/slides/slides06.html#mini-project-02",
    "title": "STA 9750 - Week 6 Update",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nMP#02 released - Hollywood Movies\n\nDue October 23rd\n\nGitHub post (used for peer feedback) AND Brightspace\n\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness\nUse what you learned from MP#01\nPre-processed data now available as well"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#upcoming-mini-projects",
    "href": "archive/AY-2024-FALL/slides/slides06.html#upcoming-mini-projects",
    "title": "STA 9750 - Week 6 Update",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#03: Political Analysis\nMP#04: Retirement Forecasting"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#pre-assignments",
    "href": "archive/AY-2024-FALL/slides/slides06.html#pre-assignments",
    "title": "STA 9750 - Week 6 Update",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is October 16th"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#course-support",
    "href": "archive/AY-2024-FALL/slides/slides06.html#course-support",
    "title": "STA 9750 - Week 6 Update",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Tuesday next week only\n\n\nAsynchronous\n\nPiazza (\\(&lt;30\\) minute average response time)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#upcoming-week",
    "href": "archive/AY-2024-FALL/slides/slides06.html#upcoming-week",
    "title": "STA 9750 - Week 6 Update",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nDue Wednesday at 11:45pm:\n\nPre-Assignment #07 (Brightspace)\n\nIntroduction to plotting with ggplot2\n\nProject Proposal Peer Feedback (Vocat)\n\nExpect back:\n\nMP#01 consolidated grades\nProject Proposal instructor feedback"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#course-project-presentations",
    "href": "archive/AY-2024-FALL/slides/slides06.html#course-project-presentations",
    "title": "STA 9750 - Week 6 Update",
    "section": "Course Project Presentations",
    "text": "Course Project Presentations\n\n\n\n\n\n\n\n\n\n\nTeam\nMembers\n\nTeam\nMembers\n\n\n\n\n1\nTD + VF + AG + EY + GZ\n\n6\nJA + CPV\n\n\n2\nNG + HM + TN + ZL(C)Y + YZ\n\n7\nCD + JCMB + PR + CKZF\n\n\n3\nAC + EL + CL + WP\n\n8\nHH(C)L + EO + GS + FW\n\n\n4\nHA + HA + SK + VL + DS\n\n9\nAA + LC + MAJ\n\n\n5\nCC + DM + AO + HS + MT\n\n10\nHB + SK + MK + CM + JV"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#project-proposal-presentations",
    "href": "archive/AY-2024-FALL/slides/slides06.html#project-proposal-presentations",
    "title": "STA 9750 - Week 6 Update",
    "section": "Project Proposal Presentations",
    "text": "Project Proposal Presentations\nOfficial Description\n\n6 minute presentation\nKey topics:\n\nAnimating Question\nTeam Roster\n\nAlso discuss: Possible specific questions, data sources, analytical plan, anticipated challenges"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#proposal-peer-feedback",
    "href": "archive/AY-2024-FALL/slides/slides06.html#proposal-peer-feedback",
    "title": "STA 9750 - Week 6 Update",
    "section": "Proposal Peer Feedback",
    "text": "Proposal Peer Feedback\nProject Peer Feedback Platform: Vocat\nAfter class:\n\nI upload presentation videos\nYou leave comments and scores on at least 1 presentation\n\nTry to comment on “under-discussed” presentations\nI will be looking for constructive feedback\nLess detail than mini-prorjects\n\n\nNew tool - learning experience for all. Bear with us."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides06.html#after-proposals",
    "href": "archive/AY-2024-FALL/slides/slides06.html#after-proposals",
    "title": "STA 9750 - Week 6 Update",
    "section": "After Proposals",
    "text": "After Proposals\nSpecial presentation on Baruch data resources"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#mini-project-04",
    "href": "archive/AY-2024-FALL/slides/slides13.html#mini-project-04",
    "title": "STA 9750 - Week 13",
    "section": "STA 9750 Mini-Project #04",
    "text": "STA 9750 Mini-Project #04\n\nCongratulations! Done with Mini-Projects!\nPeer reviews to be assigned by EoW"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#week-13-pre-assignment",
    "href": "archive/AY-2024-FALL/slides/slides13.html#week-13-pre-assignment",
    "title": "STA 9750 - Week 13",
    "section": "Week 13 Pre-Assignment",
    "text": "Week 13 Pre-Assignment\nDue at midnight tonight - take a moment to do it now if you haven’t already!\n\nReflection on course:\n\nHow far have you come?\nWhat have you learned?\nWhat was helpful? What was unhelpful?\n\n\nDone with pre-assignments!\nThis is in addition to the Baruch central course assesments."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#week-13-pre-assignment-1",
    "href": "archive/AY-2024-FALL/slides/slides13.html#week-13-pre-assignment-1",
    "title": "STA 9750 - Week 13",
    "section": "Week 13 Pre-Assignment",
    "text": "Week 13 Pre-Assignment\nUsed to improve future course offerings. This semester:\n\nAdded a second round of project feedback\n\nHelp students “scope” projects suitably\n\nMore applied analytics than programming exercises in HW\n\nOther programming resources already online;\nMany students have prior experience (Python, SQL)\nMore interest in Analytics than Software Engineering\n\nAdded GitHub and Portfolio Construction\n\nGive students evidence of skills to share with employers"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#grading",
    "href": "archive/AY-2024-FALL/slides/slides13.html#grading",
    "title": "STA 9750 - Week 13",
    "section": "Grading",
    "text": "Grading\nReturned:\n\nMid-Term Check-In Feedback\n\nWe owe you:\n\nMP#03 Grades in Brightspace"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#upcoming",
    "href": "archive/AY-2024-FALL/slides/slides13.html#upcoming",
    "title": "STA 9750 - Week 13",
    "section": "Upcoming",
    "text": "Upcoming\nDecember 12 - Last Day of Class!\n\nFinal Project Presentations\n\nReview the Rubric!\nNon-Technical Presentation - Think of yourself as a “consultant” asked by a client to investigate a topic."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#final-project-presentations",
    "href": "archive/AY-2024-FALL/slides/slides13.html#final-project-presentations",
    "title": "STA 9750 - Week 13",
    "section": "Final Project Presentations",
    "text": "Final Project Presentations\nExample Presentation Structure:\n\nMotivation\nHow your work relates to other previous work\nOverarching Question\nDiscussion of Data\n\nWhere? How good is it? Weaknesses / Limitations?\n\nSpecific Questions\n\nHow do they support overarching question?\nWhat did you do? What did you find?"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#final-project-presentations-1",
    "href": "archive/AY-2024-FALL/slides/slides13.html#final-project-presentations-1",
    "title": "STA 9750 - Week 13",
    "section": "Final Project Presentations",
    "text": "Final Project Presentations\nExample Presentation Structure (continued):\n\nIntegration of Findings\nMajor Conclusions\n\nHow do quantitative specific findings provide qualitative insights?\nWhat can you see be combining specific questions that you can’t see from a single specific question?\nIncluding Limitations of Current Study\n\nPotential Future Work"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#final-project-reports",
    "href": "archive/AY-2024-FALL/slides/slides13.html#final-project-reports",
    "title": "STA 9750 - Week 13",
    "section": "Final Project Reports",
    "text": "Final Project Reports\nGroup and Individual Reports\n\nSubmitted via GitHub and Brightspace\n\nDeadline extended to the day of the ‘final’\n\nRegistrar’s office has not released Final Exam schedule … grumble, grumble\nTentatively: December 19th\n\nWill confirm when exam schedule released\n\nNo late work accepted (I have to submit grades!)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#peer-assessment",
    "href": "archive/AY-2024-FALL/slides/slides13.html#peer-assessment",
    "title": "STA 9750 - Week 13",
    "section": "Peer Assessment",
    "text": "Peer Assessment\nOn Brightspace, I have opened an additional quiz for peer evaluation of your teammates.\n\n8 questions: scale of 1 (bad) to 3 (great)\n\nPlease submit a copy for each of your teammates.\n\nBrightspace set to allow multiple submissions.\nDue on same day as reports\n\n\nIf you don’t submit these, you will receive a 0 for your peer evaluations\nNo late work accepted (I have to submit grades!)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#final-project-grading",
    "href": "archive/AY-2024-FALL/slides/slides13.html#final-project-grading",
    "title": "STA 9750 - Week 13",
    "section": "Final Project Grading",
    "text": "Final Project Grading\nRubric is set high to give me flexibility to reward teams that take on big challenges\nHard rubric =&gt; Grades are curved generously\n\nMultiple paths to success\nIf your problem is “easy” on an element (data import in particular), that’s great! Don’t spend the effort over-complicating things. Effort is better spent elsewhere"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#agenda",
    "href": "archive/AY-2024-FALL/slides/slides13.html#agenda",
    "title": "STA 9750 - Week 13",
    "section": "Agenda",
    "text": "Agenda\n\nPredictive Modeling with tidymodels\n\nAdapted from (Case Study)[https://www.tidymodels.org/start/case-study/]"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#breakout-rooms",
    "href": "archive/AY-2024-FALL/slides/slides13.html#breakout-rooms",
    "title": "STA 9750 - Week 13",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nOrder\nTeam\n\nOrder\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#tidymodels",
    "href": "archive/AY-2024-FALL/slides/slides13.html#tidymodels",
    "title": "STA 9750 - Week 13",
    "section": "tidymodels",
    "text": "tidymodels\nStrength of R:\n\nThousands of authors contributing packages to CRAN\n\n\nWeakness of R:\n\nThousands of authors contributing slightly incompatible packages to CRAN\n\n\n\nNo two modeling packages have exactly the same API. Makes changing between interfaces cumbersome"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#tidymodels-1",
    "href": "archive/AY-2024-FALL/slides/slides13.html#tidymodels-1",
    "title": "STA 9750 - Week 13",
    "section": "tidymodels",
    "text": "tidymodels\ntidymodels attemps to provide a uniform interface to a wide variety of predictive Machine Learning tools\nAdvantages:\n\nEasy to swap out different algorithms to find the best\n\nDisadvantages:\n\nHarder to take advantage of the strengths of each approach\n\n\nI have dedicated my academic life to the differences in these methods, but 99% of the time, “black-box” prediction is good enough. In STA 9890, we get into the weeds - not here."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#ml-vs-statistical-pipelines",
    "href": "archive/AY-2024-FALL/slides/slides13.html#ml-vs-statistical-pipelines",
    "title": "STA 9750 - Week 13",
    "section": "ML vs Statistical Pipelines",
    "text": "ML vs Statistical Pipelines\nStatistics / Data Science:\n\nFind the model that fits the data best\nModel should capture all important data features\nInterpretability\nHistory: Grounded in lab sciences where experiments are expensive and data is limited"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#ml-vs-statistical-pipelines-1",
    "href": "archive/AY-2024-FALL/slides/slides13.html#ml-vs-statistical-pipelines-1",
    "title": "STA 9750 - Week 13",
    "section": "ML vs Statistical Pipelines",
    "text": "ML vs Statistical Pipelines\nMachine Learning:\n\nFind the model that predicts the data best\nNo “perfect” model - just the best one we’ve found so far\nBlack-box techniques are great, if effective\nHistory: Silicon Valley “at scale”\n\nValidation based on of-of-sample or test predictions"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#validating-predictive-power",
    "href": "archive/AY-2024-FALL/slides/slides13.html#validating-predictive-power",
    "title": "STA 9750 - Week 13",
    "section": "Validating Predictive Power",
    "text": "Validating Predictive Power\nHow to check whether a model predicts well?\n\nNeed more data! But where to get more data?\n\nActually get more data (hard, expensive, slow)\nSplit data into parts - test/training split\nCross-Validation\nResampling\n\n\n\nToday, we’ll primarily use a combination: Test/Train split & Cross-Validation!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#cross-validation",
    "href": "archive/AY-2024-FALL/slides/slides13.html#cross-validation",
    "title": "STA 9750 - Week 13",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nCross-Validation is done on the estimator, not the fitted algorithm"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#tidymodels-2",
    "href": "archive/AY-2024-FALL/slides/slides13.html#tidymodels-2",
    "title": "STA 9750 - Week 13",
    "section": "tidymodels",
    "text": "tidymodels\ntidymodels workflow:\n\nInitial Split\nPre-Process\nFit (many) models\nSelect best\nRefit\nTest Set Assessment\n\ntidymodels is very punny, so a bit hard to tell which step is which…"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#initial-split",
    "href": "archive/AY-2024-FALL/slides/slides13.html#initial-split",
    "title": "STA 9750 - Week 13",
    "section": "Initial Split",
    "text": "Initial Split\n\n# Stratified sampling to ensure balance\nsplits      &lt;- initial_split(hotels, \n                             strata = children)\n\nhotel_train &lt;- training(splits)\nhotel_test  &lt;- testing(splits)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#pre-process",
    "href": "archive/AY-2024-FALL/slides/slides13.html#pre-process",
    "title": "STA 9750 - Week 13",
    "section": "Pre-Process",
    "text": "Pre-Process\n\nholidays &lt;- c(\"AllSouls\", \"AshWednesday\", \"ChristmasEve\", \"Easter\", \n              \"ChristmasDay\", \"GoodFriday\", \"NewYearsDay\", \"PalmSunday\")\n\nrecipe &lt;- \n  recipe(children ~ ., data = hotel_other) |&gt; \n  step_date(arrival_date) |&gt; \n  step_holiday(arrival_date, holidays = holidays) |&gt; \n  step_rm(arrival_date) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#fit-models",
    "href": "archive/AY-2024-FALL/slides/slides13.html#fit-models",
    "title": "STA 9750 - Week 13",
    "section": "Fit Models",
    "text": "Fit Models\n\nlr_model &lt;- \n  logistic_reg(penalty = tune(), mixture = 1) |&gt; \n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#select-best",
    "href": "archive/AY-2024-FALL/slides/slides13.html#select-best",
    "title": "STA 9750 - Week 13",
    "section": "Select Best",
    "text": "Select Best\nFind a grid of parameters\n\nlr_reg_grid &lt;- data.frame(penalty = 10^seq(-4, -1, length.out = 30))\n\nPerform CV splits:\n\nlr_folds &lt;- vfold_cv(hotel_train, v = 5)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#select-best-1",
    "href": "archive/AY-2024-FALL/slides/slides13.html#select-best-1",
    "title": "STA 9750 - Week 13",
    "section": "Select Best",
    "text": "Select Best\nDefine a workflow:\n\nlr_workflow &lt;-  \n  workflow() |&gt; \n  add_model(lr_mod) |&gt; \n  add_recipe(lr_recipe)\n\nFit workflow to a grid of parameters:\n\nlr_results &lt;- \n  lr_workflow |&gt; \n  tune_grid(grid = lr_reg_grid,\n            control = control_grid(save_pred = TRUE),\n            resamples = lr_folds,\n            metrics = metric_set(roc_auc))"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#select-best-2",
    "href": "archive/AY-2024-FALL/slides/slides13.html#select-best-2",
    "title": "STA 9750 - Week 13",
    "section": "Select Best",
    "text": "Select Best\nVisual examination\n\nlr_results |&gt; \n  collect_metrics() |&gt; \n  ggplot(aes(x = penalty, y = mean)) + \n  geom_point() + \n  geom_line() + \n  ylab(\"Area under the ROC Curve\") +\n  scale_x_log10(labels = scales::label_number())"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#select-best-3",
    "href": "archive/AY-2024-FALL/slides/slides13.html#select-best-3",
    "title": "STA 9750 - Week 13",
    "section": "Select Best",
    "text": "Select Best\n\nlr_results |&gt; show_best()\nlr_best &lt;- lr_results |&gt; select_best()\n\nlr_best_fit &lt;- lr_results |&gt; fit_best()"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#refit",
    "href": "archive/AY-2024-FALL/slides/slides13.html#refit",
    "title": "STA 9750 - Week 13",
    "section": "Refit",
    "text": "Refit\n\nlr_best_fit &lt;- lr_results |&gt; fit_best()"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#test-set-assessment",
    "href": "archive/AY-2024-FALL/slides/slides13.html#test-set-assessment",
    "title": "STA 9750 - Week 13",
    "section": "Test Set Assessment",
    "text": "Test Set Assessment\n\npredict(lr_best_fit, hotel_test)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#exercise",
    "href": "archive/AY-2024-FALL/slides/slides13.html#exercise",
    "title": "STA 9750 - Week 13",
    "section": "Exercise",
    "text": "Exercise\nWork through the random forest components of https://www.tidymodels.org/start/case-study\nYou’ll need to work through the data import elements as well"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#other-tidymodels-tools",
    "href": "archive/AY-2024-FALL/slides/slides13.html#other-tidymodels-tools",
    "title": "STA 9750 - Week 13",
    "section": "Other tidymodels tools",
    "text": "Other tidymodels tools\n\nModel Stacking\nProbabilistic Predictions\nUncertainty Bounds (Conformal Inference)\nMultilevel (Mixed-Effect) Models\nFairness Audits"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides13.html#more-reading",
    "href": "archive/AY-2024-FALL/slides/slides13.html#more-reading",
    "title": "STA 9750 - Week 13",
    "section": "More Reading",
    "text": "More Reading\nhttps://www.tidymodels.org/start/"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#mini-project-03",
    "href": "archive/AY-2024-FALL/slides/slides11.html#mini-project-03",
    "title": "STA 9750 - Week 11",
    "section": "STA 9750 Mini-Project #03",
    "text": "STA 9750 Mini-Project #03\nI spot-checked several results - look fantastic!\n\nFewer git questions - folks getting the hang of things!\nPeer feedback assigned via GitHub\n\nYou give comments on other students’ repos\nDue in 1 week\n\n\nPay attention to the rubric"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#mini-project-04",
    "href": "archive/AY-2024-FALL/slides/slides11.html#mini-project-04",
    "title": "STA 9750 - Week 11",
    "section": "STA 9750 Mini-Project #04",
    "text": "STA 9750 Mini-Project #04\nMP#04 released today\n\nDue 2024-12-04 (\\(\\approx\\) 3 weeks)\nTopic: financial modeling\n\nComparison of two retirement plans\nHistorical data + Monte Carlo (“bootstrapping”)\n\nFormat:\n\nDecision Analytics - Play the role of financial advisor\nGitHub post AND Brightspace submission"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#week-11-pre-assignment",
    "href": "archive/AY-2024-FALL/slides/slides11.html#week-11-pre-assignment",
    "title": "STA 9750 - Week 11",
    "section": "Week 11 Pre-Assignment",
    "text": "Week 11 Pre-Assignment\nDue at midnight tonight - take a moment to do it now if you haven’t already!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#pre-assignments",
    "href": "archive/AY-2024-FALL/slides/slides11.html#pre-assignments",
    "title": "STA 9750 - Week 11",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is November 20th\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#grading",
    "href": "archive/AY-2024-FALL/slides/slides11.html#grading",
    "title": "STA 9750 - Week 11",
    "section": "Grading",
    "text": "Grading\nWe owe you:\n\nMP#02 final average (Need to upload to BS)\nMP#02 peer meta-review (Need to upload to BS)\nPosting videos to Vocat (Need to do video splitting)\nMid-Term Check-In Feedback"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#course-support",
    "href": "archive/AY-2024-FALL/slides/slides11.html#course-support",
    "title": "STA 9750 - Week 11",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(38\\) minute average response time)\n\n\nChange: MW Thursday Zoom OH now 4:00pm to 5:00pm"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#upcoming",
    "href": "archive/AY-2024-FALL/slides/slides11.html#upcoming",
    "title": "STA 9750 - Week 11",
    "section": "Upcoming",
    "text": "Upcoming\nNov 20:\n\nMP#03 Peer Feedback\nPre Assignment\n\nNov 27 - Thanksgiving Holiday (No Class on Nov 28)\n\nCheck-In Peer Feedback (Vocat)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#faq-html-vs-css",
    "href": "archive/AY-2024-FALL/slides/slides11.html#faq-html-vs-css",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: HTML vs CSS",
    "text": "FAQ: HTML vs CSS\n\nWhat is the difference between HTML and CSS?\n\nHTML is substance; CSS is style\nDistinction can be a bit blurry & CSS can live “inside” HTML\nExample"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#faq-a-in-selectorgadget",
    "href": "archive/AY-2024-FALL/slides/slides11.html#faq-a-in-selectorgadget",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: a in SelectorGadget",
    "text": "FAQ: a in SelectorGadget\n\nWhy does [SelectorGadget] display “a” in the selector when selecting a web link?\n\na is for anchor.\nConfusingly, anchors are both links and destinations.\nAnchors can reference:\n\nAnother page (http://URL)\nA particular part of another page (http://URL#place)\nA particular part of the same page (#place)\n\nQuarto supports cross-linking with anchors"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#faq-selectorgadget---multiple-clicks",
    "href": "archive/AY-2024-FALL/slides/slides11.html#faq-selectorgadget---multiple-clicks",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: SelectorGadget - Multiple Clicks",
    "text": "FAQ: SelectorGadget - Multiple Clicks\n\nWhy does SelectorGadget go “unique” when I click multiple elements of interest?\n\nCan’t find a common structure:\n\nTypically a problem within lists or common element types"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#faq-selectors---nesting",
    "href": "archive/AY-2024-FALL/slides/slides11.html#faq-selectors---nesting",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: Selectors - Nesting",
    "text": "FAQ: Selectors - Nesting\n\nHow to avoid unwanted elements such as headers or sidebars, focusing only on the main content I need?\n\nNest your selectors!\nthing1 thing2 will select only thing2s inside a thing1\nStarWars page\n\nTry main h2"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#faq-relationship-to-markdown",
    "href": "archive/AY-2024-FALL/slides/slides11.html#faq-relationship-to-markdown",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: Relationship to Markdown",
    "text": "FAQ: Relationship to Markdown\n\nIs [HTML] similar to Markdown ?\n\nMarkdown is an easier way to write (a subset of) HTML\nName is a bad joke: Markup (M in HTML) vs Markdown\nHTML can (theoretically) do more, but painful to write by hand"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#faq-messy-html",
    "href": "archive/AY-2024-FALL/slides/slides11.html#faq-messy-html",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: Messy HTML",
    "text": "FAQ: Messy HTML\n\nHow can we target data with CSS Selectors in messy HTML?\n\nPain and suffering - depends how messy.\nWorst case: a little bit of HTML selection + text processing (next week)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#selected-peer-advice",
    "href": "archive/AY-2024-FALL/slides/slides11.html#selected-peer-advice",
    "title": "STA 9750 - Week 11",
    "section": "Selected Peer Advice:",
    "text": "Selected Peer Advice:\n\nMore effort into writing, motivation, and formatting (several)\nBe sure to push image files in addition to qmd\nCode-folding!\nLearning style for more experienced peers\nMore charts\nsetNames to improve table formatting. (MW: I prefer rename YMMV)\nTable of contents\nBe selective in outputs"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#my-advice",
    "href": "archive/AY-2024-FALL/slides/slides11.html#my-advice",
    "title": "STA 9750 - Week 11",
    "section": "My Advice",
    "text": "My Advice\n\nMost effort \\(\\neq\\) most important\nReader knows less and cares less than you think\nSummary / Abstract goes a long way\nCross-linking useful\nUse suggested file names\n“What have you tried?”\nHelp me help you\nPrint less"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#agenda",
    "href": "archive/AY-2024-FALL/slides/slides11.html#agenda",
    "title": "STA 9750 - Week 11",
    "section": "Agenda",
    "text": "Agenda\n\nReview of HTML / CSS Structure\nPre-Assignment Examples\n\nStar Wars\nCUNY Table\nBaruch GPS\n\nExercise\n\nCUNY Map\nCocktails (Part 1)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#html-review",
    "href": "archive/AY-2024-FALL/slides/slides11.html#html-review",
    "title": "STA 9750 - Week 11",
    "section": "HTML Review",
    "text": "HTML Review\n\nHTML Structure\nCSS Selectors (SelectorGadget)\nIntroduction rvest"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#pre-assignment-exercises",
    "href": "archive/AY-2024-FALL/slides/slides11.html#pre-assignment-exercises",
    "title": "STA 9750 - Week 11",
    "section": "Pre-Assignment Exercises",
    "text": "Pre-Assignment Exercises\n\nStar Wars\n\n\nmain h2\n\n\n\nCUNY Table\n\n\n\ntable or tbody\n\n\n\nBaruch GPS\n\n\n\n.geo"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#exercise-1-cuny-map",
    "href": "archive/AY-2024-FALL/slides/slides11.html#exercise-1-cuny-map",
    "title": "STA 9750 - Week 11",
    "section": "Exercise 1: CUNY Map",
    "text": "Exercise 1: CUNY Map\nRecall Lab 1. Goal: extend map to all CUNYs\nSteps:\n\nRead CUNY table and extract links\nFollow links and pull coordinates\n\nTo read geo class, use this:\n\n\n\nCOORDS &lt;- html_element(\".geo\") |&gt; html_text() |&gt; str_split_1(\";\")\nLAT &lt;- as.numeric(COORDS[1])\nLON &lt;- as.numeric(COORDS[2])\n\n\nAdapt Lab 1 leaflet to show all locations"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#breakout-rooms",
    "href": "archive/AY-2024-FALL/slides/slides11.html#breakout-rooms",
    "title": "STA 9750 - Week 11",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nOrder\nTeam\n\nOrder\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides11.html#exercise-2-cocktails",
    "href": "archive/AY-2024-FALL/slides/slides11.html#exercise-2-cocktails",
    "title": "STA 9750 - Week 11",
    "section": "Exercise 2: Cocktails",
    "text": "Exercise 2: Cocktails\nGoal: create a cocktail data frame from Hadley’s Recipies\nToday: - How to find them all? - How to extract individual recepies? - How to pull items from each recipie?\nNext time: - How to convert text to numeric values + column info - Data wrangling"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#mini-project-00",
    "href": "archive/AY-2024-FALL/slides/slides04.html#mini-project-00",
    "title": "STA 9750 - Week 4 Update",
    "section": "STA 9750 Mini-Project #00",
    "text": "STA 9750 Mini-Project #00\nThank you to those of you who provided peer feedback!\n(Over 75% of the class reported receiving useful peer feedback.)\n\nInstructor’s Note: For graded MPs #01-04, be a bit more direct in peer feedback. Goal is to help your peers improve: constructive criticism.\n\n\nA few of you still haven’t completed MP#00. Too late for peer feedback, but you need to get this done in order to submit MP#01.\nNo late work accepted on graded MPs."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#mini-project-01",
    "href": "archive/AY-2024-FALL/slides/slides04.html#mini-project-01",
    "title": "STA 9750 - Week 4 Update",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\nMP#01 released - Transit System Financials\n\nDue September 25th\n\nGitHub post (used for peer feedback) AND Brightspace\n\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness, not necessarily correctness"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#mp01-corrections",
    "href": "archive/AY-2024-FALL/slides/slides04.html#mp01-corrections",
    "title": "STA 9750 - Week 4 Update",
    "section": "MP#01 Corrections",
    "text": "MP#01 Corrections\nThanks to EL and JA for finding two mistakes in MP statement:\n\nApples/Oranges problem in “longest average trip” (JA)\nData cleaning problem in FARES table (EL)\n\nWill fix after class:\n\nSkip longest average trip question\nBetter instructor-provided code for FARES table"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#upcoming-mini-projects",
    "href": "archive/AY-2024-FALL/slides/slides04.html#upcoming-mini-projects",
    "title": "STA 9750 - Week 4 Update",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nMP#02 assigned next week:\n\nHollywood Development Executive Case Study\nMP#03: Political Analysis (tentative)\nMP#04: Retirement Forecasting (tentative)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#pre-assignments",
    "href": "archive/AY-2024-FALL/slides/slides04.html#pre-assignments",
    "title": "STA 9750 - Week 4 Update",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades.\n\nI have to manually change to completion grading."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#course-project",
    "href": "archive/AY-2024-FALL/slides/slides04.html#course-project",
    "title": "STA 9750 - Week 4 Update",
    "section": "Course Project",
    "text": "Course Project\n3 teams already formed!\n\nBreakout rooms in teams\n\nTeam/Room 1: GZ + VF + EY + AG + TD\nTeam/Room 2: YZ + HM + TN + NG\nTeam/Room 3: SK + HA + DS\n\nAll team commitments due 2024-10-02"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#graduate-teaching-assistant-gta",
    "href": "archive/AY-2024-FALL/slides/slides04.html#graduate-teaching-assistant-gta",
    "title": "STA 9750 - Week 4 Update",
    "section": "Graduate Teaching Assistant (GTA)",
    "text": "Graduate Teaching Assistant (GTA)\n\nCharles Ramirez\nTwice Weekly Office Hours (Zoom - Links of Brightspace)\n\nTuesdays 4-5pm\nFridays 12-1pm\n\nWill also help coordinate peer feedback (GitHub), Piazza responses, etc.\nExcellent resource for course project advice!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#upcoming-week",
    "href": "archive/AY-2024-FALL/slides/slides04.html#upcoming-week",
    "title": "STA 9750 - Week 4 Update",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nNext Wednesday at 11:45pm:\n\nNext Pre-Assignment\nMP#01 Initial Submission due"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#faq-select-",
    "href": "archive/AY-2024-FALL/slides/slides04.html#faq-select-",
    "title": "STA 9750 - Week 4 Update",
    "section": "FAQ: select(-)",
    "text": "FAQ: select(-)\ndata |&gt; select(colname) keeps colname, dropping everything else\ndata |&gt; select(-colname) drops colname, keeping everything else\nDropping is mainly useful for\n\nPresentation (removing unwanted columns)\nAdvanced:\n\nOperations across columns"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#faq-filter-vs-group_by",
    "href": "archive/AY-2024-FALL/slides/slides04.html#faq-filter-vs-group_by",
    "title": "STA 9750 - Week 4 Update",
    "section": "FAQ: filter vs group_by",
    "text": "FAQ: filter vs group_by\ngroup_by is an adverb. On its own, it does nothing; it changes the behavior of later functionality.\n\npenguins |&gt; drop_na() |&gt; print(n=2)\n\n# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n# ℹ 331 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\npenguins |&gt; drop_na() |&gt; group_by(species) |&gt; print(n=2)\n\n# A tibble: 333 × 8\n# Groups:   species [3]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n# ℹ 331 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#faq-order-of-group_by",
    "href": "archive/AY-2024-FALL/slides/slides04.html#faq-order-of-group_by",
    "title": "STA 9750 - Week 4 Update",
    "section": "FAQ: Order of group_by",
    "text": "FAQ: Order of group_by\n\nNo change to first “grouped” operations\nChange in grouping structure of result\nLast group “removed” by summarize\nNo impact on grouped operations performed by mutate or filter"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#faq-ungroup",
    "href": "archive/AY-2024-FALL/slides/slides04.html#faq-ungroup",
    "title": "STA 9750 - Week 4 Update",
    "section": "FAQ: ungroup",
    "text": "FAQ: ungroup\n\nRemove all grouping structure\nDefensive to keep group structure from “propogating” unwantedly\n\n\nsum_penguins &lt;- penguins |&gt; \n    group_by(sex, species) |&gt; \n    summarize(mbmg = mean(body_mass_g))\n\n... # Lots of code \n\nsum_penguins |&gt; filter(mbmg == max(mbmg)) # Still grouped!!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#faq-named-arguments-in-mutate-and-summarize",
    "href": "archive/AY-2024-FALL/slides/slides04.html#faq-named-arguments-in-mutate-and-summarize",
    "title": "STA 9750 - Week 4 Update",
    "section": "FAQ: Named Arguments in mutate and summarize",
    "text": "FAQ: Named Arguments in mutate and summarize\nmutate and summarize create new columns:\n\nmutate creates “one-to-one”\nsummarize creates “one-per-group”"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#faq-pipe-syntax",
    "href": "archive/AY-2024-FALL/slides/slides04.html#faq-pipe-syntax",
    "title": "STA 9750 - Week 4 Update",
    "section": "FAQ: Pipe Syntax",
    "text": "FAQ: Pipe Syntax\nPipe syntax (|&gt;) is “syntactic sugar”\nJust makes code easier to read:\n\npenguins |&gt; group_by(species) |&gt; summarize(n_species = n())\n# vs\nsummarize(group_by(penguins, species), n_species=n())\n\nExactly the same execution: improved UX"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#faq-assignment-of-pipeline-results",
    "href": "archive/AY-2024-FALL/slides/slides04.html#faq-assignment-of-pipeline-results",
    "title": "STA 9750 - Week 4 Update",
    "section": "FAQ: Assignment of Pipeline Results",
    "text": "FAQ: Assignment of Pipeline Results\nWhen to start a pipeline with NAME &lt;-? Creating a new variable:\n\n\nData you intend to reuse\nAssignment operator ‘up front’ indicates important\nMy rules of thumb for names:\n\nNew names for “new complete thoughts” - whole summary in one pipeline\nOverwrite existing names for “like-for-like improvements” (USAGE &lt;- USAGE |&gt; code(...))\n\nRecoding variable names, fixing typos, etc.\nUse name repeatedly so downstream code picks up effects ‘for free’"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#faq-comparison-with-sql-and-pandas-python",
    "href": "archive/AY-2024-FALL/slides/slides04.html#faq-comparison-with-sql-and-pandas-python",
    "title": "STA 9750 - Week 4 Update",
    "section": "FAQ: Comparison with SQL and Pandas (Python)",
    "text": "FAQ: Comparison with SQL and Pandas (Python)\ndplyr is heavily inspired by SQL (standard query language for data bases)\n\nMW (2014): “Why bother? Can’t folks just use SQL”\n\npandas (in Python) inspired by R data.frame and SQL:\n\nA bit older than dplyr (cousins?)\n“New hotness” (polars) directly inspired by dplyr"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#faq-performance",
    "href": "archive/AY-2024-FALL/slides/slides04.html#faq-performance",
    "title": "STA 9750 - Week 4 Update",
    "section": "FAQ: Performance",
    "text": "FAQ: Performance\ndplyr is fast, but advanced options:\n\ndbplyr: translates dplyr syntax to SQL and executes in DB\ndtplyr: uses alternate data.table back-end (HFT)\n\nHard to have bad performance in single-table analysis\n\nDanger of accidentally creating ‘extra’ data in multi-table context\nWill discuss more next week"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides04.html#diving-deeper-with-group_by-filter-and-summarize",
    "href": "archive/AY-2024-FALL/slides/slides04.html#diving-deeper-with-group_by-filter-and-summarize",
    "title": "STA 9750 - Week 4 Update",
    "section": "Diving Deeper with group_by, filter, and summarize",
    "text": "Diving Deeper with group_by, filter, and summarize\nData Set: nycflights13\nExercise: Lab #04"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#mini-projects",
    "href": "archive/AY-2024-FALL/slides/slides14.html#mini-projects",
    "title": "STA 9750 - Week 14",
    "section": "STA 9750 Mini-Projects",
    "text": "STA 9750 Mini-Projects\n\nCongratulations! Done with all Mini-Projects!\n\nFantastic work this semester!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#week-13-pre-assignment",
    "href": "archive/AY-2024-FALL/slides/slides14.html#week-13-pre-assignment",
    "title": "STA 9750 - Week 14",
    "section": "Week 13 Pre-Assignment",
    "text": "Week 13 Pre-Assignment\n\nReflection on course:\n\nHow far have you come?\nWhat have you learned?\nWhat was helpful? What was unhelpful?\n\n\nThank you for your comments!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#grading",
    "href": "archive/AY-2024-FALL/slides/slides14.html#grading",
    "title": "STA 9750 - Week 14",
    "section": "Grading",
    "text": "Grading\nReturned:\n\nMid-Term Check-In Feedback\nMP#03 Grades\n\nWe owe you:\n\nMP#04 Grades"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#final-report---group",
    "href": "archive/AY-2024-FALL/slides/slides14.html#final-report---group",
    "title": "STA 9750 - Week 14",
    "section": "Final Report - Group",
    "text": "Final Report - Group\nNon-Technical Presentation - Think of yourself as a “consultant” asked by a client to investigate a topic."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#final-report---individual",
    "href": "archive/AY-2024-FALL/slides/slides14.html#final-report---individual",
    "title": "STA 9750 - Week 14",
    "section": "Final Report - Individual",
    "text": "Final Report - Individual\nTechnical Appendix to Group Report. Still requires writing, context, etc. but this is in particular where I’m going to look at your code."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#final-project-reports",
    "href": "archive/AY-2024-FALL/slides/slides14.html#final-project-reports",
    "title": "STA 9750 - Week 14",
    "section": "Final Project Reports",
    "text": "Final Project Reports\nGroup and Individual Reports\n\nSubmitted via GitHub and Brightspace\n\nDeadline extended to the day of the ‘final’, December 19th\nNo late work accepted (I have to submit grades!)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#peer-assessment",
    "href": "archive/AY-2024-FALL/slides/slides14.html#peer-assessment",
    "title": "STA 9750 - Week 14",
    "section": "Peer Assessment",
    "text": "Peer Assessment\nOn Brightspace, I have opened an additional quiz for peer evaluation of your teammates.\n\n8 questions: scale of 1 (bad) to 3 (great)\n\nPlease submit a copy for each of your teammates.\n\nBrightspace set to allow multiple submissions.\nDue on same day as reports\n\n\nIf you don’t submit these, you will receive a 0 for your peer evaluations\nNo late work accepted (I have to submit grades!)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#final-project-grading",
    "href": "archive/AY-2024-FALL/slides/slides14.html#final-project-grading",
    "title": "STA 9750 - Week 14",
    "section": "Final Project Grading",
    "text": "Final Project Grading\nRubric is set high to give me flexibility to reward teams that take on big challenges\nHard rubric =&gt; Grades are curved generously\n\nMultiple paths to success\nIf your problem is “easy” on an element (data import in particular), that’s great! Don’t spend the effort over-complicating things. Effort is better spent elsewhere"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#instructors-reflection",
    "href": "archive/AY-2024-FALL/slides/slides14.html#instructors-reflection",
    "title": "STA 9750 - Week 14",
    "section": "Instructor’s Reflection",
    "text": "Instructor’s Reflection"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#baruch-course-evaluations",
    "href": "archive/AY-2024-FALL/slides/slides14.html#baruch-course-evaluations",
    "title": "STA 9750 - Week 14",
    "section": "Baruch Course Evaluations",
    "text": "Baruch Course Evaluations\nShort Break to complete course evaluations\nhttp://baruch.cuny.edu/EVALS\nRoughly:\n\nScores used by central administration\nComments used by faculty and departments to improve offerings"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#hall-of-fame",
    "href": "archive/AY-2024-FALL/slides/slides14.html#hall-of-fame",
    "title": "STA 9750 - Week 14",
    "section": "Hall of Fame",
    "text": "Hall of Fame\nSubmit Hall of Fame Nominations"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#today",
    "href": "archive/AY-2024-FALL/slides/slides14.html#today",
    "title": "STA 9750 - Week 14",
    "section": "Today",
    "text": "Today\nDecember 12 - Last Day of Class!\n\nFinal Project Presentations"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides14.html#presentation-order",
    "href": "archive/AY-2024-FALL/slides/slides14.html#presentation-order",
    "title": "STA 9750 - Week 14",
    "section": "Presentation Order",
    "text": "Presentation Order\n\n\n\nOrder\nTeam\n\nOrder\nTeam\n\n\n\n\n10\nRat Pack\n\n7\nCa$h VZ\n\n\n8\nSubway Surfers\n\n6\nListing Legends\n\n\n2\nChart Toppers\n\n9\nTDSSG\n\n\n5\nMetro Mindset\n\n3\nBroker T’s\n\n\n1\nApple Watch\n\n4\nEVengers"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#week-2-update",
    "href": "archive/AY-2024-FALL/slides/slides02.html#week-2-update",
    "title": "STA 9750 - Week 2 Update",
    "section": "STA 9750 Week 2 Update",
    "text": "STA 9750 Week 2 Update\n\nWeekly feature\nBrief updates and reminders about course logistics\nSyllabus and Brightspace are binding\n\nIf something is left out of here, it still happens!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#course-administration-google-calendar-help",
    "href": "archive/AY-2024-FALL/slides/slides02.html#course-administration-google-calendar-help",
    "title": "STA 9750 - Week 2 Update",
    "section": "Course Administration: Google Calendar Help",
    "text": "Course Administration: Google Calendar Help\nThanks to WP (Piazza #15) for delving into Google Calendar formatting\nI’ve updated the course homepage to provide a CSV file with all course deadlines suitable for import to Google Calendar."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#course-administration-course-project-description-released",
    "href": "archive/AY-2024-FALL/slides/slides02.html#course-administration-course-project-description-released",
    "title": "STA 9750 - Week 2 Update",
    "section": "Course Administration: Course Project Description Released",
    "text": "Course Administration: Course Project Description Released\nCourse project description is now online\nDetailed discussion of:\n\nProject structure\nKey deadlines\nGrading rubrics\n\nFirst step: by October 2nd, email me your group members."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#course-enrollment",
    "href": "archive/AY-2024-FALL/slides/slides02.html#course-enrollment",
    "title": "STA 9750 - Week 2 Update",
    "section": "Course Enrollment",
    "text": "Course Enrollment\nTotal enrollment: 46 = 33 (STA) + 13 (OPR)\n\nCourses \\(\\geq\\) 46 are ‘doubles’ in Zicklin\n\nAllows additional instructional resources\nThanks to (unknown) #46\n\nI expect \\(\\approx 12\\) final project teams"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#graduate-teaching-assistant-gta",
    "href": "archive/AY-2024-FALL/slides/slides02.html#graduate-teaching-assistant-gta",
    "title": "STA 9750 - Week 2 Update",
    "section": "Graduate Teaching Assistant (GTA)",
    "text": "Graduate Teaching Assistant (GTA)\n\nGTA hiring in process - name TBA\nGTA to hold weekly virtual office hours (likely 2x)\n\nDoodle Poll to schedule"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#piazza",
    "href": "archive/AY-2024-FALL/slides/slides02.html#piazza",
    "title": "STA 9750 - Week 2 Update",
    "section": "Piazza",
    "text": "Piazza\n\n28 sign-ups: 18 still need to sign up\nGood discussion of GitHub security problems\n4 final project teammate searches underway\n\nGreat tool!\nSay what you’re interested in as a project topic to help coordination"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#pre-assignments",
    "href": "archive/AY-2024-FALL/slides/slides02.html#pre-assignments",
    "title": "STA 9750 - Week 2 Update",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nPre-Assignment 02: - 40 / 46 submitted - Ignore “5/10 grading” - limitation of Brightspace - I manually adjust\nPre-Assignment 03: - Before midnight: Available on website + Brightspace"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#mini-project-00",
    "href": "archive/AY-2024-FALL/slides/slides02.html#mini-project-00",
    "title": "STA 9750 - Week 2 Update",
    "section": "Mini-Project 00",
    "text": "Mini-Project 00\n\n7 submissions via Piazza (thank you - I will respond soon)\nDue in (slightly less than) a week\nPossible tech issues, so start early"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#course-bot",
    "href": "archive/AY-2024-FALL/slides/slides02.html#course-bot",
    "title": "STA 9750 - Week 2 Update",
    "section": "Course Bot",
    "text": "Course Bot\nBecause this course is a double, I have resources to create a “bot” to help with course organization on GitHub.\nBot will begin to acknowledge completed MP#00 over the weekend.\nCurrent name: CISSOID: CIS and Statistics bot for Organizing Instructional Delivery\n\nParticular mathematical shape\nName means “Ivy-like”:\n\nUse technology to overcome resource limitations of CUNY\n\n\nBetter name suggestions very welcome!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#today",
    "href": "archive/AY-2024-FALL/slides/slides02.html#today",
    "title": "STA 9750 - Week 2 Update",
    "section": "Today",
    "text": "Today\n\nReview of Questions from Pre-Assign #02\nCourse Project Overview\nIntroduction to Markdown and Quarto\nIntroduction to GitHub pages\nHow to ask for help"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#q1",
    "href": "archive/AY-2024-FALL/slides/slides02.html#q1",
    "title": "STA 9750 - Week 2 Update",
    "section": "Q1",
    "text": "Q1\n\nWhat is Markdown?\n\nPer Wikipedia: “Markdown is a light-weight, plain-text, markup language specification”\n\n\nLight-weight: relatively simple, focus on content than formatting\nPlain-text: accessible using almost any text editor (RStudio, GitHub, VS Code, etc)\n\nNot locked into specific software (e.g., MS Word)\nEasily incorporated into a variety of technologies"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#q1-1",
    "href": "archive/AY-2024-FALL/slides/slides02.html#q1-1",
    "title": "STA 9750 - Week 2 Update",
    "section": "Q1",
    "text": "Q1\n\nWhat is Markdown?\n\n\n\nMarkup language: a ‘mini-coding language’ for text documents\n\nOther famous examples: HTML, XML\n\nSpecification:\n\nCommonMark defines ‘standard’ Markdown\nSome software allows extensions\nPandoc often powers under the hood"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#q2",
    "href": "archive/AY-2024-FALL/slides/slides02.html#q2",
    "title": "STA 9750 - Week 2 Update",
    "section": "Q2",
    "text": "Q2\n\nOther than text formatting, does Markdown ha[ve] any other us[]es?\n\nOn its own, Markdown is just text formatting (but that’s a lot!)\n\nWe will use Quarto which augments markdown for reproducible research. We can embed code-and its output-inside Markdown documents."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#q3",
    "href": "archive/AY-2024-FALL/slides/slides02.html#q3",
    "title": "STA 9750 - Week 2 Update",
    "section": "Q3",
    "text": "Q3\n\n[W]hat documents use[] Markdown?\n\nSo much! Markdown is used by Bitbucket, GitHub, OpenStreetMap, Reddit, Stack Exchange, Drupal, ChatGPT, Discord, MS Teams and many more!\n\nWith tools like Pandoc/Quarto, Markdown can be rendered to:\n\n\n\nHTML\nPDF\nWeb Slides\nEBooks\n\n\n\nResearch Papers\nWord Documents\nPowerPoint slides\nand so much more!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#q4",
    "href": "archive/AY-2024-FALL/slides/slides02.html#q4",
    "title": "STA 9750 - Week 2 Update",
    "section": "Q4",
    "text": "Q4\n\n[What is] the difference between [a] Code section and [a] Nested List[? A]re they just different ways of indenting?\n\nNo. Nested lists are ‘just’ text\nCode formatting enables much more if rendering engine supports it:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#q5",
    "href": "archive/AY-2024-FALL/slides/slides02.html#q5",
    "title": "STA 9750 - Week 2 Update",
    "section": "Q5",
    "text": "Q5\n\n[H]ow are we going to use Markdown?\n\nAll written work (mini-projects and final project) in this course will be submitted using Markdown (by way of Quarto).\n\nSpecifically: - Submission pages for 5 mini-projects - Individual reports for course project - Summary (team) report for final project\nYou are also encouraged (but not required) to use Markdown for presentation slides (like these!)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#q6",
    "href": "archive/AY-2024-FALL/slides/slides02.html#q6",
    "title": "STA 9750 - Week 2 Update",
    "section": "Q6",
    "text": "Q6\n\nHow can I create Tables in Markdown?\n\nMarkdown has two table syntaxes:\n\nan easy one with minimal control\na hard one which allows fine grained control (alignment, column widths, etc.)\n\nIf you are using the advanced (“pipe table”) synatx, I suggest you use RStudio’s Visual editor mode. (DEMO!)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#q7",
    "href": "archive/AY-2024-FALL/slides/slides02.html#q7",
    "title": "STA 9750 - Week 2 Update",
    "section": "Q7",
    "text": "Q7\n\nHow to create images and links?\n\nBasic hyperlinks look like this:\n[link text](https://the.url/goes/here)\n\nIf you want to embed the contents of a link, prepend it with an exclamation point. This is most useful for images:\n![Image Caption](https://the.url/goes/here.png)\n\n\nYou can even put a link inside an image to be fancy:\n[![Elephant](elephant.png)](https://en.wikipedia.org/wiki/Elephant)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#q7-1",
    "href": "archive/AY-2024-FALL/slides/slides02.html#q7-1",
    "title": "STA 9750 - Week 2 Update",
    "section": "Q7",
    "text": "Q7\n\nHow to create images and links?\n\nQuarto automatically embeds the results of plotting code:\n\nplot(1:5, main=\"Behold, a Plot!\", col=2:6, cex=5, \n     pch=16, xlab=\"\", cex.main=5)\n\n\nHere, Quarto handles all the file creation and link targeting for us. If I change the code, the figure will change automatically."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#course-project",
    "href": "archive/AY-2024-FALL/slides/slides02.html#course-project",
    "title": "STA 9750 - Week 2 Update",
    "section": "Course Project",
    "text": "Course Project\n\nTeams: 3-5 classmates (either section)\nStages:\n\nProposal (in class presentation)\nMid-semester check-in (in class presentation)\nFinal: in class presentation, individual report, summary report\n\nStructure:\n\nShared “Overarching Question”\nIndividual “Specific Question”\n\n\nFull description online"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#finding-data",
    "href": "archive/AY-2024-FALL/slides/slides02.html#finding-data",
    "title": "STA 9750 - Week 2 Update",
    "section": "Finding Data",
    "text": "Finding Data\n\nStart early!\nNYC Open Data is great\n\nSee also: FRED, Federal Open Data, Nasa EarthData, Kaggle\nAsk on Piazza for pointers\nLots of data hidden in Wikipedia\n\nNothing paid or private without express instructor submission\nEveryone loves spatial data!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#presentation-hints",
    "href": "archive/AY-2024-FALL/slides/slides02.html#presentation-hints",
    "title": "STA 9750 - Week 2 Update",
    "section": "Presentation Hints",
    "text": "Presentation Hints\n\nLongest time \\(\\neq\\) most important\nStory, story, story! Why are you making these choices?\nHourglass Structure\n\nStart big\nMotivate your overarching question\nSpecific questions\nTie specific to overarching\nFrom overarching back to big motivation\n\nNo less than one figure every other slide"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#markdown-and-quarto-1",
    "href": "archive/AY-2024-FALL/slides/slides02.html#markdown-and-quarto-1",
    "title": "STA 9750 - Week 2 Update",
    "section": "Markdown and Quarto",
    "text": "Markdown and Quarto\n\nQuarto implements Markdown with data-analytic extensions\nSeamless (ideally!) integration of code and text\nNo more copy and paste\n\nQuarto user guide is fantastic!\nSee also source for course materials."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#lab-activity-part-0",
    "href": "archive/AY-2024-FALL/slides/slides02.html#lab-activity-part-0",
    "title": "STA 9750 - Week 2 Update",
    "section": "Lab Activity: Part 0",
    "text": "Lab Activity: Part 0\nIf you haven’t already, install Quarto."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#lab-activity-part-1",
    "href": "archive/AY-2024-FALL/slides/slides02.html#lab-activity-part-1",
    "title": "STA 9750 - Week 2 Update",
    "section": "Lab Activity: Part 1",
    "text": "Lab Activity: Part 1\nCreate a simple PDF quarto document using the RStudio wizard.\n(Note that you may need to install tinytex for this to work properly, but Quarto should install it for you automatically.)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#lab-activity-part-2",
    "href": "archive/AY-2024-FALL/slides/slides02.html#lab-activity-part-2",
    "title": "STA 9750 - Week 2 Update",
    "section": "Lab Activity: Part 2",
    "text": "Lab Activity: Part 2\nCreate a 5 slide presentation showing the Houston housing market. This should include:\n\nA title slide\nThree body slides with a figure and some text\nA conclusion slide\n\nYou may use the following code snippets:"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#lab-activity-part-3",
    "href": "archive/AY-2024-FALL/slides/slides02.html#lab-activity-part-3",
    "title": "STA 9750 - Week 2 Update",
    "section": "Lab Activity: Part 3",
    "text": "Lab Activity: Part 3\nView the Quarto Demo Slides and add one new element to your slides from the previous section."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#github-pages-1",
    "href": "archive/AY-2024-FALL/slides/slides02.html#github-pages-1",
    "title": "STA 9750 - Week 2 Update",
    "section": "GitHub Pages",
    "text": "GitHub Pages\nIn-class discussion of what a static web page is and the role of GitHub Pages as a static web server."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides02.html#how-to-ask-for-help-1",
    "href": "archive/AY-2024-FALL/slides/slides02.html#how-to-ask-for-help-1",
    "title": "STA 9750 - Week 2 Update",
    "section": "How to Ask for Help",
    "text": "How to Ask for Help\nProfessional programming is at least half looking things up; at beginning stages, the fraction is even higher.\nSo it’s important to know how to see help the smart way:\n\nOfficial documentation. Free software almost never becomes famous without great documentation: R and its packages are no exception. Everything we will use in this class has solid documentation.\n\n\nTidyverse.org"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#mini-project-02",
    "href": "archive/AY-2024-FALL/slides/slides08.html#mini-project-02",
    "title": "STA 9750 - Week 8",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nSubmission due yesterday at 11:45pm\n\nY’all have it out for The Shawshank Redemption…\n\n\n\nVery entertained by fake posters\nImpressive graphics and analysis - very nice."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#the-shark",
    "href": "archive/AY-2024-FALL/slides/slides08.html#the-shark",
    "title": "STA 9750 - Week 8",
    "section": "The Shark!",
    "text": "The Shark!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#mini-project-02-1",
    "href": "archive/AY-2024-FALL/slides/slides08.html#mini-project-02-1",
    "title": "STA 9750 - Week 8",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nAdd the following to your .gitignore file:\n/.quarto/\n.Rproj.user\n*/*_cache/*\n*tsv*\n*csv*\n*xlsx*\n*zip\n*gz\n.DS_Store\n.Rhistory\nThis instructs git to ignore matching files - less chance of accidental “super commits.”\n(Possible to override if you necessary)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#mini-project-02---peer-feedback",
    "href": "archive/AY-2024-FALL/slides/slides08.html#mini-project-02---peer-feedback",
    "title": "STA 9750 - Week 8",
    "section": "STA 9750 Mini-Project #02 - Peer Feedback",
    "text": "STA 9750 Mini-Project #02 - Peer Feedback\nPeer feedback assigned on GitHub this morning\n\n\\(\\approx 4\\) feedbacks each\nTake this seriously: around 20% of this assignment is “meta-review”\nGoal: rigorous constructive critique\n\n\nSubmissions may not map perfectly to rubric - use your best judgement\n\n\nLearn from this! What can you adapt for MP#03?"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#mini-project-03",
    "href": "archive/AY-2024-FALL/slides/slides08.html#mini-project-03",
    "title": "STA 9750 - Week 8",
    "section": "STA 9750 Mini-Project #03",
    "text": "STA 9750 Mini-Project #03\nNow online\nDue November 13th\n\nGitHub post (used for peer feedback) AND Brightspace\nThree Weeks: don’t wait until the very end\nShould be less demanding than MP #01 and MP#02\n\nLots of little files. No big files!\nMaps and election retrodiction\n\n\n\nPay attention to the rubric"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#thank-you-1",
    "href": "archive/AY-2024-FALL/slides/slides08.html#thank-you-1",
    "title": "STA 9750 - Week 8",
    "section": "Thank you!",
    "text": "Thank you!\nA personal note, if you allow me:\n\nI’m really enjoying this class - thank you all!\n\n\nYour effort is not unnoticed - I know this class starts “pedal-to-the-metal” but hopefully you’ve seen just how powerful these tools R.\n\n\nMore than that - I appreciate your good attitude and willingness to share your frustrations and triumphs. Reading comments on PA quiz this week was uplifting."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#continual-improvement",
    "href": "archive/AY-2024-FALL/slides/slides08.html#continual-improvement",
    "title": "STA 9750 - Week 8",
    "section": "Continual Improvement",
    "text": "Continual Improvement\nI’ve set up a TODO file with everything I want to improve for next cohort.\nSuggestions welcome.\nI will also ask Charles to distribute an opportunity for anonymous feedback."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#upcoming-mini-projects",
    "href": "archive/AY-2024-FALL/slides/slides08.html#upcoming-mini-projects",
    "title": "STA 9750 - Week 8",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#04: Something financial\n\nAny requests?"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#course-project",
    "href": "archive/AY-2024-FALL/slides/slides08.html#course-project",
    "title": "STA 9750 - Week 8",
    "section": "Course Project",
    "text": "Course Project\nFeedback from Charles and from me - I’m behind on returning.\nProject should be your main focus for rest of course\n\nStill need to do mini-projects and pre-assignments"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#pre-assignments",
    "href": "archive/AY-2024-FALL/slides/slides08.html#pre-assignments",
    "title": "STA 9750 - Week 8",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is October 30th\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#course-support",
    "href": "archive/AY-2024-FALL/slides/slides08.html#course-support",
    "title": "STA 9750 - Week 8",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday for rest of semester\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(&lt;50\\) minute average response time)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#upcoming-week",
    "href": "archive/AY-2024-FALL/slides/slides08.html#upcoming-week",
    "title": "STA 9750 - Week 8",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nDue Wednesday at 11:45pm:\n\nPre-Assignment #09 (Brightspace)\n\nData Import\n\nMP #02 Peer Feedback on GitHub AND Brightspace"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---aes",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---aes",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - aes()",
    "text": "FAQ: ggplot2 - aes()\nWhat is the aes function - stands between data and geom_\n\nEach geom_ takes a fixed set of “coordinates”\nEach data set has its own column names\naes ties these together\n\n\nlibrary(ggplot2); library(patchwork)\np1 &lt;- ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point()\np2 &lt;- ggplot(iris, aes(y=Sepal.Length, x=Sepal.Width)) + geom_point()\np1 + p2"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---why-do-pie-charts-have-a-bad-reputation",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---why-do-pie-charts-have-a-bad-reputation",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - Why do Pie Charts have a bad reputation?",
    "text": "FAQ: ggplot2 - Why do Pie Charts have a bad reputation?\n\nUse of area and angle over length: less accurate perception\nDepends on fill to convey category - limited categories\n\n\nBut honestly - “insider smugness” and hate of Excel"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---plot-type-choice",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---plot-type-choice",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - Plot Type Choice",
    "text": "FAQ: ggplot2 - Plot Type Choice\nFor me:\n\nExploratory mode:\n\nSimple: line, scatter, bar, frequency\n\nPublication mode:\n\nVery context specific"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---font-sizing",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---font-sizing",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - Font Sizing",
    "text": "FAQ: ggplot2 - Font Sizing\nTheme machinery!\n\nggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + \n    geom_point() + theme(axis.text = element_text(size=24))"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---overplotting-scatterblobs",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---overplotting-scatterblobs",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - Overplotting / ScatterBlobs",
    "text": "FAQ: ggplot2 - Overplotting / ScatterBlobs\nStudent asked about “scatterblobs” - typo(?) but I love it!\n\n\nDensity based plotting: hexbins, histograms, rugplots\nData reduction: summarization or sub-sampling"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---optimizing-performance",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---optimizing-performance",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - Optimizing Performance",
    "text": "FAQ: ggplot2 - Optimizing Performance\nActive project of ggplot2 team - not much you can do\nPractical advice: plot less (see previous slide)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---beyond-scatter-and-line",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---beyond-scatter-and-line",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - Beyond Scatter and Line",
    "text": "FAQ: ggplot2 - Beyond Scatter and Line\nSome favorite semi-advanced plot types:\n\nViolin plots: combination of boxplot and histogram\nRidgelines\nBeeswarms\n\nDeep rabbit hole"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---geospatial-visualizations",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---geospatial-visualizations",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - Geospatial Visualizations",
    "text": "FAQ: ggplot2 - Geospatial Visualizations\nThat’s our goal for today!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---high-dimensional-data",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---high-dimensional-data",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - High-Dimensional Data",
    "text": "FAQ: ggplot2 - High-Dimensional Data\nHigh-dimensional data: measure many variables per observation (“wide”)\nHigh-dimensional data is hard to visualize\n\nApproaches:\n\nPair plots for “moderate” HDD\nPCA (or similar dimension reduction. Take 9890!)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---creating-a-custom-theme",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---creating-a-custom-theme",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - Creating a Custom Theme",
    "text": "FAQ: ggplot2 - Creating a Custom Theme\n\nmy_theme &lt;- theme_bw() + theme(panel.background = element_rect(fill = 'lightblue'))\nggplot(iris, aes(x=Sepal.Length, y = Sepal.Width)) + geom_point() + my_theme\n\n\nAdvanced:\n\ntheme_set() - change ggplot2 defaults\n.Rprofile - set code to run every time you start R"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---when-not-to-use",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-ggplot2---when-not-to-use",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: ggplot2 - When Not to Use",
    "text": "FAQ: ggplot2 - When Not to Use\nggplot2 is designed to make good statistical graphics. Sub-par for:\n\nAdvanced interactivity\nReally big data\nHardcore customization / “infographics”"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#faq-git-wtf",
    "href": "archive/AY-2024-FALL/slides/slides08.html#faq-git-wtf",
    "title": "STA 9750 - Week 8",
    "section": "FAQ: git WTF",
    "text": "FAQ: git WTF\nReference: Happy Git with R"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#warm-up",
    "href": "archive/AY-2024-FALL/slides/slides08.html#warm-up",
    "title": "STA 9750 - Week 8",
    "section": "Warm-Up",
    "text": "Warm-Up\n“Datasaurus Dozen”:\n\ninstall.packages(\"datasauRus\") (Note capital R)\nlibrary(datasauRus); data(datasaurus_dozen)\n\nCreate an animated (gganimate) plot:\n\n\\(x, y\\) scatterplot\nAnimate different values of dataset\n\nIf you are having trouble with gganimate, facet instead."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#warm-up-1",
    "href": "archive/AY-2024-FALL/slides/slides08.html#warm-up-1",
    "title": "STA 9750 - Week 8",
    "section": "Warm-Up",
    "text": "Warm-Up"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#diving-deeper-with-ggplot2",
    "href": "archive/AY-2024-FALL/slides/slides08.html#diving-deeper-with-ggplot2",
    "title": "STA 9750 - Week 8",
    "section": "Diving Deeper with ggplot2",
    "text": "Diving Deeper with ggplot2\nToday: maps!\nInstall the sf package: Simple Features for Spatial Data\nExercise: Lab #08"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides08.html#breakout-rooms",
    "href": "archive/AY-2024-FALL/slides/slides08.html#breakout-rooms",
    "title": "STA 9750 - Week 8",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nRoom\nTeam\n\nRoom\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "archive/AY-2024-FALL/objectives.html",
    "href": "archive/AY-2024-FALL/objectives.html",
    "title": "STA 9750 - Course Learning Objectives",
    "section": "",
    "text": "This course provides an understanding of the principles and concepts of using computer tools for data analysis and visualization. Students will learn to use a scientific programming language (such as R) to import and export data from and into spreadsheets or other statistical software packages. They will gain experience in analyzing both quantitative and qualitative data, and statistical modelling techniques will be introduced. Written reports will prepare students for clear communication of their analysis in professional settings. This course is designed primarily for Masters’ students in statistics and quantitative methods and modeling (QMM), and those interested in carrying out sophisticated statistical analyses of data using statistical software.\n\n\n\nThis course provides an understanding of the principles and concepts of using computer tools for data analysis. Students will learn to use the SAS programming language to handle the collection, editing and storing of large datasets, as well as to simulate data, import and export data from and into spreadsheets or other statistical software packages. They will gain experience in analyzing both quantitative and qualitative data, as well as repeated measure data. Written projects and class presentation will prepare students for clear communication of their analysis in professional settings. This course is designed primarily for statistics and quantitative methods and modeling (QMM) majors, PhD candidates, and those interested in carrying out sophisticated statistical analyses of data using statistical software.\nInstructor’s Note: Contra the official OPR 9750 description, this course will be taught using R not SAS. STA 9750 and OPR 9750 will be jointly taught and graded. Please consult with your degree program director to determine which listing is appropriate for you."
  },
  {
    "objectID": "archive/AY-2024-FALL/objectives.html#official-course-description",
    "href": "archive/AY-2024-FALL/objectives.html#official-course-description",
    "title": "STA 9750 - Course Learning Objectives",
    "section": "",
    "text": "This course provides an understanding of the principles and concepts of using computer tools for data analysis and visualization. Students will learn to use a scientific programming language (such as R) to import and export data from and into spreadsheets or other statistical software packages. They will gain experience in analyzing both quantitative and qualitative data, and statistical modelling techniques will be introduced. Written reports will prepare students for clear communication of their analysis in professional settings. This course is designed primarily for Masters’ students in statistics and quantitative methods and modeling (QMM), and those interested in carrying out sophisticated statistical analyses of data using statistical software.\n\n\n\nThis course provides an understanding of the principles and concepts of using computer tools for data analysis. Students will learn to use the SAS programming language to handle the collection, editing and storing of large datasets, as well as to simulate data, import and export data from and into spreadsheets or other statistical software packages. They will gain experience in analyzing both quantitative and qualitative data, as well as repeated measure data. Written projects and class presentation will prepare students for clear communication of their analysis in professional settings. This course is designed primarily for statistics and quantitative methods and modeling (QMM) majors, PhD candidates, and those interested in carrying out sophisticated statistical analyses of data using statistical software.\nInstructor’s Note: Contra the official OPR 9750 description, this course will be taught using R not SAS. STA 9750 and OPR 9750 will be jointly taught and graded. Please consult with your degree program director to determine which listing is appropriate for you."
  },
  {
    "objectID": "archive/AY-2024-FALL/objectives.html#course-learning-objectives",
    "href": "archive/AY-2024-FALL/objectives.html#course-learning-objectives",
    "title": "STA 9750 - Course Learning Objectives",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nStudents successfully completing STA 9750 will be able to:\n\nEffectively communicate the reuslts of data analyses.\nManipulate tabular data in R\nDevelop effective and compelling visualizations using standard statistical software\nManipulate `wild-caught’ data from web-based sources\nUse computational approaches to statistical inference\nDevelop novel analytical products to convey actionable insights.\n\nThe following course elements contribute to these goals:\n\nContribution of Course Elements to Learning Goals\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Goal 1\nLearning Goal 2\nLearning Goal 3\nLearning Goal 4\nLearning Goal 5\nLearning Goal 6\n\n\n\n\nMini Project #00\n✓\n\n\n\n\n\n\n\nMini Project #01\n✓\n✓\n\n\n\n\n\n\nMini Project #02\n✓\n✓\n✓\n\n\n\n\n\nMini Project #03\n✓\n✓\n✓\n✓\n\n\n\n\nMini Project #04\n✓\n✓\n✓\n✓\n✓\n\n\n\nCourse Project\n✓\n✓\n✓\n✓\n✓\n✓"
  },
  {
    "objectID": "archive/AY-2024-FALL/objectives.html#program-learning-goals",
    "href": "archive/AY-2024-FALL/objectives.html#program-learning-goals",
    "title": "STA 9750 - Course Learning Objectives",
    "section": "Program Learning Goals",
    "text": "Program Learning Goals\nThis course contributes to the program learning goals of several MS programs offered by the Zicklin School of Business.\n\nMS in Business Analytics\nThis course contributes to the following Program Learning Goals for the MS in Business Analytics:\n\nMSBA Program Learning Goals\n\n\n\n\n\n\n\nSTA 9750 Learning Goal\nMSBA Learning Goal\nDescription\n\n\n\n\n✓\nData Management\nStudents will be able to apply methods, tools, and software for acquiring, managing/storing, and accessing structured and unstructured data. Students will also demonstrate knowledge of the strategic uses of data.\n\n\n✓\nFoundational Statistical / Quantitative Skills\nStudents will be able to prepare data for statistical analysis, perform basic exploratory and descriptive analysis as well as employ foundational statistical techniques needed to analyze data.\n\n\n✓\nAdvanced Statistical/Quantitative Skills\nStudents will be able to build and interpret advanced predictive models. Students will be able to combine business rules and mathematical models to optimize business decisions from data.\n\n\n\nEthical Awareness\nStudents will be able to articulate an understanding of ethical issues in all phases of business analytics with particular emphasis on the new possibilities afforded by the emergence of big data.\n\n\n✓\nProfessional Communication\nStudents will be able to explain complex analytical models and their results orally and in writing to technical and non technical/lay audiences.\n\n\n✓\nKnowledge Integration\nStudents will be able to apply the three key types of analytics (descriptive, predictive, and prescriptive) in a business domain to add value to business decision-making.\n\n\n\n\n\nMS in Quantitative Methods & Modeling\nThis course contributes to the following Program Learning Goals for the MS in Quantitative Methods & Modeling:\n\nMSQMM Program Learning Goals\n\n\n\n\n\n\n\nSTA 9750 Learning Goal\nMSQMM Learning Goal\nDescription\n\n\n\n\n✓\nOperations Research & Mathematical Modeling\nStudents will be able to effectively model, evaluate, and solve quantitative (business) problems using quantitative modeling methods (e.g. deterministic and probabilistic operations research techniques).\n\n\n✓\nStatistics\nStudents will be able to correctly apply appropriate statistical methods when defining, solving, and analyzing problems.\n\n\n✓\nTechnology Competency\nStudents will be able to use current technological tools, including spreadsheets and specialized software, when solving problems.\n\n\n✓\nProfessional Communication\nStudents will be able to effectively communicate their problem solving methods and solutions to technical and non-technical audiences.\n\n\n\n\n\nMS in Statistics\nThis course contributes to the following Program Learning Goals for the MS in Statistics:\n\nMS Statistics Program Learning Goals\n\n\n\n\n\n\n\nSTA 9750 Learning Goal\nMS Stat Learning Goal\nDescription\n\n\n\n\n✓\nGeneral Statistical Competence\nStudents will be able to apply appropriate probability models and statistical techniques when analyzing problems from business and other fields.\n\n\n✓\nStatistical Practice\nStudents will become familiar with the standard tools of statistical practice for multiple regression, along with the tools of a subset of specialized statistical areas such as multivariate analysis, applied sampling, time series analysis, experimental design, data mining, categorical analysis, and/or stochastic processes.\n\n\n✓\nTechnology Competency\nStudents will learn to use one or more of the benchmark statistical software platforms, such as SAS or R."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa07.html",
    "href": "archive/AY-2024-FALL/preassigns/pa07.html",
    "title": "STA 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "",
    "text": "Due Date: 2024-10-16 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week we begin to explore statistical visualizations. Visualizations play several interrelated roles in statistical practice: we use visualization to explore new data sets, to see how well models fit to data, and to communicate the results of analyses to new audiences. Compared with the ‘point summary’ tools we have discussed to date, visualizations are far more flexible and more powerful: we can extract novel insights from data visualizations, but we can also deceive ourselves and others.\nAs you review this document, also watch how I iterate and refine each figure until I have something I’m finally happy with. This is quite typical of how working data scientists produce plots: you rarely know exactly what you want, particularly before you begin to explore your data. You should adapt a similar pattern of “take a sad plot and make it better” as you create plots for your mini-projects and, ultimately, your final project."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa07.html#grammar-of-graphics",
    "href": "archive/AY-2024-FALL/preassigns/pa07.html#grammar-of-graphics",
    "title": "STA 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nComputer graphics is a field of essentially infinite possibility - anything you dream can be represented in the digital domain. At one limit, we have the option to work on a “per pixel” basis, telling the computer exactly what to draw and where. Because this is clearly overwhelmingly monotonous, very little work is actually done at such fine detail and abstraction layers are provided to automate the low-level detail work. We will use an abstraction model known as the Grammar of Graphics.\nDesigned by Leland Wilkinson and popularized by Hadley Wickham, the Grammar of Graphics poses a set of rules for visualizing (tidy) data. It draws its name from the concept of linguistic grammar - the rules that dictate how basic elements (nouns, verbs, and adjectives) may and may not be combined into clear and meaningful sentences The Grammar of Graphics provides specifications for combining different plot elements (legends, data, axes, etc.) into clear and meaningful statistical graphics. Taking the metaphor too far, we might say that the Grammar of Graphics is the Strunk and White to the Ska that are “Infographics.”\nThe grammar of graphics has several interconnected components, which are combined to form a meaningful graphic. We assume that we have a “tidy” data set we want to visualize; recall that, by “tidy”, we mean that our data is\n\nOrganized in a rectangular array\nHomogenously-typed within a column\nOne observation per row\nOne value per cell\n\nGiven this form of tidy data, the grammar of graphics provides us the “parts of speach” necessary to convert tidy data to a visual representation. We’ll only cover the basic components here, leaving more advanced tools for class session:\n\nThe aesthetics are mappings between columns of the data and aspects of the visualization. For instance, “put the grade column on the \\(y\\) axis” or “use color to represent the course ID”.\nScales convert data values to the aesthetics: scales may be quite trivial, e.g. placing continuous values on the \\(x\\) axis in proper order, or more advanced, e.g., binning values and converting them to a sequence of perceptually-ordered colors. )\nGeometric elements or geoms specify how the data are represented on the page through the scales. geoms include basic representations, like points for a scatter plot or lines for a trend plot, as well as more complex objects like boundaries on a map.\nGuides provide interpretational assistance to the viewer. Most guides take the form of legends.\n\nThat’s all a bit abstract, so let’s put it into practice. For now, you shouldn’t worry so much about what each of these really mean; it’s just useful to have a rough sense of what “knob” you want to turn to modify plots."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa07.html#getting-started-with-ggplot2",
    "href": "archive/AY-2024-FALL/preassigns/pa07.html#getting-started-with-ggplot2",
    "title": "STA 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "Getting started with ggplot2",
    "text": "Getting started with ggplot2\nThe leading implementation of the grammar of graphics is the ggplot2 package in R (gg = Grammar of Graphics). It comes to us from Hadley Wickham and the tidyverse team, who also developed the dplyr and tidyr tools we have been using for the past several weeks.\nLet’s begin by using ggplot2 to explore our penguins data:\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nggplot(penguins)\n\n\n\n\n\n\n\n\nSomewhat underwhelming…\nggplot2, like many of the tools in this course, do exactly what we ask, and no more. Because we have not specified any of the Grammar of Graphics elements, we only get a blank canvas. Let’s now begin by adding an aesthetic to map some of the elements of our data to aspects of our plot.\nSpecifically, suppose we want to see how flipper length correlates with body mass. Let’s make a scatter plot with flipper length on the \\(x\\)-axis and body mass on the \\(y\\)-axis.1\n\nggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g))\n\n\n\n\n\n\n\n\nOk - this is perhaps a bit better. We can see that the columns flipper_lenth_mm and body_mass_g have been placed on the \\(x\\)- and \\(y\\)-axes as we wanted, but we still don’t see anything.\nWe need a geom to actually put “ink to paper”. The simplest geom is a point, useful for making scatter plots.\n\nggplot(penguins, \n       aes(x=flipper_length_mm, y=body_mass_g)) + \n    geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nPretty nifty! Before we go forward, note that ggplot2 adds elements to create ever more complex plots. This is different than dplyr where we “piped” data from one step to the next, refining it along the way.\nHow can we improve the plot above? Before anything else, let’s clean up the \\(x\\) and \\(y\\) axis labels. While the default behavior of showing variable names is helpful for exploratory data analysis, we never want to let variable names “leak” in plots we intend to share with others. We should instead use meaningful (and attractive) axis labels.\n\nggplot(penguins, \n       aes(x=flipper_length_mm, y=body_mass_g)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe can also take advantage of the theme mechanisms of ggplot2 to change the color of the “infrastructure” of our plot. The theme mechanism doesn’t change how the data itself is visualized, but it controls how things like the background, font sizing, etc behave. I tend to prefer the black and white theme over the grey default:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, y=body_mass_g)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdding Color to Depict Species\nOur data set three distinct species and the correlation between flipper and body size may vary across species. Let’s add some color to our plot: since color maps a data element (species) to a graphical aspect (color) we add it to our aesthetic mapping (aes).\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe see here that a few things happened automatically for us:\n\nThe color element was automatically propagated into geom_point. By default, any “top-level” aesthetics are automatically applied to any geom that can handle them.\nA legend was created.\nA color scale was chosen.\n\nThe geom_point help page tells us which aesthetics are required (\\(x, y\\)) and which are optional for the point geom. We couldn’t have gotten away without providing \\(x, y\\) coordinates, but until this point, we were just using the default (black) color.\nTo improve the look of the colors, we can choose a different color scale. I tend to like the colors of the Color Brewer project, though strictly speaking these are designed for use in maps, not scatter plots. You can access these in ggplot2 as follows:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", palette=2)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere, I’m using a type=\"qual\" (qualitative) palette because there is no inherent ordering to the penguin types. I like the “bolder” colors of the second palette in this set, but you can adjust the number to try different schemes.\nNext, let’s improve the look of the legend. As before, we see that it is by default titled with the variable name (species). We can provide a proper title instead:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nI still don’t like this legend on the side - it takes too much room, so let’s move it below the image instead. This involves changing a “non-data” element of the plot, so we go through the theme machinery. theme() allows an enormous number of possible changes, but here we want legend.position:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNot so bad!\n\n\nVisualizing Statistical Relationships\nRecall that our goal was to measure the correlation between Body Mass and Flipper Length. We can visualize this correlation on the plot by adding a regression line (recall that for univariate regression like this, the slope of the regression line is \\(\\hat{\\beta} = \\rho_{XY}\\frac{\\sigma_Y}{\\sigma_X}\\)).\nThis is a new geometric element, called a smoother. ggplot2 allows many possible smoothers, but let’s use the lm (linear model) version, which we specify by setting the method argument:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\") + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nBy default, ggplot2 is giving us confidence intervals around the linear trend. These are sometimes useful, but perhaps a bit crowded for now, so let’s turn them off:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\", \n                se=FALSE) + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis is nicer, but maybe still a bit crowded. It would be nicer if we could avoid the “overlaps” of the different species. Here, let’s break out a “small multiples” plot: this is, in essence, a group_by for plotting.\nIn ggplot2 speak, this is called a faceted plot:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\", \n                se=FALSE) + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNot too shabby. But now it’s a bit too hard to tell the lines from the points. Let’s override the color used:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\", \n                se=FALSE, \n                color=\"black\") + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThese plots give us some idea of the correlation, but what if we want actual numbers? We can’t do this in “plain” ggplot2, but now we can take advantage of the enormous number of ggplot2 extension packages. It turns out that the ggpmisc package supports what we need, so let’s download and install it:\n\nif(!require(\"ggpmisc\")) install.packages(\"ggpmisc\")\n\nLoading required package: ggpmisc\n\n\nLoading required package: ggpp\n\n\nRegistered S3 methods overwritten by 'ggpp':\n  method                  from   \n  heightDetails.titleGrob ggplot2\n  widthDetails.titleGrob  ggplot2\n\n\n\nAttaching package: 'ggpp'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(ggpmisc)\n\nNow we have access to the various geom_, scale_, etc objects from that package. We can now introduce a new category, stat_, that represents statistical transformations or modeling. Generally, these are applied “automagically” for us, as in geom_smooth, but here we need to build our regression models explicitly:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSee how, even though we’re using functionality from outside ggplot2, the structure of the “grammar” makes it easy for all these tools to work well together.\n\n\nFinal Polish\nWe are almost done, but every figure needs a bit of final polish. Firstly, we should add a title, using the ggtitle function. (R has a built-in title function but that won’t help us here)\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn good academic practice, we should always add a footnote citing the source of our data. The palmerpenguins site has appropriate source information:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\") + \n    labs(caption=\"Data provided by Dr. K. Gorman and the Palmer Station, Antarctica LTER\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAt this point, the bottom of our figure looks a bit crowded. To clear out some space, let’s remove the legend from the bottom, since it simply repeats the facet labels:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\") + \n    labs(caption=\"Data provided by Dr. K. Gorman and the Palmer Station, Antarctica LTER\") + \n    guides(color=\"none\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nCleaner and no loss of information. In designing good scientific graphics, the concept of a “ink-to-information” ratio is useful: if you can remove some ink without removing any (relevant) information, you should generally do so. This makes it easier for the reader to identify the important elements of the plot.\nTo make our point even clearer, it is sometimes useful to add a short “summary” to a plot like this:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\", \n            subtitle=\"Flipper Length and Body Mass are positively correlated across species\\nGentoo penguins exhibit the strongest relationship at 70% correlation\") + \n    labs(caption=\"Data provided by Dr. K. Gorman and the Palmer Station, Antarctica LTER\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis is not always a good idea - it requires “hard-coding” the insights for your reader and takes up some space. In scientific writing, I generally prefer to put this sort of summary in a figure caption, while I tend to “say the point” verbally if the figure is destined for a presentation.\nPersonally, I only use this sort of “here is the point” text if I expect a figure to “escape beyond” my presentation and need it to stand fully on its own.\n\n\nConclusions\nggplot2 provides an exceptionally powerful and flexible set of tools for creating statistical visualizations. We will explore it in more depth in class. For now, review the examples above and make sure you see how each plot is created “piecewise” from its various components.\nTo see more about what ggplot2 can do, check out the R Graphics Gallery. If you want to see the specifics of each ggplot2 function, check out the package reference page. To go further with ggplot2, you should also explore its extension gallery."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa07.html#footnotes",
    "href": "archive/AY-2024-FALL/preassigns/pa07.html#footnotes",
    "title": "STA 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI once had a boss who refused to use the terms \\(x\\) and \\(y\\) axis unless quantities called \\(x\\) and \\(y\\) were actually being plotted. Being Cantabrigian, he insisted I use the terms abscissa and ordinate. I will not inflict such pedantry in this course.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa04.html",
    "href": "archive/AY-2024-FALL/preassigns/pa04.html",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "",
    "text": "Due Date: 2024-09-18 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week, we begin manipulating R’s most important structure the data.frame. While base R provides tools for working with data.frame objects, our primary focus will be on the tidyverse family of tools. These provide a unified and consistent set of tools for working with data objects."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa04.html#what-is-a-data.frame",
    "href": "archive/AY-2024-FALL/preassigns/pa04.html#what-is-a-data.frame",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "What is a data.frame?",
    "text": "What is a data.frame?\nLast week, we discussed vectors, one-dimensional collections of the same type of object. While vectors are an important computational tool, real data is rarely quite so simple: we collect multiple features (covariates) from each of our observations and these features may be of different type. For example, when performing a political survey, we may record the:\n\nName (character)\nAge (integer)\nGender (factor)1\nDate of Contact (Date)\nLevel of candidate support (double or numeric)\n\nof each respondant. It is natural to organize this in tabular (“spreadsheet”) form:\n\n\n\nName\nAge\nGender\nDate of Contact\nLevel of Support\n\n\n\n\nTimmy\n25\nM\n2024-09-13\n0.25\n\n\nTammy\n50\nF\n2024-06-20\n-1.35\n\n\nTaylor\n70\nX\n2024-08-15\n200\n\n\nTony\n40\nM\n2024-12-25\n0\n\n\nToni\n65\nF\n2024-11-28\n-4\n\n\n\nNote several important features of this data:\n\nEach row corresponds to one, and only one, sample\nEach column corresponds to one, and only one, feature\nThe values in each column are all of the same type\nThe order doesn’t matter: all important data is reflected in the values, not the presentation\n\nGenerally, data in this pattern will be called “tidy”. R represents this type of data as a data.frame object. For more on what it means for data to be “tidy”, read this paper.\n\nTibbles and the Tidyverse\nMany of the tools we will discuss in this class are from a set of related R packages, collectively known as the tidyverse. They are designed to i) read data from outside of R into tidy formats; ii) manipulate data from one tidy format to another; iii) communicate the results of tidy data analyses.\nWhile you can load these packages separately, they are used together so frequently that the tidyverse package exists to load them all simultaneously. I recommend you start most of your analyses with the command:\n\nlibrary(tidyverse)\n\nThis will automatically load the following packages:\n\nlubridate for date and time manipulation\nforcats for factor manipulation\nstringr for string manipulation\ndplyr for data frame manipulation\npurrr for functional programming\nreadr for tidy data import\ntidyr for tidy data manipulation\ntibble for data frame enhancement\nggplot2 for data visualization\n\nThis week, we are focusing on functionality from the dplyr package.\nYou may, from time to time, see reference to tibbles in R documentation. A tibble is a “souped-up” data frame with somewhat better default printing. For almost all purposes-and everywhere in this course- you can substitute tibble with data.frame without issue.\nBefore we get into dplyr, let’s make sure we have a data frame to play with. Let’s bring back our friends, the Palmer penguins:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa04.html#subsetting-data-frames",
    "href": "archive/AY-2024-FALL/preassigns/pa04.html#subsetting-data-frames",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Subsetting Data Frames",
    "text": "Subsetting Data Frames\nOur first task will be to subset data frames: that is, to select only some rows and columns and to return a smaller data frame than the one we started with.\n\nSubsetting Columns\nThe main dplyr function for column subsetting is select(). In its simplest form, we provide a set of column names we want to keep and everything else gets dropped.\n\nselect(penguins, species, island)\n\n# A tibble: 344 × 2\n   species island   \n   &lt;fct&gt;   &lt;fct&gt;    \n 1 Adelie  Torgersen\n 2 Adelie  Torgersen\n 3 Adelie  Torgersen\n 4 Adelie  Torgersen\n 5 Adelie  Torgersen\n 6 Adelie  Torgersen\n 7 Adelie  Torgersen\n 8 Adelie  Torgersen\n 9 Adelie  Torgersen\n10 Adelie  Torgersen\n# ℹ 334 more rows\n\n\nHere, we keep the species and island columns and remove the others.\nPause for a moment to note how the select function is structured: the first argument is the data frame on which we are operating; all following arguments control the resulting behavior. This is a common pattern in dplyr functions, designed to take advantage of another key R functionality, the pipe operator.\nR provides an operator |&gt; which “rewrites” code, so\n\nselect(penguins, species, island)\n\nand\n\npenguins |&gt; select(species, island)\n\nare exactly the same thing as far as R is concerned. You may well ask yourself why bother: the second “piped” operator is a bit longer and a bit harder to type. But just hold on - we’ll see this makes for far cleaner code in the long run.\nJust like base R, if we include - signs in our select statement, we get everything except a certain column:\n\npenguins |&gt; select(-bill_length_mm, -bill_depth_mm, -flipper_length_mm)\n\n# A tibble: 344 × 5\n   species island    body_mass_g sex     year\n   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n 1 Adelie  Torgersen        3750 male    2007\n 2 Adelie  Torgersen        3800 female  2007\n 3 Adelie  Torgersen        3250 female  2007\n 4 Adelie  Torgersen          NA &lt;NA&gt;    2007\n 5 Adelie  Torgersen        3450 female  2007\n 6 Adelie  Torgersen        3650 male    2007\n 7 Adelie  Torgersen        3625 female  2007\n 8 Adelie  Torgersen        4675 male    2007\n 9 Adelie  Torgersen        3475 &lt;NA&gt;    2007\n10 Adelie  Torgersen        4250 &lt;NA&gt;    2007\n# ℹ 334 more rows\n\n\nHere, we dropped three columns.\nThe select operator provides more advanced functionality which is useful for very complex data structures, but we don’t need to dive into that just yet.\n\n\nSubsetting Rows\nOften in data analysis, we want to focus on a subset of the entire population: e.g., we might want to know the fraction of women supporting a certain political candidate or the rate of a rare cancer among patients 65 or older. In this case, we need to select only those rows of our data that match some criterion. This brings us to the filter operator.\nfilter takes a logical vector and uses it to select rows of a data frame. Most commonly, this logical vector is created by performing some sort of tests on the values in the data frame. For example, if we want to select only the male penguins in our data set, we may write:\n\npenguins |&gt; filter(sex == \"male\")\n\n# A tibble: 168 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.3          20.6               190        3650\n 3 Adelie  Torgersen           39.2          19.6               195        4675\n 4 Adelie  Torgersen           38.6          21.2               191        3800\n 5 Adelie  Torgersen           34.6          21.1               198        4400\n 6 Adelie  Torgersen           42.5          20.7               197        4500\n 7 Adelie  Torgersen           46            21.5               194        4200\n 8 Adelie  Biscoe              37.7          18.7               180        3600\n 9 Adelie  Biscoe              38.2          18.1               185        3950\n10 Adelie  Biscoe              38.8          17.2               180        3800\n# ℹ 158 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nHere, note the use of the == operator for testing equality. A very common mistake is to use the single equals (assignment) operator inside of filter. Thankfully, dplyr will alert us with an error if we make this mistake:\n\npenguins |&gt; filter(sex = \"male\")\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `sex == \"male\"`?\n\n\nIf we supply multiple tests to filter, we get the intersection: that is, we get rows that pass all tests.\n\npenguins |&gt; filter(sex == \"male\", bill_length_mm &gt; 38)\n\n# A tibble: 156 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.3          20.6               190        3650\n 3 Adelie  Torgersen           39.2          19.6               195        4675\n 4 Adelie  Torgersen           38.6          21.2               191        3800\n 5 Adelie  Torgersen           42.5          20.7               197        4500\n 6 Adelie  Torgersen           46            21.5               194        4200\n 7 Adelie  Biscoe              38.2          18.1               185        3950\n 8 Adelie  Biscoe              38.8          17.2               180        3800\n 9 Adelie  Biscoe              40.6          18.6               183        3550\n10 Adelie  Biscoe              40.5          18.9               180        3950\n# ℹ 146 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIf we want the union-rows that satisfy any of the tests-we have to use the logical operators we previous applied to vectors:\n\npenguins |&gt; filter( (sex == \"male\") | (bill_length_mm &gt; 38))\n\n# A tibble: 292 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           39.3          20.6               190        3650\n 5 Adelie  Torgersen           38.9          17.8               181        3625\n 6 Adelie  Torgersen           39.2          19.6               195        4675\n 7 Adelie  Torgersen           42            20.2               190        4250\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 282 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIt’s a bit clunky, but thankfully, this is somewhat less common than checking that all conditions are satisfied.\ndplyr provides all sorts of useful helpers for creating test statements, e.g., the\n\nbetween\nnear\n\nfunctions.\nEven more useful than these, however, are the slice_*() functions which can be used to perform “top \\(k\\)” type operations. If we want the five largest Adelie penguins, we might try something like:\n\npenguins |&gt; filter(species == \"Adelie\") |&gt; slice_max(body_mass_g, n=5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Biscoe              43.2          19                 197        4775\n2 Adelie  Biscoe              41            20                 203        4725\n3 Adelie  Torgersen           42.9          17.6               196        4700\n4 Adelie  Torgersen           39.2          19.6               195        4675\n5 Adelie  Dream               39.8          19.1               184        4650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nslice_min() works similarly. slice_head and slice_tail get the first and last rows by sort order - in general, I recommend against using these. A key rule of data analysis is that the row order is not semantically meaningful, but it’s good to keep them in the back of your mind just in case. slice_sample can be used to select random subsets of data.\nAnother important subseting function is drop_na which will drop any rows with missing (NA) data.2 This is a good and useful tool, but before you apply it, always ask yourself “why is this data missing?” Understanding the abscence of data is often just as important as understanding the non-missing data. For example, is a student’s SAT score missing on a college application because they i) forgot to list it; ii) never took the SAT; or iii) took the test but chose to omit their score because they did poorly? Proper handling of missing data is often very problem-specific and very hard."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa04.html#manipulating-and-creating-columns",
    "href": "archive/AY-2024-FALL/preassigns/pa04.html#manipulating-and-creating-columns",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Manipulating and Creating Columns",
    "text": "Manipulating and Creating Columns\nA key task in data analysis is transforming data. As discussed in class, one of the guiding themes of R programming is data integrity and an important way R ensures this is by applying commands to the entire vector. In the data frame context, we apply commands to an entire column. The mutate function is dplyr’s main interface for column creation and manipulation.\nIn general, each argument to mutate takes a name = value pair: the name is the name of a column to be created from value. value can be an arbitrary function of other columns. If name corresponds to an existing column, that column is silently overwritten.\nFor example, if we want to convert penguin bill lengths from millimeters to inches, we might operator:\n\npenguins |&gt; mutate(bill_length_in = bill_length_mm / 25.4)\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_length_in &lt;dbl&gt;\n\n\nHere, note that the bill_length_mm column is retained - and all columns are kept! mutate only creates new columns; it won’t secretly drop them.\nA particularly common operator is changing the name of a column without changing its values. You can use mutate and select(-) for this, but rename provides essentially the same interface and its semantically clearer:\n\npenguins |&gt; rename(mass = body_mass_g)\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm  mass sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt; &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen           39.1          18.7               181  3750 male  \n 2 Adelie  Torgersen           39.5          17.4               186  3800 female\n 3 Adelie  Torgersen           40.3          18                 195  3250 female\n 4 Adelie  Torgersen           NA            NA                  NA    NA &lt;NA&gt;  \n 5 Adelie  Torgersen           36.7          19.3               193  3450 female\n 6 Adelie  Torgersen           39.3          20.6               190  3650 male  \n 7 Adelie  Torgersen           38.9          17.8               181  3625 female\n 8 Adelie  Torgersen           39.2          19.6               195  4675 male  \n 9 Adelie  Torgersen           34.1          18.1               193  3475 &lt;NA&gt;  \n10 Adelie  Torgersen           42            20.2               190  4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;"
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa04.html#operating-with-group-structure",
    "href": "archive/AY-2024-FALL/preassigns/pa04.html#operating-with-group-structure",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Operating with Group Structure",
    "text": "Operating with Group Structure\nWe often want to summarize our data in useful ways: these can be simple summaries like mean (average) or standard deviation or more complex operations (trend lines). In the world of dplyr, the summarize function is used whenever we want to reduce multiple rows to a single data point. Note how this differs from filter: filter drops rows, summarize combines and reduces them.\nFor example, if we want to get the number of male penguins in our data set, we can use the n() function, which counts the number of rows:\n\npenguins |&gt; summarize(number = n())\n\n# A tibble: 1 × 1\n  number\n   &lt;int&gt;\n1    344\n\n\nLook at all those penguins!\nThis is a relatively simple operation, but we can be a bit more complex: e.g., with the mean function:\n\npenguins |&gt; summarize(body_mass_avg = mean(body_mass_g))\n\n# A tibble: 1 × 1\n  body_mass_avg\n          &lt;dbl&gt;\n1            NA\n\n\nWait! Why didn’t that work?\nRecall that the penguins data had some missing (NA) values. When we ask R to compute the average, it can’t! Specifically, depending on the missing values, the mean could be anything, so R returns a missing (unknown) value for the mean as well. Many base R functions have this default behavior and have an optional flag for automatically removing NA values:\n\npenguins |&gt; summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  body_mass_avg\n          &lt;dbl&gt;\n1         4202.\n\n\nHere na.rm=TRUE means to remove all na values before computing the mean.\nThe summarize function is particularly powerful when applied groupwise: e.g., what is the average body mass by species? In dplyr world, this is a two-step operation:\n\npenguins |&gt; \n    group_by(species) |&gt;\n    summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE))\n\n# A tibble: 3 × 2\n  species   body_mass_avg\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Adelie            3701.\n2 Chinstrap         3733.\n3 Gentoo            5076.\n\n\nWe added the group_by operator here. Note that, on its own, group_by doesn’t really do anything:\n\npenguins |&gt; \n    group_by(species)\n\n# A tibble: 344 × 8\n# Groups:   species [3]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nWe have added some grouping metadata but the actual data does not get changed until the summarize step.\nWe can also group by more than one element:\n\npenguins |&gt; \n    group_by(species, sex) |&gt;\n    summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex    body_mass_avg\n  &lt;fct&gt;     &lt;fct&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Adelie    &lt;NA&gt;           3540 \n4 Chinstrap female         3527.\n5 Chinstrap male           3939.\n6 Gentoo    female         4680.\n7 Gentoo    male           5485.\n8 Gentoo    &lt;NA&gt;           4588.\n\n\nNote here that the result is still grouped and that only the last (sex) grouping was removed. That means that any future operations will be automatically grouped by species. If you want to remove all grouping structure, add the ungroup operator at the end:\n\npenguins |&gt; \n    group_by(species, sex) |&gt;\n    summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE)) |&gt;\n    ungroup()\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n  species   sex    body_mass_avg\n  &lt;fct&gt;     &lt;fct&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Adelie    &lt;NA&gt;           3540 \n4 Chinstrap female         3527.\n5 Chinstrap male           3939.\n6 Gentoo    female         4680.\n7 Gentoo    male           5485.\n8 Gentoo    &lt;NA&gt;           4588.\n\n\ngroup_by metadata is also useful when summary statistics are computed implicitly by other functions. E.g., if we want to get all penguins that are above average mass for their species, we might try the following:\n\npenguins |&gt; \n    group_by(species) |&gt;\n    filter(body_mass_g &gt;= mean(body_mass_g, na.rm=TRUE)) |&gt;\n    ungroup()\n\n# A tibble: 159 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           39.2          19.6               195        4675\n 4 Adelie  Torgersen           42            20.2               190        4250\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           42.5          20.7               197        4500\n 8 Adelie  Torgersen           46            21.5               194        4200\n 9 Adelie  Biscoe              35.9          19.2               189        3800\n10 Adelie  Biscoe              38.2          18.1               185        3950\n# ℹ 149 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nHere, R applies the summarization function mean groupwise for us.\nBefore we close out, let’s put this all together: what species has the largest difference in average body mass between the sexes?\nAnswer: Gentoo penguins have the largest sex difference in average body mass.\nBefore completing the Brightspace submission for this assignment, look up the source for this document on my GitHub (Hint: see the buttons on the sidebar) and see i) how I computed the answer; and ii) how I included it in the rendered text. This will be helpful as you begin preparing your first report."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa04.html#footnotes",
    "href": "archive/AY-2024-FALL/preassigns/pa04.html#footnotes",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRecall a factor is a vector with a fixed set of possible values, often used to represent categorical data. Here, we follow the NY DMV and allow M, F, and X values for sex, but, in general, representation of sex and gender in databases is a tricky problem. See this essay for a list of some of the complexity of real people. (This essay follows in a longer tradition of “the world is much more complicated than you would believe” essays: names, time, addresses. People - and the world we create - are infinitely complex.↩︎\nWe will say much more about R’s missing data model in class this week.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa05.html",
    "href": "archive/AY-2024-FALL/preassigns/pa05.html",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "",
    "text": "Due Date: 2024-09-25 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nLast week, we considered single-table verbs: these are appropriate for asking complex questions of a nicely formatted data frame. Sadly, we are rarely provided data frames suitable for every question we might seek to answer. Instead, we typically need to combine information from multiple sources. For instance, if we want to examine the relationship between demographics and electoral results, we will need to combine information from the US Census Bureau and local elections administrators. Or, if we want to investigate the relationship between a company’s financial status and its stock performance, we might need to combine information from multiple databases."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa05.html#basic-joins",
    "href": "archive/AY-2024-FALL/preassigns/pa05.html#basic-joins",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Basic Joins",
    "text": "Basic Joins\nThe basic operator of combining different tables is the join, patterned after SQL. Each join operates using some notion of “match” between tables. In the best case, this is done using a unique identifier - one universal and consistent name for each entity. Sadly, such perfect identifiers rarely exist. For instance, companies change their names and their ticker symbols somewhat regularly (e.g., Facebook becoming Meta)\nThe simplest join is the inner_join, which returns rows which match between the two tables:\n\nlibrary(dplyr)\nband_members\n\n# A tibble: 3 × 2\n  name  band   \n  &lt;chr&gt; &lt;chr&gt;  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\n\n\nband_instruments\n\n# A tibble: 3 × 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\n\nIn this case, the name column forms a unique ID, so we can use it for our join.\n\ninner_join(band_members, band_instruments)\n\nJoining with `by = join_by(name)`\n\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nWe see here that R automatically performed the join using the common column (name): if we want to be clearer, let’s specify the join element ourselves:\n\ninner_join(band_members, band_instruments, join_by(name))\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nHere join_by is a helper function that can be used to specify the form of the join used. In some contexts, the “common” column has different names in the two tables, so we can use a more explicit call to join_by:\n\nband_instruments2\n\n# A tibble: 3 × 2\n  artist plays \n  &lt;chr&gt;  &lt;chr&gt; \n1 John   guitar\n2 Paul   bass  \n3 Keith  guitar\n\n\nNote that this is the same as band_instruments, but with the name column changed to artist.\n\ninner_join(band_members, band_instruments2, join_by(name == artist))\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nI like to always use this final - most explicit - form, even when the column names are the same between the two tables (join_by(name == name)).\nLet’s look more closely at the result here: we return a table with 3 columns and two rows:\n\ninner_join(band_members, band_instruments, join_by(name == name))\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nThe three columns are pretty easy to understand:\n\nname is the (shared) name column from each table\nband comes from the band_members table\nplays comes from the band_instruments table.\n\nThe two rows are a bit trickier: each of our input tables had three rows, but there were only two “overlaps” so that’s what we get back from an inner_join. Specifically, we drop Mick [Jagger] from band_members because he doesn’t appear in band_instrumentsand we drop Keith [Richards] from band_instruments because he doesn’t appear in band_members.\nIn brief, an inner join is an intersection join. We only get rows back which have a match in both tables.\nOther join operators have complimentary behaviors: the full join (also sometimes called an outer join) is basically a union join. We get back all rows from both tables, regardless of whether a match has been found. But what happens with those unmatched rows?\n\nfull_join(band_members, band_instruments, join_by(name == name))\n\n# A tibble: 4 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n\nR fills in the “blanks” with NA values. Here, we can assume Mick [Jagger] plays an instrument, but it is unknown to R here.\nFinally, we have the intermediate left join, which keeps all rows from one table whether or not they have a match:\n\nleft_join(band_members, band_instruments, join_by(name == name))\n\n# A tibble: 3 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\n\nHere we keep Mick because he is in band_members, even though he is missing from band_instruments. Conversely, we drop Keith because he isn’t in band_members (even though he is in band_instruments).\nR also provides a right_join, but it’s not really different: it’s just a “flipped” left_join: left_join(x, y) == right_join(y, x).\nThe following image1 summarizes the different types of joins:\n\nThe anti_join returns elements that appear in one data set, but not the other. It’s rarer, but occasionally useful."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa05.html#joins-with-repeats",
    "href": "archive/AY-2024-FALL/preassigns/pa05.html#joins-with-repeats",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Joins with Repeats",
    "text": "Joins with Repeats\nIn the previous examples, we have seen joins that have a “one-to-one” (inner) or possibly “one-to-none” (full, left) structure. In many circumstances, we find ourselves with a “one-to-many” type structure, even when both data sets are “tidy”. This typically occurs because different data sets have different models of what a “unit” is. For example, consider a hypothetical instructor who has i) a table with student names and contact information; and ii) a table with grades on different assignments.\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 3\n)\n\ngrades &lt;- tribble(\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    3,           \"B\",            50,\n    1,           \"C\",            95,\n    2,           \"C\",            50,\n    3,           \"C\",            80\n)\n\nWhat happens if we join these?\n\ninner_join(students, grades, join_by(id == student_id))\n\n# A tibble: 9 × 5\n  name     email                id assignment_id grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1 A               100\n2 Bernard  bernard@cuny.edu      1 B                95\n3 Bernard  bernard@cuny.edu      1 C                95\n4 Hunter   hunter@cuny.edu       2 A                95\n5 Hunter   hunter@cuny.edu       2 B                80\n6 Hunter   hunter@cuny.edu       2 C                50\n7 John Jay john.jay@cuny.edu     3 A                80\n8 John Jay john.jay@cuny.edu     3 B                50\n9 John Jay john.jay@cuny.edu     3 C                80\n\n\n(Note here that we need the explicit join_by since the column names don’t match between the two tables: id in students gets joined to student_id. This pattern of id in table tbl getting joined to tbl_id elsewhere is quite common in database design.)\nWe get repeats of the student rows: for each valid student-grade pair, we have a row. From here, we can compute final grades:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    group_by(name, email, id) |&gt;\n    summarize(final_avg = mean(grade)) |&gt;\n    mutate(final_grade = \n               case_when(final_avg &gt; 90 ~ \"A\", \n                         final_avg &gt; 80 ~ \"B\", \n                         final_avg &gt; 70 ~ \"C\", \n                         final_avg &gt; 60 ~ \"D\", \n                         TRUE ~ \"F\")) # In a case_when, TRUE == \"else\"\n\n`summarise()` has grouped output by 'name', 'email'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 5\n# Groups:   name, email [3]\n  name     email                id final_avg final_grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n1 Bernard  bernard@cuny.edu      1      96.7 A          \n2 Hunter   hunter@cuny.edu       2      75   C          \n3 John Jay john.jay@cuny.edu     3      70   D          \n\n\nIn this case, everything works well. But let’s try a slightly trickier case, with some students who never fail to submit certain assignments.\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 3\n)\n\ngrades &lt;- tribble(\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    1,           \"C\",            95,\n    3,           \"C\",            80\n)\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    group_by(name, email, id) |&gt;\n    summarize(final_avg = mean(grade)) |&gt;\n    mutate(final_grade = \n               case_when(final_avg &gt; 90 ~ \"A\", \n                         final_avg &gt; 80 ~ \"B\", \n                         final_avg &gt; 70 ~ \"C\", \n                         final_avg &gt; 60 ~ \"D\", \n                         TRUE ~ \"F\"))\n\n`summarise()` has grouped output by 'name', 'email'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 5\n# Groups:   name, email [3]\n  name     email                id final_avg final_grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n1 Bernard  bernard@cuny.edu      1      96.7 A          \n2 Hunter   hunter@cuny.edu       2      87.5 B          \n3 John Jay john.jay@cuny.edu     3      80   C          \n\n\nWhy did the final grades go up after we deleted rows?\n\ninner_join(students, \n           grades, \n           join_by(id == student_id))\n\n# A tibble: 7 × 5\n  name     email                id assignment_id grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1 A               100\n2 Bernard  bernard@cuny.edu      1 B                95\n3 Bernard  bernard@cuny.edu      1 C                95\n4 Hunter   hunter@cuny.edu       2 A                95\n5 Hunter   hunter@cuny.edu       2 B                80\n6 John Jay john.jay@cuny.edu     3 A                80\n7 John Jay john.jay@cuny.edu     3 C                80\n\n\nThe “missing” assignments for Hunter and John Jay aren’t reported as zeros - they are just ignored! And hence R takes an average over the two assignments where these students did well, not all three assignments. We’ll talk about one way to fix this below, but for now I’m just flagging it as a possible issue that can come up with missing data and joins. (Here the rows were missing, so it’s harder to catch than a plain NA; better data management would have included a “0” row instead of deleting them, but we don’t always get to assume super well-organized data.)\nSo far, our join results have been relatively straightforward because we have had ‘good’ unique identifiers. If we find ourselves in a situation where we lack unique IDs, things can go wrong quickly:\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 2  # Accidentally repeat an ID\n)\ngrades &lt;- tribble( # Back to the complete data\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    3,           \"B\",            50,\n    1,           \"C\",            95,\n    2,           \"C\",            50,\n    3,           \"C\",            80\n)\n\nfull_join(students, \n          grades, \n          join_by(id == student_id))\n\nWarning in full_join(students, grades, join_by(id == student_id)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 2 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 12 × 5\n   name     email                id assignment_id grade\n   &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 Bernard  bernard@cuny.edu      1 A               100\n 2 Bernard  bernard@cuny.edu      1 B                95\n 3 Bernard  bernard@cuny.edu      1 C                95\n 4 Hunter   hunter@cuny.edu       2 A                95\n 5 Hunter   hunter@cuny.edu       2 B                80\n 6 Hunter   hunter@cuny.edu       2 C                50\n 7 John Jay john.jay@cuny.edu     2 A                95\n 8 John Jay john.jay@cuny.edu     2 B                80\n 9 John Jay john.jay@cuny.edu     2 C                50\n10 &lt;NA&gt;     &lt;NA&gt;                  3 A                80\n11 &lt;NA&gt;     &lt;NA&gt;                  3 B                50\n12 &lt;NA&gt;     &lt;NA&gt;                  3 C                80\n\n\nIn this case, R is kind enough to warn us that a “many-to-many” join has happened (joining multiple students to one grade and multiple grades to one student). This is a very good warning and it highlights a true error here. If faced with data like this, you may not be able to address it with fixing the underlying data, but at least you know something has gone awry."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa05.html#compound-joins",
    "href": "archive/AY-2024-FALL/preassigns/pa05.html#compound-joins",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Compound Joins",
    "text": "Compound Joins\nOften, data lack a unique identifier, but you can piece one together with several columns: that is, taken on its own, no column is unique, but the tuples formed by comining several columns are unique, e.g., data with year, month, and day columns.\n\nrevenues &lt;- tribble(\n    ~year, ~month, ~day, ~revenue,\n    2024,  09,     22,   100,\n    2024,  09,     23,   200,\n    2024,  10,     22,   200,\n    2024,  10,     22,   200,\n    2025,  09,     22,   500\n    )\n\nexpenses &lt;- tribble(\n    ~year, ~month, ~day, ~expenses,\n    2024,  09,     22,   -200,\n    2024,  09,     23,   -200,\n    2024,  10,     22,   -200,\n    2024,  10,     23,   -200,\n    2025,  09,     22,   -300\n    )\n\nIn this case, a simple join on any one column goes astray:\n\ninner_join(revenues, expenses, join_by(day == day))\n\nWarning in inner_join(revenues, expenses, join_by(day == day)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 14 × 7\n   year.x month.x   day revenue year.y month.y expenses\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1   2024       9    22     100   2024       9     -200\n 2   2024       9    22     100   2024      10     -200\n 3   2024       9    22     100   2025       9     -300\n 4   2024       9    23     200   2024       9     -200\n 5   2024       9    23     200   2024      10     -200\n 6   2024      10    22     200   2024       9     -200\n 7   2024      10    22     200   2024      10     -200\n 8   2024      10    22     200   2025       9     -300\n 9   2024      10    22     200   2024       9     -200\n10   2024      10    22     200   2024      10     -200\n11   2024      10    22     200   2025       9     -300\n12   2025       9    22     500   2024       9     -200\n13   2025       9    22     500   2024      10     -200\n14   2025       9    22     500   2025       9     -300\n\n\nNote the warning!\nIn this scenario, we should really “tidy” up the data by combining the date information, which is spread across three columns, into a single column, but we have the alternative option of a compound join:\n\ninner_join(revenues, expenses, \n           join_by(day == day, \n                   month == month, \n                   year == year))\n\n# A tibble: 5 × 5\n   year month   day revenue expenses\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1  2024     9    22     100     -200\n2  2024     9    23     200     -200\n3  2024    10    22     200     -200\n4  2024    10    22     200     -200\n5  2025     9    22     500     -300\n\n\nHere, as with filter, the list of conditions looks for an intersection: we want all three parts of the date to match."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa05.html#pivots",
    "href": "archive/AY-2024-FALL/preassigns/pa05.html#pivots",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Pivots",
    "text": "Pivots\nFinally, we may want to re-arrange the output of a join. Returning to our grades example from above:\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 3\n)\n\ngrades &lt;- tribble(\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    3,           \"B\",            50,\n    1,           \"C\",            95,\n    2,           \"C\",            50,\n    3,           \"C\",            80\n)\n\ngrade_book &lt;- inner_join(students, \n                         grades, \n                         join_by(id == student_id))\n\ngrade_book\n\n# A tibble: 9 × 5\n  name     email                id assignment_id grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1 A               100\n2 Bernard  bernard@cuny.edu      1 B                95\n3 Bernard  bernard@cuny.edu      1 C                95\n4 Hunter   hunter@cuny.edu       2 A                95\n5 Hunter   hunter@cuny.edu       2 B                80\n6 Hunter   hunter@cuny.edu       2 C                50\n7 John Jay john.jay@cuny.edu     3 A                80\n8 John Jay john.jay@cuny.edu     3 B                50\n9 John Jay john.jay@cuny.edu     3 C                80\n\n\nThis isn’t really how we like to see gradebooks: a “wider” format, with a column for each assignment, may be more preferable. In this case, we want to use the pivot_wider column from the tidyr package.\npivot_wider takes a few key arguments:\n\nid_cols which columns that (taken together) uniquely identify a row in the final table\nnames_from: where should we get the column names of the new table\nvalues_from: where should we get the values of the new table\n\nThis is maybe easier by example:\n\ngrade_book |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from=assignment_id,\n                values_from=grade)\n\n# A tibble: 3 × 6\n  name     email                id     A     B     C\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95\n2 Hunter   hunter@cuny.edu       2    95    80    50\n3 John Jay john.jay@cuny.edu     3    80    50    80\n\n\nThis pivot trick is particularly useful for finding missing rows, like those that tripped us up earlier:\n\ngrades &lt;- tribble( # Implicit missing values\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    1,           \"C\",            95,\n    3,           \"C\",            80\n)\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade)\n\n# A tibble: 3 × 6\n  name     email                id     A     B     C\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95\n2 Hunter   hunter@cuny.edu       2    95    80    NA\n3 John Jay john.jay@cuny.edu     3    80    NA    80\n\n\nHere, our missing values are now explicit!\nWe can also explicitly fill the NA with a value of our choice, here 0:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0)\n\n# A tibble: 3 × 6\n  name     email                id     A     B     C\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95\n2 Hunter   hunter@cuny.edu       2    95    80     0\n3 John Jay john.jay@cuny.edu     3    80     0    80\n\n\nThere is also an inverse operator pivot_longer which takes a wide table (like this) and makes it longer.\nTo complete our grade book example, we might want to take the average across the three grade columns:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0) |&gt;\n    group_by(name) |&gt;\n    mutate(final_avg = mean(c_across(A:C)))\n\n# A tibble: 3 × 7\n# Groups:   name [3]\n  name     email                id     A     B     C final_avg\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95      96.7\n2 Hunter   hunter@cuny.edu       2    95    80     0      58.3\n3 John Jay john.jay@cuny.edu     3    80     0    80      53.3\n\n\nNote here that we need to use a mutate since our final grade book has the same number of rows before and after we add the final average column. The c_across column here is a variant of the standard c function used to combine scalars: here we’re creating a new length-3 vector of the student’s three grades and passing it to the mean function.\nWhat is group_by(name) doing here? See what happens without it:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0) |&gt;\n    mutate(final_avg = mean(c_across(A:C)))\n\n# A tibble: 3 × 7\n  name     email                id     A     B     C final_avg\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95      69.4\n2 Hunter   hunter@cuny.edu       2    95    80     0      69.4\n3 John Jay john.jay@cuny.edu     3    80     0    80      69.4\n\n\nRecall that mean is a summarization function - it will combine data from across rows if no grouping structure is present. Since we want seperate averages for each student, we need a group_by. In this case, name is a unique identifier for each student, so we can group on it. We also have the rowwise() helper, which automatically creates group structure with each group a separate row. If you don’t have a clean unique identifier, or just can’t think of one easily, this is sometimes a useful helper.\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0) |&gt;\n    rowwise() |&gt;\n    mutate(final_avg = mean(c_across(A:C)))\n\n# A tibble: 3 × 7\n# Rowwise: \n  name     email                id     A     B     C final_avg\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95      96.7\n2 Hunter   hunter@cuny.edu       2    95    80     0      58.3\n3 John Jay john.jay@cuny.edu     3    80     0    80      53.3\n\n\nIn class, we will explore joins in more detail by combining the flights data with plane, airport, and weather factors.\nPlease now go fill out the weekly quiz on Brightspace."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa05.html#footnotes",
    "href": "archive/AY-2024-FALL/preassigns/pa05.html#footnotes",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAdapted from Data Carpentry↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa03.html",
    "href": "archive/AY-2024-FALL/preassigns/pa03.html",
    "title": "STA 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "",
    "text": "Due Date: 2024-09-11 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nIt is now time for us to start programming in R properly. In this week’s pre-assignment, we’re going to focus on three basic elements of programming in R:\nBefore we get into these however, let’s introduce the feedback mechanism used throughout this pre-assignment. Throughout this page, you will encounter blocks like the below:\nWait for the exercise to fully load (the blue dot next to Run Code will disappear) and then try giving correct and incorrect solutions.\nThese little code-blocks throughout this pre-assignment will be used to give similar feedback. You can always hit Show Solution to get the correct answer. The feedback engine is a bit overly picky at times, so if your answer is substantially similar to the official solution, I wouldn’t worry too much.\nYou will see the R output sometimes has a [1] before it. Don’t worry about that until you get to the section on vectorized semantics below.\nBlocks that aren’t listed as “Exercise” are interactive snippets. Feel free to adjust the code to check your understanding."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa03.html#calculator-math-with-r",
    "href": "archive/AY-2024-FALL/preassigns/pa03.html#calculator-math-with-r",
    "title": "STA 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "Calculator Math with R",
    "text": "Calculator Math with R\nLet’s start by using R as a calculator. R implements all the basic operations of arithmetic:\n\na + b: (binary) addition \\(a + b\\)\na - b: (binary) subtraction \\(a - b\\)\n*: (binary) multiplication \\(ab\\)\n/: (binary) division \\(a/b\\)\n-b: (unary) negation \\(-b\\)\n\nYou can type integers and decimals in the usual manner:\n\n\n\n\n\n\n\n\nCompute \\(5! = 5 * 4 * 3 * 2 * 1\\) using R:\n\n\n\n\n\n\n\n\n\nThe blanks should be filled with 3 and 1\n\n\n5 * 4 * 3 * 2 * 1\n5 * 4 * 3 * 2 * 1\n\n\n\n\n\n\nExponentials (powers) can be implemented with either a double star ** or a carrot ^:\n\n\n\n\n\n\n\n\nI tend to prefer the carrot ^ as its one fewer character.\nIn general, R respects the standard “PEMDAS” order of operations:\n\nParentheses\nExponentiation\nMultiplication and Division\nAddition and Subtraction\n\nSo we can compute \\(3 * (2 + 1)\\) as:\n\n\n\n\n\n\n\n\n\nExercises\nCompute the following algebraic expressions using R:\n\n\\[3 * 2^2\\]\n\n\n\n\n\n\n\n\n\n\n\n\n3 * 2^2\n3 * 2^2\n\n\n\n\n\n\n\n\\[(3 * 2)^2\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(3 * 2)^2\n(3 * 2)^2\n\n\n\n\n\n\n\n\\[3 + 2 - 1 + 4\\]\n\n\n\n\n\n\n\n\n\n\n\n\n3 + 2 - 1 + 4\n3 + 2 - 1 + 4\n\n\n\n\n\n\n\n\\[3 + 2 - (1 + 4)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n3 + 2 - (1 + 4)\n3 + 2 - (1 + 4)\n\n\n\n\n\n\n\nExecution in RStudio\nNow, redo these exercises in the RStudio Console. At each step, type the relevant code next to the &gt; prompt and hit enter to execute the command.\nR has greedy execution. When you hit enter, R tries its best to execute the whole line of code. If you enter an incomplete line of code, e.g., 3 +, R will change the &gt; prompt to a + prompt, indicating there is more to be done.\nCompare\n\n\n\n\n\n\n\n\nwith\n\n\n\n\n\n\n\n\nIn the first example, 3- is not a complete mathematical statement, so R knew there had to be more code and continued to await input. In the second, 3 is a perfectly valid (if very simple) mathematical command on its own, so R simply executes it as is.\nContinuation prompts from dangling math are quite rare, but you will often find yourself in this scenario if you let parentheses become mismatched. If you are ever stuck and can’t figure out how to appease R, simply type Cntrl-C to “interrupt” the command and get back to the standard prompt."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa03.html#function-calls",
    "href": "archive/AY-2024-FALL/preassigns/pa03.html#function-calls",
    "title": "STA 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "Function Calls",
    "text": "Function Calls\nR comes built in with a quite robust mathematical library. You can in general call a function like this:\n\n\n\n\n\n\n\n\n(R also comes with the mathematical constant \\(\\pi\\) pre-loaded.)\nIn general a function call is a “name” immediately followed by parentheses. If a function takes input or arguments, the input is located between the parentheses, separated by commas.\nSo above, cos(pi) implements the math \\(\\cos(\\pi)\\).\nUseful built-in functions are:\n\nsin - in radians\ncos - in radians\nexp - base \\(e\\) exponential\nlog - by default this is the natural logarithm (\\(\\ln\\))\nsqrt\nabs - absolute value\nfactorial - \\(n! = n * (n - 1) * (n - 2) * \\dots * 3 * 2 * 1\\)\n\nUse the built-in functions to compute \\(5!\\):\n\n\n\n\n\n\n\n\n\n\n\nfactorial(5)\nfactorial(5)\n\n\n\n\n\n\n\nExercises\nUsing these built-in functions, compute the following arithmetic expressions:\n\n\\[\\cos^2(\\pi / 4)\\]\n\n\n\n\n\n\n\n\n\n\n\n\ncos(pi/4)^2\ncos(pi/4)^2\n\n\n\n\n\n\n\n\\[e^{\\log(\\pi) + 3}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nexp(log(pi) + 3)\nexp(log(pi) + 3)"
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa03.html#vectorized-semantics",
    "href": "archive/AY-2024-FALL/preassigns/pa03.html#vectorized-semantics",
    "title": "STA 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "Vectorized Semantics",
    "text": "Vectorized Semantics\nA distinguishing feature of R is its vectorized semantics. By default, R wants to operate on collections of data - not individual values (scalars). You’ve seen some evidence of this already. Whenever you run a bit of math, R puts [1] at the front of the output. This is R helping you count the number of elements in the solution; it’s been pretty trivial so far, as all our calculations have returned a single number. But this is about to change!\nThe easiest vectors to create in R are sequences: e.g., the list of numbers from 1 to 10:\n\n\n\n\n\n\n\n\nHere, our output is a vector of 10 elements. R still tells us where the vector starts (at the first element) but nothing else. Change this code to create the first 100 elements and read R’s output. Do you understand the output?\nWhen R starts a new line of output, it tells you where you are in the vector. In RStudio, type letters to see the built-in vector of letters; this is a nice example of how the position information can be helpful in sorting through printed output.\nBy default, R operates on vectors elementwise:\n\n\n\n\n\n\n\n\nHere the sqrt function is applied to each element separately.\nWhen two vectors are combined, the operation also works elementwise:\n\n\n\n\n\n\n\n\n(Note that the sequence operator (:) has higher precedence than most arithmetic so this does “what you’d expect.”)\nThings get weird if the two vectors are of different lenghts:\n\n\n\n\n\n\n\n\nUnder the hood, R “recycles” 3 to be a vector of length 5 and then operators elementwise. That is, R computes\n\n3 + 1\n3 + 2\n3 + 3\n3 + 4\n3 + 5\n\nand combines the results.\nThis in general gives useful results, but the results can be quite alarming if combining vectors of unaligned size:\n\n\n\n\n\n\n\n\nThankfully, R gives a warning that something weird happened. (This might seem annoying, but warnings are great! They help you find likely errors before anything too bad happens. Most experienced programmers wish R had more built-in warnings.)\nIt’s worth distinguishing warnings from errors. Errors occur when R absolutely cannot and will not execute your command:\n\n\n\n\n\n\n\n\nIn this case, it is impossible to add the number 3 to a letter, so R throws an error.\nWarnings are hints of possible problems, but do not prevent execution. When dealing with external software and packages, you will often get warnings about old versions of software. These are encouraging you to update, but unless you see an error, things probably worked out ok.\nSome of R’s built-in functions can be used to “summarize” a vector down to a single scalar (or, more precisely, a length 1 vector). These include sum, max, min, and mean. For example, we can compute the sum of the first 100 numbers as follows:\n\n\n\n\n\n\n\n\nApocryphally, a young C.F. Gauss did this calculation in his head to the great surprise of his school teacher. We might not have Gauss’s skills at arithmetic, but we can do quite a lot with R.\nFor example, the famous “Bessel problem” of mathematics is to compute\n\\[ \\sum_{n=1}^{\\infty} \\frac{1}{n^2} = 1 + \\frac{1}{2^2} + \\frac{1}{3^2} + \\dots\\]\nEuler showed, somewhat remarkably, that the answer is \\(\\pi^2 / 6\\). We won’t repeat Euler’s analysis here, but let’s confirm it using R.\n\n\n\n\n\n\n\n\nPretty good alignment!\nDo you understand everything that happened here? If so, you’re ready for next week’s class. Go ahead and fill out this week’s Brightspace quiz."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa09.html",
    "href": "archive/AY-2024-FALL/preassigns/pa09.html",
    "title": "STA 9750 Week 9 Pre Assignment: Flat-File Data Ingest",
    "section": "",
    "text": "We now turn to the next “unit” of this course - acquiring data and preparing it for further analysis. In what follows, our goal will be go get data to a “tidy” format suitable for use with tidyverse tools like dplyr and ggplot2. Unfortunately, this problem becomes a rather difficult one. While every “tidy” data set shares certain characteristics, non-tidy data, by definition, won’t. Still - we are analysts and data scientists - so we soldier on and begin to work with the nasty and difficult data the real world often presents us with.\nWe are spending two weeks on this topics of “getting data into R”. Roughly, this will break into two sections:\nIn this pre-assignment, we’re focusing on the first part of the first section: reading data from a file into R. This will be relatively fast - we’ve already done some of it this semester - but it’s an essential skill set."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa09.html#what-is-a-file",
    "href": "archive/AY-2024-FALL/preassigns/pa09.html#what-is-a-file",
    "title": "STA 9750 Week 9 Pre Assignment: Flat-File Data Ingest",
    "section": "What is a File?",
    "text": "What is a File?\nBefore we work on getting data from files into R, it’s worth taking a moment to review what a file actually is. Modern technology often tries to hide the “true nature” of things behind ever increasing stacks of accounts and cloud services, but we’re going back to basics.\nA file is a long series of bits, with a well-defined beginning and end.\nThat’s it - files aren’t required to have extensions, formats, or anything else. Computers associated each file with a name, organized into a file system, but this all lives outside the file.\nThat general definition - lots of bits - is not particularly helpful for sharing of content and data, so files are conventionally shared with formats. A set of rules that tell programs how to interpret the bits of a file. Essentially, a file format is a social convention that exists “around” the file, enabling us to work with it productively and easily.\nA huge number of file formats exist - images, movies, text documents, financial transaction records, data bases - and you thankfully do not need to know them all. We can generally divide these into two buckets:\n\nPlain text formats. These are formats which are designed to be easily readable across a wide range of tools. This flexibility makes plain text formats particularly popular among tech-types, as users are not forced into using a piece of (typically expensive) software.[^1]\nMost of the files we have seen in this course are plain-text: qmd files, csv files, R files, etc. We have edited them primarily in RStudio, but you could just as easily access them in tools like VS Code, Eclipse, Jupyter, or even rather primitive tools like Notepad.\nBinary formats. These are typically specialized formats used to encode complex data formats. Image formats, a common example of binary formats, are necessarily more complex than anything we could represent in text. Because of this complexity, binary formats tend to require specialized software to read and edit. For popular formats, suitable software can be found on almost any machine: e.g., almost any computer you will use comes with some PDF-reading software pre-installed, even if you later choose to upgrade it to a more advanced tool.\n\nSome files blend these two elements, e.g., an HTML page with a mix of plain text and an embedded image, but it’s still a useful distinction.\nGiven all that, how do our computers actually know what to do with a file? We open and close files all day and rarely have to tell our computers what software to use or how to read the file.\nThere are two general conventions, one more popular in Unix-type systems (MacOS and Linux) and one more popular on Windows-like machines.\nMost file formats - and especially most binary formats - begin with a “magic string” that clearly states what format is used to encode the file. By reading this, the operating system can tell what type of file format is used and call the appropriate software. For instance, PDF files must start with the string %PDF - 1.4. When your computer reads this, it knows it has a found a PDF and calls the appropriate software.\nThis type of convention is powerful, though it sometimes struggles with very rare formats if the destination machine has never been programmed to handle that type of file. It has the advantage that the magic string is inside the file. If the file is transferred without corruption, it’s impossible to “misplace” the file format information.\nThe other convention used to identify file formats is the extension. A file extension is a set of a few letters following a period at the end of the file name. We often adopt these extensions into our everyday discourse, e.g., a “PDF” file or a “GIF”. It is important to note that the extension is not part of the file and that it is a second piece of information in addition to the file contents. As such, this is a second place where things can be corrupted and mistakes can be made. This isn’t a huge problem - despite our best efforts, computers are pretty reliable - but it is something to be aware of. Importantly, we can change the extension without ever editing the file itself; this is a double-edged sword.\nIn an attempt to be more user-friendly, certain operating systems - especially Microsoft Windows - are quite dogmatic about the use of file extensions, sometimes even going so far as to “hide” the extension from the user. If you’ve ever seen a file with a name like “doc.html.html”, it’s almost always because a user tried to add or change an extension, but Windows hid the “true” extension away from them. (If you’re on a Windows machine, I recommend setting Windows to always show the extensions to avoid this type of mistake.)\nExtension-based formatting is popular and, when it works well, can be quite efficient, but it’s worth re-emphasizing: the truth is in the bits, not the name. Pragmatically, the true test of whether a file is in a format is whether software designed to read that format can handle it."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa09.html#so-what",
    "href": "archive/AY-2024-FALL/preassigns/pa09.html#so-what",
    "title": "STA 9750 Week 9 Pre Assignment: Flat-File Data Ingest",
    "section": "So What?",
    "text": "So What?\nAt this point, you should probably be asking yourself why any of this matters. The key is to understand that a format is just a set of rules for interpreting the bits of a file. To read a file into R, we only then need to find a function that knows how to map that format into R.\nPeople love R, and many such “reader” functions exist. We have already seen\n\nreadr::read_csv\nsf::read_sf\n\nseveral times in this course. But there are many others. The readr package provides functions for reading standard “plain-text” formats, including:\n\nreadr::read_csv (“comma separated values”)\nreadr::read_tsv (“tab separated values”)\nreadr::read_delim (read files with an arbitrary delimiter, e.g., semi-colon separated values)\nreadr::read_fwf (read files where columns have a fixed width)\n\nSome of these functions are quite advanced, and can even automatically download and decompress files from web services, if you pass it a URL instead of a (local) file name.[^2]\nThe haven package provides tools for reading data formats generated by other statistical software:\n\nhaven::read_stata\nhaven::read_spss\nhaven::read_sas\n\nIn general, if you are reading a not-entirely bespoke file format into R, there is almost always a function for doing so. If you come across a format for your course project that you are struggling to read, ask the course staff."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa09.html#reading-in-action",
    "href": "archive/AY-2024-FALL/preassigns/pa09.html#reading-in-action",
    "title": "STA 9750 Week 9 Pre Assignment: Flat-File Data Ingest",
    "section": "Reading in Action",
    "text": "Reading in Action\nWe have already read many files in this course, and when all goes well, the process is quite seamless:\n\nlibrary(readr)\ncars &lt;- read_csv(\"https://github.com/tidyverse/readr/raw/main/inst/extdata/mtcars.csv\")\n\nRows: 32 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (11): mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf the data provider produces well-formatted files, you usually do not require any more thought than identifying the right read function (possibly from a package which you need to install and load as well) and finding the file path. Unfortunately, data providers are rarely as perfect as we might hope.\nCon-Ed, NYC’s electric utility, provides electric usage data in 15-minute intervals. My usage data for 2024-10-24 looks something like this:\n\n\n\nName,MICHAEL WEYLANDT\nAddress,ADDRESS REDACTED\nAccount Number,ACCOUNT NUMBER REDACTED\nService,Service 1\n\nTYPE,DATE,START TIME,END TIME,USAGE (kWh),NOTES\nElectric usage,2024-10-24,00:00,00:14,0.08,\nElectric usage,2024-10-24,00:15,00:29,0.08,\nElectric usage,2024-10-24,00:30,00:44,0.08,\nElectric usage,2024-10-24,00:45,00:59,0.08,\nElectric usage,2024-10-24,01:00,01:14,0.09,\nElectric usage,2024-10-24,01:15,01:29,0.08,\nElectric usage,2024-10-24,01:30,01:44,0.08,\nElectric usage,2024-10-24,01:45,01:59,0.1,\nElectric usage,2024-10-24,02:00,02:14,0.11,\nElectric usage,2024-10-24,02:15,02:29,0.11,\nElectric usage,2024-10-24,02:30,02:44,0.1,\nElectric usage,2024-10-24,02:45,02:59,0.08,\nElectric usage,2024-10-24,03:00,03:14,0.08,\n\n\nIt’s clear that this file is “csv-ish”, but is not a standard CSV file. ConEd includes a “header” of questionable utility. While we could edit this file by hand to only include the “data parts”, we can also adjust our file reading code.\nThe default read.csv function in R fails on this file:\n\nread.csv(\"mw_coned_20241024.csv\")\n\nError in read.table(file = file, header = header, sep = sep, quote = quote, : more columns than column names\n\n\nThis message is a bit opaque, but it’s essentially saying that there are more columns in the middle of the file (where the data “should” be) than there are names in the first (blank) line of the file.\nThe improved read_csv function from the readr package tries harder and seems to succeed, but it also fails:\n\nlibrary(readr)\nread_csv(\"mw_coned_20241024.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 38 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, MICHAEL WEYLANDT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 38 × 2\n   Name           `MICHAEL WEYLANDT`                        \n   &lt;chr&gt;          &lt;chr&gt;                                     \n 1 Address        ADDRESS REDACTED                          \n 2 Account Number ACCOUNT NUMBER REDACTED                   \n 3 Service        Service 1                                 \n 4 TYPE           DATE,START TIME,END TIME,USAGE (kWh),NOTES\n 5 Electric usage 2024-10-24,00:00,00:14,0.08,              \n 6 Electric usage 2024-10-24,00:15,00:29,0.08,              \n 7 Electric usage 2024-10-24,00:30,00:44,0.08,              \n 8 Electric usage 2024-10-24,00:45,00:59,0.08,              \n 9 Electric usage 2024-10-24,01:00,01:14,0.09,              \n10 Electric usage 2024-10-24,01:15,01:29,0.08,              \n# ℹ 28 more rows\n\n\nIf you look at this closely, it thinks we only have a two column file. But the data clearly has several more columns than that.\nThe warning message here is helpful: it encourages us to use the problems() function to get more information on possible issues:\n\nlibrary(readr)\ncon_ed &lt;- read_csv(\"mw_coned_20241024.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 38 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, MICHAEL WEYLANDT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproblems(con_ed)\n\n# A tibble: 35 × 5\n     row   col expected  actual    file                                         \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;                                        \n 1     5     6 2 columns 6 columns /Users/weylandt/sta9750/archive/AY-2024-FALL…\n 2     6     6 2 columns 6 columns /Users/weylandt/sta9750/archive/AY-2024-FALL…\n 3     7     6 2 columns 6 columns /Users/weylandt/sta9750/archive/AY-2024-FALL…\n 4     8     6 2 columns 6 columns /Users/weylandt/sta9750/archive/AY-2024-FALL…\n 5     9     6 2 columns 6 columns /Users/weylandt/sta9750/archive/AY-2024-FALL…\n 6    10     6 2 columns 6 columns /Users/weylandt/sta9750/archive/AY-2024-FALL…\n 7    11     6 2 columns 6 columns /Users/weylandt/sta9750/archive/AY-2024-FALL…\n 8    12     6 2 columns 6 columns /Users/weylandt/sta9750/archive/AY-2024-FALL…\n 9    13     6 2 columns 6 columns /Users/weylandt/sta9750/archive/AY-2024-FALL…\n10    14     6 2 columns 6 columns /Users/weylandt/sta9750/archive/AY-2024-FALL…\n# ℹ 25 more rows\n\n\nWe see here that R expected only two columns, based on the top section of the file, but every row of “substance” beyond the ConEd-special header actually has six columns. Note that R is giving us a warning here - it could conceivably continue, here by smashing together the extra columns - so it does so. This type of warning is not uncommon in reading malformatted data. It is your responsibility as an analyst to investigate the warnings and see if they are true signs of trouble or false positives. A particularly common warning, which you have already encountered in this course, is when data sets use a non-standard way to encode NA values.\nIn this case, there’s actually an easy fix - we just want to skip those first rows for the header:\n\nlibrary(readr)\ncon_ed &lt;- read_csv(\"mw_coned_20241024.csv\", skip = 6)\n\nRows: 34 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): TYPE\ndbl  (1): USAGE (kWh)\nlgl  (1): NOTES\ndate (1): DATE\ntime (2): START TIME, END TIME\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe see lots of useful information in this read out. Our data has\n\n34 rows\n6 columns\nComma delimited fields\nThe columns types are:\n\nCharacter (1)\nDouble / Numeric (1)\nLogical (1)\nDate (1)\nTime (2)\n\n\nHere the logical column (NOTES) is a bit of a red herring; it’s all empty for this file, so R interprets as the simplest data type that fits,\nFrom here, I’m going to turn off printing of the data import message to keep this file readable, but you should always at least quickly eyeball them when importing a new data set.\nAt this point, we’re ready to proceed with tidying up our data. The USAGE (kWh) column is clearly the most important to us, but R doesn’t like column names with spaces and punctuation, so let’s go ahead and manually rename it:\n\nlibrary(readr); library(dplyr)\ncon_ed &lt;- read_csv(\"mw_coned_20241024.csv\", skip = 6) |&gt;\n    rename(usage = `USAGE (kWh)`)\n\nRecall that we have to use double back ticks to surround “weird” names. We can also drop the unused NOTES column and clean up the start and end time columns as well.\n\nlibrary(readr); library(dplyr)\ncon_ed &lt;- read_csv(\"mw_coned_20241024.csv\", skip = 6) |&gt;\n    rename(usage = `USAGE (kWh)`, \n           start_time = `START TIME`,\n           end_time = `END TIME`) |&gt;\n    select(-NOTES)\n\nWe now have nice tidy data that we can use to make a plot:\n\nlibrary(readr); library(dplyr); library(ggplot2)\nread_csv(\"mw_coned_20241024.csv\", skip = 6) |&gt;\n    rename(usage = `USAGE (kWh)`, \n           start_time = `START TIME`,\n           end_time = `END TIME`) |&gt;\n    select(-NOTES) |&gt;\n    ggplot(aes(x=start_time, y=usage)) + \n    geom_line()\n\n\n\n\n\n\n\n\nCan you see i) when my phone finished charting overnight; and ii) when I started to make breakfast on my electric stovetop?\n\n\n\n\n\n\nConEd Practice\n\n\n\nIf you have your own ConEd account, download your Energy Usage data file by hand and import it into R. Can you find patterns of your daily life in this data?\n\n\nAfter finishing this document, complete the Weekly Pre-Assignment Quiz on Brightspace."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini01.html",
    "href": "archive/AY-2024-FALL/miniprojects/mini01.html",
    "title": "STA 9750 Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "Released to Students: 2024-09-12\nInitial Submission: 2024-09-25 11:45pm ET on GitHub and Brightspace\n\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-09-26 on GitHub\nPeer Feedback Due: 2024-10-02 11:45pm ET on GitHub"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini01.html#welcome-to-mini-projects",
    "href": "archive/AY-2024-FALL/miniprojects/mini01.html#welcome-to-mini-projects",
    "title": "STA 9750 Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Welcome to STA 9750 Mini Projects!",
    "text": "Welcome to STA 9750 Mini Projects!\nIn the STA 9750 Mini-Projects, you will perform basic data analyses intended to model best practices for your course final project. (Note, however, that these are mini-projects; your final course project is expected to be far more extensive than any single MP.)\nFor purposes of MPs, we are dividing the basic data analytic workflow into several major stages:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\nIn this course, our primary focus is on the first four stages: you will take other courses that develop analytical and modeling techniques for a variety of data types. As we progress through the course, you will eventually be responsible for the first four steps. Specifically, you are responsible for the following stages of each mini-project:\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn early stages of the course, such as this MP, I will ‘scaffold’ much of the analysis for you, leaving only those stages we have discussed in class for you to fill in. As the course progresses, the mini-projects will be more self-directed and results less standardized.\nRubric\nSTA 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nLess than half of the instructor-provided tasks were successfully completed.\nReport exhibits particularly creative insights beyond instructor specifications.\n\n\nFormatting & Display\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation. Table has a suitable caption.\nColumn names and digits are well-chosen, but formatting could be improved.\nBad column names (opaque variable names or other undefined acronyms)\nUnfiltered ‘data dump’ instead of curated table.\nNo tables.\nReport includes one or more high-quality graphics (created using R).\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nAutomatic (10/10). Out of scope for this mini-project\n\n\n\n\nReport modifies instructor-provided import code to use additional columns or data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp01.qmd so the rendered document can be found at docs/mp01.html in the student’s repository and served at the URL:\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp01.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2025-SPRING/issues/new .\n\nTitle the issue STA 9750 &lt;GITHUB_USERNAME&gt; MiniProject #01 and fill in the following text for the issue:\nHi @michaelweylandt!\n\nI've uploaded my work for MiniProject #01 - check it out!\n\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2025-SPRING/mp01.html\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini01.html#mini-project-01-fiscal-characteristics-of-major-us-public-transit-systems",
    "href": "archive/AY-2024-FALL/miniprojects/mini01.html#mini-project-01-fiscal-characteristics-of-major-us-public-transit-systems",
    "title": "STA 9750 Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "text": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems\nFor MP#01, we are taking inspiration from the popular CityNerd YouTube channel. In particular, we’re taking this presentation on farebox recovery, i.e. the fraction of revenues raised from fares instead of taxes, as our starting point:\n\n\nWe will use data from the National Transit Database as our primary source. In particular, since we want to analyze farebox revenues, total number of trips, total number of vehicle miles traveled, and total revenues and expenses by source, we will need to analyze several different tables:\n\nThe 2022 Fare Revenue table\nThe latest Monthly Ridership tables\nThe 2022 Operating Expenses reports\n\nBecause this data is reported on a lag, we will use the 2022 version of all reports. Our data may have some post-pandemic irregularities, but that’s ok. We aren’t looking to make any long-term forecasts in this project.\nThe following code will download, clean, and join the tables. You don’t need to edit it in this project, but you may want to bookmark it as a useful example for later projects where you are responsible for downloading and cleaning the data.\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n    # directory.\n    download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                  destfile=\"2022_fare_revenue.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_expenses.csv\" in your project\n    # directory.\n    download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                  destfile=\"2022_expenses.csv\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\nFinally, let’s extract monthly transit numbers:\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"ridership.xlsx\" in your project\n    # directory.\n    download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                  destfile=\"ridership.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nThis creates a table as follows:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\n\n\n\n\nInstructor’s Note: You might want to explore the functions of the DT package to create more attractive tables. Even more advanced is the gt package.\nThis is useful, but not exactly what we want. Here, the UPT column refers to Unlinked Passenger Trips, which is a measure of rides (controlling for connections and transfers), and VRM refers to Vehicle Revenue Miles, roughly how far the transit provider travelled in total. Some of the other column names are less helpful, so let’s rename them using the rename function.\n\n\n\n\n\n\nTask 1 - Creating Syntatic Names\n\n\n\nRename a column: UZA Name to metro_area.\n\n\nBecause they have no spaces in them, these names will be easier to manipulate in code. Recall that non-syntactic names (names with spaces, punctuation, or strange characters) have to be quoted in backticks or quotes (depending on the context), while most tidyverse functions take syntactic names without quotes.\nThe Mode column is also helpful, but it uses a set of codes that aren’t interpretable. To make life easier for ourselves, let’s use a case_when statement to transform this into something we can make sense of.\n\n\n\n\n\n\nTask 2: Recoding the Mode column\n\n\n\nFirst, find the unique Mode codes in our data using the distinct function. Next, examine the NTD website and find the interpretations of these codes. Complete the following snippet to recode the Mode column.\n\n## This code needs to be modified\nUSAGE &lt;- USAGE |&gt;\n    mutate(Mode=case_when(\n        Mode == \"HR\" ~ \"Heavy Rail\", \n        ...\n        ...\n        TRUE ~ \"Unknown\"))\n\n\n\nNow that your data is clean, you may want to create an attractive summary table of your cleaned up USAGE table using the following snippet:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\nTo make your table cleaner, you might want to modify this code to unselect the NTD ID and 3 Mode columns and/or rename the UPT and VRM columns.\nNote: The use of sample_n here is just to make a sufficiently small sample to view it in a table. For your actual analysis, you should use the entire data set.\n\n\n\n\n\n\nTask 3: Answering Instructor Specified Questions with dplyr\n\n\n\nUsing functions we have studied in class, including filter, group_by, summarize, arrange, answer the following questions in your analysis:\n\nWhat transit agency had the most total VRM in our data set?\nWhat transit mode had the most total VRM in our data set?\nHow many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n\nWhat mode of transport had the longest average trip in May 2024?\nNote: This question can’t be answered with vehicle miles. To get average passenger trip length, we need passenger miles.\n\nHow much did NYC subway ridership fall between April 2019 and April 2020?\n\n\n\n\n\n\n\n\n\nTask 4: Explore and Analyze\n\n\n\nFind three more interesting transit facts in this data other than those above.\n\n\nWe are now ready to combine these usage statistics with the revenue and expenses data. Because our fare data is for 2022 total, we need to convert our usage table to 2022 summary info.\n\n\n\n\n\n\nTask 5: Table Summarization\n\n\n\nCreate a new table from USAGE that has annual total (sum) UPT and VRM for 2022. This will require use of the group_by, summarize, and filter functions. You will also want to use the year function, to extract a year from the month column.\nThe resulting table should have the following columns:\n\nNTD ID\nAgency\nmetro_area\nMode\nUPT\nVRM\n\nMake sure to ungroup your table after creating it.\nName this table USAGE_2022_ANNUAL.\n\n\nOnce you have created this new table, you can merge it to the FINANCIALS data as follows:\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n           FINANCIALS, \n           join_by(`NTD ID`, Mode)) |&gt;\n    drop_na()\n\nNote that the name fields differ between the ridership and financials tables, so it’s a good thing we had the unique identifier NTD ID to rely on.\nWe are now finally ready to our original question about farebox recovery.\n\n\n\n\n\n\nTask 6: Farebox Recovery Among Major Systems\n\n\n\nUsing the USAGE_AND_FINANCIALS table, answer the following questions:\n\nWhich transit system (agency and mode) had the most UPT in 2022?\nWhich transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nWhich transit system (agency and mode) has the lowest expenses per UPT?\nWhich transit system (agency and mode) has the highest total fares per UPT?\nWhich transit system (agency and mode) has the lowest expenses per VRM?\nWhich transit system (agency and mode) has the highest total fares per VRM?\n\nYou may wish to restrict your answer to major transit systems, which you can define as those with 400,000 UPT per annum.\n\n\nBased on all of this, what do you believe to be the most efficient transit system in the country? (Your answer may differ depending on which form of ‘efficiency’ you care most about)\n\nThis work ©2024 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini02.html",
    "href": "archive/AY-2024-FALL/miniprojects/mini02.html",
    "title": "STA 9750 Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "Warning: package 'glue' was built under R version 4.4.1"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini02.html#introduction",
    "href": "archive/AY-2024-FALL/miniprojects/mini02.html#introduction",
    "title": "STA 9750 Mini-Project #02: The Business of Show Business",
    "section": "Introduction",
    "text": "Introduction\nWelcome to Mini-Project #02. In this project, you will play the role of a Hollywood development executive; that is, you are the executive in charge of coming up with new movie ideas. Historically, development executives would source the “life rights” necessary to make “based on a true story” movies, would secure production options on promising new novels, and would partner with owners of established intellectual property (IP) to develop movie adaptations. Recently, however, the development process has been criticized by Hollywood insiders and audiences alike for over-reliance on rote sequels. Our goal is to develop a set of data-driven ideas for new movies. Before doing so, however, we will dive into Hollywood history to identify key characteristics of successful movies, to identify successful filmmakers and actors, and to examine some of Hollywood’s most famous flops.\nStudent Responsbilities\nRecall our basic analytic workflow and table of student responsibilities:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn this mini-project, you are mainly responsible for data alignment and basic statistical analyses. While not the main focus of this mini-project, you are also expected to provide basic data visualizations to support your findings; the grading rubric, below, emphasizes the dplyr tools used in this project, but reports without any visualization will be penalized severely. Note that data visualization will play a larger role in Mini-Project #03.\nAs before, I will provide code to automatically download and read in the data used for this project. Also note that, as compared with Mini-Project #01, I am providing significantly less ‘scaffolding’: students are more responsible for directing their own analyses.\nRubric\nSTA 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.\nCode completes all instructor-provided tasks satisfactorially.\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three or ore instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nReport exhibits particularly creative insights drawn from thorough student-initiated analyses.\n\n\nFormatting & Display\n\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation.\nFigures are ‘publication-quality’, with suitable axis labels, well-chosen structure, attractive color schemes, titles, subtitles, and captions, etc.\n\n\nTables are well-formatted, but still have room for improvement.\nFigures are above ‘exploratory-quality’, but do not reach full ‘publication-quality’.\n\n\nTables lack significant ‘polish’ and need improvement in substance (filtering and down-selecting of presented data) or style.\nFigures are suitable to support claims made, but are ‘exploratory-quality’, reflecting minimal effort to customize and ‘polish’ beyond ggplot2 defaults.\n\n\nUnfiltered ‘data dump’ instead of curated table.\nBaseline figures that do not fully support claims made.\n\nReport lacks basic tables; OR report lacks basic figures.\nReport includes one or more high-quality graphics (created using R) using tools beyond static basic ggplot2. These can be created using extensions toggplot2 or speciality packages for interactive graphics.\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nAutomatic (10/10). Out of scope for this mini-project\n\n\n\n\nReport modifies instructor-provided import code to use additional columns or data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp2.qmd so the rendered document can be found at docs/mp2.html in the student’s repository and served at the URL:\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp02.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2025-SPRING/issues/new .\n\nTitle the issue STA 9750 &lt;GITHUB_USERNAME&gt; MiniProject #02 and fill in the following text for the issue:\nHi @michaelweylandt!\n\nI've uploaded my work for MiniProject #02 - check it out!\n\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2025-SPRING/mp02.html\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini02.html#mini-project-02",
    "href": "archive/AY-2024-FALL/miniprojects/mini02.html#mini-project-02",
    "title": "STA 9750 Mini-Project #02: The Business of Show Business",
    "section": "Mini-Project #02",
    "text": "Mini-Project #02\nData\nFor this project, we will use data from the Internet Movie Database (IMDb). Specifically, we will use the tables from the IMDb non-commercial release. These files are made freely available by IMDb for non-commercial use.\nThe following code will automatically download and load these files into R:\n\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\n\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\n\n\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\n\n\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\n\n\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\n\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\nNote that these are large files and it will take some time for them to download the first time. Because these files are so large, it will also take a little while to read them. If you want to speed up this stage, you can cache the code chunk that reads the files. This will ‘save’ the result of the chunk and only require it to be re-executed when it is changed.\nData Sub-Sampling\nThis data is large enough that we’re going to need to immediately start down-selecting to get to a data set that we can analyze fluidly. For our NAME_BASICS table, we’ll restrict our attention to people with at least two “known for” credits.1\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\nIMDb has a long tail of obscure movies:\n\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\nTo keep our computers from working too hard, let’s throw out any title with less than 100 ratings. It’s not too hard to see that this drops about 75% of the entire data set:\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     101 2942823 \n\n\nApplying this drop, we significantly reduce the size of our data set:\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\nWe want to perform the same filtering on our other TITLE_* tables. This is a rare use for the semi_join. Recall that a semi_join returns only values which have a match,but doesn’t actually add columns.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\nAt this point, we’ve filtered down our data significantly and are ready to begin analysis in earnest. Note that our sub-sampling may induce some ‘dangling’ references: some of the people dropped from the NAME_BASICS table may only appear in one famous movie, and we’ve likely lost their info.\n\n\n\n\n\n\nProcessing Large Data\n\n\n\nEven with this processing, this a non-trivial amount of data, requiring approximately 2 GB of memory. If your computer is significantly struggling to perform this pre-processing, the instructor may be able to provide smaller data files upon request. (Even on my quite modern laptop, the initial processing phase takes a few minutes: by ‘significant struggling’, I’m referring to processing taking upwards of half an hour or exhausting all available memory.) Please contact the instructor and TA through the course discussion board to discuss this possibility.\nProcessing large data sets is a skill, however, so we’re starting with the large data set to help you practice it.\n\n\n\n\n\n\n\n\nPre-Processed Data\n\n\n\n\n\nExports of the pre-processed data can be found on the course GitHub repo. If your computer is struggling to handle the full data set, you may choose to use these instead. The readr::read_csv files handles zip compression transparently, but you will need to modify get_imdb_file above to:\n\nPoint to my GitHub instead of the IMDB archive\nUse .csv.zip files instead of .tsv.gz\n\n\nNote also that, to get the compressed files small enough to store on GitHub, I had to apply more filtering than the code above uses. Make sure to note if you are using this extra-filtered extract so that a reader knows why you might be getting different answers.\n\n\n\nInitial Exploration\nAt this point, let’s start examining our data more closely. Use the glimpse function to examine each table, taking care to note the type or mode of each column. For this data set, most columns appear to be read in as character (string) vectors, even when they should be numeric. This can occur when “null” values are represented in some non-standard way. For instance, in these files, we see that missing values are represented as \\\\N. R does not know that these are NA values and so retains them as strings.2\nTo fix this, we need to use:\n\nthe mutate command, since we’re changing the type of a column\nthe as.numeric command to change the type of the column.\n\nWe can clean the NAMES_BASIC command as follows:\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n    mutate(birthYear = as.numeric(birthYear),\n           deathYear = as.numeric(deathYear))\n\n\n\n\n\n\n\nTask 1: Column Type Correction\n\n\n\nCorrect the column types of the TITLE tables using a combination of mutate and the coercion functions as.numeric and as.logical.\n\n\nAnother non-tidy aspect of this data is that it combines multiple pieces of information in a single cell separated by commas. We already saw a bit of this in the NAME_BASICS table, where both the primaryProfession and knownForTitles columns combine multiple values.\n\nglimpse(NAME_BASICS)\n\nRows: 3,175,526\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;dbl&gt; 1899, 1924, 1934, 1949, 1918, 1915, 1899, 1924, 1925…\n$ deathYear         &lt;dbl&gt; 1987, 2014, NA, 1982, 2007, 1982, 1957, 2004, 1984, …\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nWe can use the separate_longer_delim function to break these into multiple rows: for example\n\nNAME_BASICS |&gt; separate_longer_delim(knownForTitles, \",\") |&gt; slice_head(n=10)\n\n      nconst     primaryName birthYear deathYear\n1  nm0000001    Fred Astaire      1899      1987\n2  nm0000001    Fred Astaire      1899      1987\n3  nm0000001    Fred Astaire      1899      1987\n4  nm0000001    Fred Astaire      1899      1987\n5  nm0000002   Lauren Bacall      1924      2014\n6  nm0000002   Lauren Bacall      1924      2014\n7  nm0000002   Lauren Bacall      1924      2014\n8  nm0000002   Lauren Bacall      1924      2014\n9  nm0000003 Brigitte Bardot      1934        NA\n10 nm0000003 Brigitte Bardot      1934        NA\n                    primaryProfession knownForTitles\n1        actor,miscellaneous,producer      tt0072308\n2        actor,miscellaneous,producer      tt0050419\n3        actor,miscellaneous,producer      tt0053137\n4        actor,miscellaneous,producer      tt0027125\n5  actress,soundtrack,archive_footage      tt0037382\n6  actress,soundtrack,archive_footage      tt0075213\n7  actress,soundtrack,archive_footage      tt0117057\n8  actress,soundtrack,archive_footage      tt0038355\n9   actress,music_department,producer      tt0057345\n10  actress,music_department,producer      tt0049189\n\n\nTo preserve flexibility, let’s not fully separate NAME_BASICS just yet, but you will need to use separate_longer_delim to answer various questions.\nUsing your knowledge of dplyr functionality, answer the following questions\n\n\n\n\n\n\nTask 2: Instructor-Provided Questions\n\n\n\n\nHow many movies are in our data set? How many TV series? How many TV episodes?\nWho is the oldest living person in our data set?\nThere is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\nWhat four projects is the actor Mark Hamill most known for?\nWhat TV series, with more than 12 episodes, has the highest average rating?\n\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\nIs it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\n\nHint: It may be useful to create a “map” of which columns map to which tables before attempting these questions. While these can be quite formal, even some basic sketches on a scratch piece of paper are often quite clarifying.\n\n\nQuantifying Success\nOur goal is to proposal successful new movies. To do so, we need a way of measuring the success of a movie given only IMDb ratings.3 While there’s no “magic number” for success, it is logical to assume that a successful project will have both a high average IMDb rating, indicating quality, and a large number of ratings, indicating broad awareness in the public.\n\n\n\n\n\n\nTask 3: Custom Success Metric\n\n\n\nDesign a ‘success’ measure for IMDb entries, reflecting both quality and broad popular awareness. Implement your success metric using a mutate operator to add a new column to the TITLE_RATINGS table.\nValidate your success metric as follows:\n\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\nChoose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nPerform at least one other form of ‘spot check’ validation.\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value \\(v\\) such that movies above \\(v\\) are all “solid” or better.\n\n\n\nYou will use your success metric and threshold to complete the rest of this Mini-Project. You may, if you wish, restrict your attention to movies for the remainder of your analysis, though a good development executive should also consider making TV series.\nExamining Success by Genre and Decade\nNow that you have a working proxy for success, it’s time to look at trends in success over time. Answer the following questions. Your responses should include at least 2 graphics.\n\n\n\n\n\n\nTask 4: Trends in Success Over Time\n\n\n\nUsing questions like the following, identify a good “genre” for your next film. You do not need to answer these questions precisely, but these are may help guide your thinking.\n\nWhat was the genre with the most “successes” in each decade?\nWhat genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nWhat genre has become more popular in recent years?\n\n\n\nBased on your findings, select a genre for your next project. Note that you may wish to avoid an “oversatured” genre; you just need to make the argument that your proposal is a good investment, not necessarily the most studio-produced focus-grouped committee-designed generic satisfying choice, so feel free to lean in to your own artistic preferences, as long as you can make an argument for them.\nSuccessful Personnel in the Genre\nNow that you have selected a target genre, identify two actors and one director who will anchor your project. You want to identify key personnel who have worked in the genre before, with at least modest success, and who have at least one major success to their credit.\nAs you develop your team, you may want to consider the following possibilities:\n\nAn older established actor and an up-and-coming actor\nAn actor/director pair who have been successful together\nAn actor/director pair who are both highly successful but have never worked together\nA pair of established actors who have had success in many genres\n\nAs you select your key personnel, consider what IMDb says they are known for; this will be useful in developing your marketing materials.\n\n\n\n\n\n\nTask 5: Key Personnel\n\n\n\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\n\n\nNostalgia and Remakes\nNow that you have found a target genre and key talent for your project, you need a story. Like any good development executive, your first instinct should be to produce a remake of a classic film in the genre.\n\n\n\n\n\n\nTask 6: Finding a Classic Movie to Remake\n\n\n\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.4\nOnce you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini02.html#putting-it-together",
    "href": "archive/AY-2024-FALL/miniprojects/mini02.html#putting-it-together",
    "title": "STA 9750 Mini-Project #02: The Business of Show Business",
    "section": "Putting It Together",
    "text": "Putting It Together\n\n\n\n\n\n\nTask 7: Write and Deliver Your Pitch\n\n\n\nNow that you have completed your analysis, write an “elevator pitch” of approximately 200-250 words for your proposed Hollywood project. This is the pitch you will bring to the studio head (your boss); if the studio head likes your pitch, you will be given a small sum of money to start securing the story rights and locking down tentative deals with key talent.\nYour pitch needs to synthesize the analysis above into two to three quick and compelling points. (E.g., “The market for animated young adult horror musicals has grown 200% in the past decade” or “Over 90% of Director D’s movies are successes.”) You need to present the strongest argument for each element of your pitch, including genre, director, actors, and story.\nIf your boss approves the pitch, you will need to have a brief trailer ready for the next quarterly earnings call. The marketing department has asked that you prepare a classic 90’s style teaser for them. Adapt the following cliched formula for your pitch.\n\nFrom director D, the visionary mind between N1; and From actor A, beloved star of N2; and From actor A2, Hollywood icon of genre G, Comes the timeless tail N3 A story of TOPIC, TOPIC, and TOPIC Coming soon to a theater near you.\n\nIf you’re creatively-minded, you could have some fun here using Generative tools to draft a script or mock up a movie poster for your pitch."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini02.html#general-remarks",
    "href": "archive/AY-2024-FALL/miniprojects/mini02.html#general-remarks",
    "title": "STA 9750 Mini-Project #02: The Business of Show Business",
    "section": "General Remarks",
    "text": "General Remarks\nAs you approach this project, recall there are no right or wrong answers. You are exploring data looking for exciting and actionable findings. You have several key decisions to make and you can support them with data, but the decisions are ultimately yours. This project is an exercise both in the “nuts-and-bolts” of analyzing a large data set and in using data to inform and refine what is ultimately still a “gut feeling” qualitative business decision.\nAs you iterate on this project, you will see that seemingly small different choices can produce very different results. That’s ok! As data analysts, we are constantly faced with small and essentially arbitrary decisions. An important “meta-skill” is knowing which of these decisions radically change our findings and which are meaningless. (An arbitrary decision with no impact on the bottom line is harmless; an arbitrary decision that could entirely change the plan for the next ten years is a problem.) Our responsibility is to clearly communicate these choices to our partners and clients: then we can receive their feedback on which way they would like to proceed.\nWorking in tools like Quarto and R helps here: if we provide clean and reproducible code, it should be easy to modify to see how our final conclusions are changed. Graphics also play an essential role in this form of clear communication: a ‘point estimate’ like “Action A is the best” is far less interpretable than a chart showing the predicted outcomes of several different actions.\nAs you approach this project, focus on justifying and communicating the choices you make. Structure your argument to communicate both key findings and uncertainties around them. Think about how you can use both document structure (headings vs subsections) and graphics to communicate with both clarity and nuance.\nGood luck!\n\nThis work ©2024 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini02.html#footnotes",
    "href": "archive/AY-2024-FALL/miniprojects/mini02.html#footnotes",
    "title": "STA 9750 Mini-Project #02: The Business of Show Business",
    "section": "Footnotes",
    "text": "Footnotes\n\nIt’s not entirely transparent who IMDb decides what projects an actor or director is “known for”. Still, it’s a reasonable filter that leaves us with more than enough to work with for this project.↩︎\nRecall that strings can contain essentially any data type and so are a safe fall-back. For instance, a column containing 1 and a can be losslessly represented by the string vector c(\"1\", \"a\") but coercion to the numeric vector c(1, NA) is lossy. R tries very hard not to destroy any information and so it doesn’t perform this conversion for us unless we explicitly request it.↩︎\nSadly, I couldn’t find permissively licensed movie box office data. If you are aware of some, please let me know!↩︎\nIn order to see that a movie has not been recently remade, it is sufficient to confirm that no movie has been made with the same name in the past 25 years.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects.html",
    "href": "archive/AY-2024-FALL/miniprojects.html",
    "title": "STA 9750 - Mini Projects",
    "section": "",
    "text": "In lieu of traditional homework, STA 9750 has a series of mini-projects designed to achieve several interlocking goals:\n\nImprove your skills at data analysis\nImprove your improve your ability to give feedback on data analysis work\nSeed a ‘portfolio’ of data science work you can demonstrate to potential employers\n\nEach Mini-Project will be submitted via GitHub, an industry-standard code management platform, as both raw analysis code and as a HTML document hosted on GitHub pages.\nAfter each Mini-Project is submitted, 2-3 peer reviewers will be assigned to give feedback and to assign an initial grade following an instructor provided rubric. This feedback will be given via GitHub Issues.\nIn order to ensure good peer feedback, the peer feedback will be evaluated by the instructor in a “meta-review” worth a small fraction of the overall grade.\nIf you believe your mini-project has received inaccurate peer feedback, please contact the instructor directly within 48 hours of the peer feedback deadline. No student-initiated requests for re-grading will be accepted after that time, though the instructor may re-grade the work during the meta-review stage.\n\nMini-Projects\n\nMini-Project #00: Course Set-Up\nDue Dates:\n\nReleased to Students: 2024-08-29\nInitial Submission: 2024-09-11 11:45pm ET\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-09-12\nPeer Feedback Due: 2024-09-18 11:45pm ET\n\n\nIn the ungraded Mini-Project #00, there is no data analysis required, but you will set up the basic web tooling used to submit projects #01 to #04.\nNote that, even though ungraded, Mini-Project #00 must be completed to remain enrolled in this course and before any other Mini-Projects can be submitted.\n\n\nMini-Project #01: Fiscal Characteristics of Major US Public Transit Systems\nDue Dates:\n\nReleased to Students: 2024-09-12\nInitial Submission: 2024-09-25 11:45pm ET\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-09-26\nPeer Feedback Due: 2024-10-02 11:45pm ET\n\n\nIn Mini-Project #01, students will investigate the fiscal characteristics of US public transit authorities. In this project, I handle the data import and tidying; students are mainly responsible for “single table” dplyr operations (mutate, group_by, summarize, select, arrange, rename) to produce summary statistics.\n\n\nMini-Project #02: Business of Show Business\nDue Dates:\n\nReleased to Students: 2024-09-26\nInitial Submission: 2024-10-23 11:45pm ET\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-10-24\nPeer Feedback Due: 2024-10-30 11:45pm ET\n\n\nIn Mini-Project #02, students will act as Hollywood development executives, diving deep into Hollywood history to develop a pitch for a new movie. Students develop their skills in working with large (\\(\\approx 20\\) GB) data and in working with relational data structures (joins and their kin). This project uses the IMDb Non-Commerical Data Release.\n\n\nMini-Project #03: Exploring the Effect of State-Level Electoral College Vote Allocation on Presidential Elections\nDue Dates:\n\nReleased to Students: 2024-10-24\nInitial Submission: 2024-11-13 11:45pm ET\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-11-14\nPeer Feedback Due: 2024-11-20 11:45pm ET\n\n\nIn Mini-Project #03, students will analyze the effect of the US Electoral College on US Presidential elections, with an eye towards “fairness” and proportionality. They will compare different ways that states allocate their Electoral College Votes to determine whether this has any systematic effect on who is elected the President of these United States.\nStudents will gain experience visualizing spatial data through the use of “red/blue electoral maps”, working with official electoral records, and systematically accessing public data repositories, such as those of the US Census Bureau. This project uses data from the MIT Election Lab and from the Census Bureau’s TIGER repository.\n\n\nMini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans\nDue Dates:\n\nReleased to Students: 2024-11-14\nInitial Submission: 2024-12-04 11:45pm ET\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-12-05\nPeer Feedback Due: 2024-12-11 11:45pm ET\n\n\nIn Mini-Project #04, students will compare two different retirement plans, using simulation to assess the probabilities of various good and bad retirement outcomes. They will use historical financial and macro-economic data to perform a Monte Carlo analysis of two complex financial instruments. Based on the outputs of the Monte Carlo simulations, students will compare these two retirement plans on a variety of important metrics.\nStudents will gain experience accessing data from real commercial and government APIs, implementing complex financial simulations, and applying Monte Carlo / bootstrap inference techniques to quantify uncertainty in their predictions. As an additional challenge, students may explore technologies used to create interactive dashboards, allowing for more dynamic and nuanced analysis of simulation results. While the focus of this project is a comparison of two specific CUNY retirement plans, the skills developed here can be applied to a wide variety of personal and commerical financial decisions."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab08.html",
    "href": "archive/AY-2024-FALL/labs/lab08.html",
    "title": "STA 9750 Week 8 In-Class Activity: Advanced ggplot2",
    "section": "",
    "text": "Week 8 Slides"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab08.html#spatial-visualization-with-ggplot2",
    "href": "archive/AY-2024-FALL/labs/lab08.html#spatial-visualization-with-ggplot2",
    "title": "STA 9750 Week 8 In-Class Activity: Advanced ggplot2",
    "section": "Spatial Visualization with ggplot2",
    "text": "Spatial Visualization with ggplot2\nToday, we’re going to dive deeper into ggplot2, with a focus on visualizing spatial data. Spatial data can be quite complex, but we can get pretty far with the simple features (sf) paradigm. Also, if we focus on relatively small regions, we can avoid the complexities that come from Earth not being planar.\n\nKey to this approach is a shapefile. A shapefile, conventionally stored with a shp extension, gives precise coordinates outlining spatial regions. By plotting a polygon with those points at its boundaries, we can visualize spatial regions.\nThe sf package includes a shp file for the counties of North Carolina:\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.4.1\n\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nnc &lt;- read_sf(system.file(\"shape/nc.shp\", package=\"sf\"))\n\nnc\n\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n# A tibble: 100 × 15\n    AREA PERIMETER CNTY_ CNTY_ID NAME  FIPS  FIPSNO CRESS_ID BIR74 SID74 NWBIR74\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 0.114      1.44  1825    1825 Ashe  37009  37009        5  1091     1      10\n 2 0.061      1.23  1827    1827 Alle… 37005  37005        3   487     0      10\n 3 0.143      1.63  1828    1828 Surry 37171  37171       86  3188     5     208\n 4 0.07       2.97  1831    1831 Curr… 37053  37053       27   508     1     123\n 5 0.153      2.21  1832    1832 Nort… 37131  37131       66  1421     9    1066\n 6 0.097      1.67  1833    1833 Hert… 37091  37091       46  1452     7     954\n 7 0.062      1.55  1834    1834 Camd… 37029  37029       15   286     0     115\n 8 0.091      1.28  1835    1835 Gates 37073  37073       37   420     0     254\n 9 0.118      1.42  1836    1836 Warr… 37185  37185       93   968     4     748\n10 0.124      1.43  1837    1837 Stok… 37169  37169       85  1612     1     160\n# ℹ 90 more rows\n# ℹ 4 more variables: BIR79 &lt;dbl&gt;, SID79 &lt;dbl&gt;, NWBIR79 &lt;dbl&gt;,\n#   geometry &lt;MULTIPOLYGON [°]&gt;\n\n\nWe represent this data in “tidy” format, with each row being a county. The “magic” is in the geometry column:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nnc |&gt; select(NAME, AREA, PERIMETER, geometry)\n\nSimple feature collection with 100 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n# A tibble: 100 × 4\n   NAME         AREA PERIMETER                                          geometry\n   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;                                &lt;MULTIPOLYGON [°]&gt;\n 1 Ashe        0.114      1.44 (((-81.47276 36.23436, -81.54084 36.27251, -81.5…\n 2 Alleghany   0.061      1.23 (((-81.23989 36.36536, -81.24069 36.37942, -81.2…\n 3 Surry       0.143      1.63 (((-80.45634 36.24256, -80.47639 36.25473, -80.5…\n 4 Currituck   0.07       2.97 (((-76.00897 36.3196, -76.01735 36.33773, -76.03…\n 5 Northampton 0.153      2.21 (((-77.21767 36.24098, -77.23461 36.2146, -77.29…\n 6 Hertford    0.097      1.67 (((-76.74506 36.23392, -76.98069 36.23024, -76.9…\n 7 Camden      0.062      1.55 (((-76.00897 36.3196, -75.95718 36.19377, -75.98…\n 8 Gates       0.091      1.28 (((-76.56251 36.34057, -76.60424 36.31498, -76.6…\n 9 Warren      0.118      1.42 (((-78.30876 36.26004, -78.28293 36.29188, -78.3…\n10 Stokes      0.124      1.43 (((-80.02567 36.25023, -80.45301 36.25709, -80.4…\n# ℹ 90 more rows\n\n\nThe geometry column is of type MULTIPOLYGON, essentially a list of GPS coordinates.\nIn this course, we’ll mainly just use these for plotting, though it is possible to do quite sophisticated analyses here.\nggplot2’s geom_sf makes it easy to visualize spatial data. It has one required aesthetic, fittingly named geometry:\n\nlibrary(ggplot2)\nggplot(nc, aes(geometry = geometry)) + geom_sf()\n\n\n\n\n\n\n\n\nNote that ggplot2 is doing some work behind the scenes to keep the aspect ratio reasonable and not “stretching” North Carolina north-south.\nThis is nice enough, if we like maps for maps’ sake, but we can do more. The BIR74 is the number of children born in that county between July 1, 1974 and June 30, 1978. We can use that to set the fills for each county.\n\nlibrary(ggplot2)\nggplot(nc, aes(geometry = geometry, \n               fill = BIR74)) + geom_sf()\n\n\n\n\n\n\n\n\nThis type of plot is called a chloropleth plot, coming from the Greek for “area-multitude”. It is commonly used to visualize areal geospatial data - that is quantities associated with spatial regions - as opposed to point data.\nBuilding a chloropleth plot is not too hard, if you have an appropriate shape file. Thankfully, governments are in the business of knowing exactly what the boundaries of their territories are and providing shape files. Typically, these shape files are distributed in a “zip” format, with other geodata files that we won’t use in this course.1\n\n\n\n\n\n\nAdding Points on Chloropleths\n\n\n\n\n\nWe can add the City of Charlotte, the largest city of North Carolina on this map:\n\ncharlotte &lt;- data.frame(\n    x = -80 - 50/60 - 35/60^2, # West = negative\n    y = +35 + 13/60 + 38/60^2  # North = positive\n)\n\nggplot() + \n    geom_sf(data=nc, \n            aes(geometry = geometry, \n                fill = BIR74)) + \n    geom_point(data=charlotte, \n               aes(x=x, y=y),\n               color=\"red4\")\n\n\n\n\n\n\n\n\nNote that I pass different data sets and aes mappings to each layer here, so I do them inside the individual geom_s instead of in the global ggplot() call."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab08.html#exercises-1---geospatial-visualizations",
    "href": "archive/AY-2024-FALL/labs/lab08.html#exercises-1---geospatial-visualizations",
    "title": "STA 9750 Week 8 In-Class Activity: Advanced ggplot2",
    "section": "Exercises 1 - Geospatial Visualizations",
    "text": "Exercises 1 - Geospatial Visualizations\n\nDownload the New York City Council Districts (Clipped to Shoreline) shapefiles. Unzip the download and read the shape file, nycc.shp, using the sf::read_sf function.\nUse ggplot2::geom_sf to create a basic map of NYC’s city council districts. This should just be a plot of the outlines of each district (recognizable as NYC, but otherwise showing no data).\nUsing the nyc_demos.csv data file, create a chloropleth map of NYC, where the color variable represents the 2010 under 5-year-old population of each district.\nNote that you will need to use some dplyr *_join functions to combine the shape data with the demographic data before passing it to ggplot2.2 Which parts of NYC have the fewest young children? Does this seem right?\nUnder the “one person one vote” principle, the number of adult residents in each council district should be roughly equal. Create a chloropleth plot showing how many more/fewer adult residents each district has than the average district. Are any districts (significantly) over/under-represented on the NY city council?\nTo do this, you will need to first 1) determine the number of voting age residents in each district; 2) compute the average number of voting age residents per district; 3) see how each district compares to the average (above/below and by how much).\nCreate a facet plot where each facet is a chloropleth indicating the percentage of residents in each district identifying as a member of each census-designated racial categories.3 Note that the nyc_demos.csv file contains total counts by race, so you will need to normalize by overall population to get percentages.\nTo get the data into suitable form for plotting, you will need to pivot the data into a longer format using the pivot_longer function from the tidyr package.\nCreate a visualization to compare the ratio of rental vs owner-owned housing per district with the age demographics of this district. What do you find? Is this what you would have expected?"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab08.html#limitations-and-extensions-of-chloropleths",
    "href": "archive/AY-2024-FALL/labs/lab08.html#limitations-and-extensions-of-chloropleths",
    "title": "STA 9750 Week 8 In-Class Activity: Advanced ggplot2",
    "section": "Limitations and Extensions of Chloropleths",
    "text": "Limitations and Extensions of Chloropleths\nChloropleth plots are rightly popular but they have a major limitation: humans are not uniformly distributed across space! Smaller regions may have higher populations (cf Connecticut vs Montana) and administrative regions are not the same size. Because human perception is naturally drawn to larger regions, we often need to adjust the color schemes used to compensate for area effects. This process can be powerful, but it is a bit difficult to get right.\nFor instance, let’s see what happens if we create a chloropleth of the SID74 data, the number of children who die of SIDS during our sample period in North Carolina:\n\nggplot(nc, aes(geometry = geometry, fill = SID74)) + \n    geom_sf()\n\n\n\n\n\n\n\n\nHere, Mecklenberg County (middle, bottom) sticks out because it has a large population, not because SIDS was particularly more common in that county. We can modify our plot to show the SIDS rate, rather than raw counts:\n\nnc |&gt;\n    mutate(sids_rate_74 = SID74 / BIR74) |&gt;\n    ggplot(aes(geometry = geometry, \n               fill = sids_rate_74)) + \n    geom_sf()\n\n\n\n\n\n\n\n\nWe see less variation here, but there is still some. Note that this type of data – rate estimation from rare counts – can be somewhat tricky to analyze, but that’s not the primary focus of this class so we’ll leave it here.\nThis plot still isn’t perfect however: Sampson county is the largest county in NC, but it has a relatively small population. That means that the largest area - and hence the place our eyes will most immediately look - is assigned to a relatively unimportant county. Relatedly, Anson County is a small population county, but it appears to be a “hot-spot”. This could be true, but it is more likely a noise-effect resulting from a small population.\nTo address these issues, we can use a cartogram which will “adjust” the map so that area maps to a relevant quantity. This is quite complex mathematically, but the cartogram package handles it reasonably transparently for us:\n\nlibrary(cartogram)\nnc |&gt; \n    mutate(sids_rate_74 = SID74 / BIR74) |&gt;\n    st_transform(26916) |&gt; \n    cartogram_cont(weight=\"BIR74\") |&gt; \n    ggplot(aes(geometry=geometry, \n               fill = sids_rate_74)) + \n    geom_sf()\n\n\n\n\n\n\n\n\nHere the st_transform function species what projection to equilibrate area in. I’m not an expert on this, but the internet seems to suggest 26916 is not a bad default.\nLooking at this plot, we see that counties are now adjusted to population, or really birth count and Anson county - while still high - is clearly down-weighted in accordance with its population."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab08.html#exercises-2---cartograms-of-nyc",
    "href": "archive/AY-2024-FALL/labs/lab08.html#exercises-2---cartograms-of-nyc",
    "title": "STA 9750 Week 8 In-Class Activity: Advanced ggplot2",
    "section": "Exercises 2 - Cartograms of NYC",
    "text": "Exercises 2 - Cartograms of NYC\n\nModify your plot of NYC to adjust city council districts by area.\nNext, modify your analysis to use the Dorling cartogram (cartogram_dorling) which gives a more regular cartogram representation."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab08.html#footnotes",
    "href": "archive/AY-2024-FALL/labs/lab08.html#footnotes",
    "title": "STA 9750 Week 8 In-Class Activity: Advanced ggplot2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn MP#03, you will learn how to extract the shp file from a zip archive automatically, but for now you can do so “by hand” by opening the zip file as if it were a regular file.↩︎\nIf you get an error reading “stat_sf() requires the following missing aesthetics: geometry”, make sure you include geometry=geometry in your aes (5 points)↩︎\nThese are c(\"White Hispanic\", \"Black Hispanic\", \"Asian and Pacific Islander Nonhispanic\", \"Other Nonhispanic\", \"Two or More Races Nonhispanic\", \"Hispanic Origin\"). Note that the census has changed these over time and the categories for the 2020 and 2030 censuses will be different.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab07.html",
    "href": "archive/AY-2024-FALL/labs/lab07.html",
    "title": "STA 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "",
    "text": "Update Slides: Slides 07\nThis week, we’re going to break into project groups and do three ggplot2 exercises of increasing difficulty. As you work through these with your teammates, be sure to reflect on what plots and what tools you will need to best present your mini-project and course project findings."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab07.html#exercise-1-basic-ggplot2-15-minutes",
    "href": "archive/AY-2024-FALL/labs/lab07.html#exercise-1-basic-ggplot2-15-minutes",
    "title": "STA 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Exercise 1: Basic ggplot2 (15 minutes)",
    "text": "Exercise 1: Basic ggplot2 (15 minutes)\nIn this exercise, you will create ggplot2 graphics to analyze the diamonds data from the ggplot2 package. This data contains pricing and measurements for 50,000 diamonds sold in the US. (Note that these prices are rather out of date.) Before beginning this exercise, you might want to read about the “4 C’s of Diamonds” commonly used to measure quality.\n\nMake a scatter plot of price vs carat and facet it by cut.\nUse geom_smooth to see how the price-carat relationship changes by color.\nCreate a frequency polygon plot of price, broken out by different diamond cuts.\nCreate a scatter plot of color by clarity. Why is this plot not useful?\n\nStretch Goal: Make a better plot to visualize this relationship using the ggmosaic package."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab07.html#exercise-2-trend-analysis-with-ggplot2-30-minutes",
    "href": "archive/AY-2024-FALL/labs/lab07.html#exercise-2-trend-analysis-with-ggplot2-30-minutes",
    "title": "STA 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Exercise 2: Trend Analysis with ggplot2 (30 minutes)",
    "text": "Exercise 2: Trend Analysis with ggplot2 (30 minutes)\nThe Carbon Dioxide Information and Analysis Center studies the effect of carbon dioxide on global and local temperature trends. A key tool in their analysis is the temperature “anomaly”. An anomaly is the difference between observed temperature (in a world with anthropogenic atmospheric CO2) and ‘natural’ temperature (from a world without anthropogenic gases). Note that these anomalies require significant analysis to compute and are not “simple observational” data.\nPoliticians have adopted the tools of temperature anomaly to set national and international emissions targets, e.g., the 2 Degree Target. Note that 2 degrees is calculated as a global average: in practice, some regions will experience a much larger change in temperature, while others may experience a smaller change or even a negative change.\nThe CVXR package includes the cdiac dataset, capturing CDIAC’s estimated global temperature anomalies from 1850 to 2015. In this question, you will explore these estimated anomalies. Note that you may need to install the CVXR package before beginning this question.1\n\ninstall.packages(\"CVXR\")\n\n\nlibrary(CVXR)\nlibrary(tidyverse)\ndata(cdiac)\nglimpse(cdiac)\n\nRows: 166\nColumns: 14\n$ year   &lt;int&gt; 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 186…\n$ jan    &lt;dbl&gt; -0.702, -0.303, -0.308, -0.177, -0.360, -0.176, -0.119, -0.512,…\n$ feb    &lt;dbl&gt; -0.284, -0.362, -0.477, -0.330, -0.280, -0.400, -0.373, -0.344,…\n$ mar    &lt;dbl&gt; -0.732, -0.485, -0.505, -0.318, -0.284, -0.303, -0.513, -0.434,…\n$ apr    &lt;dbl&gt; -0.570, -0.445, -0.559, -0.352, -0.349, -0.217, -0.371, -0.646,…\n$ may    &lt;dbl&gt; -0.325, -0.302, -0.209, -0.268, -0.230, -0.336, -0.119, -0.567,…\n$ jun    &lt;dbl&gt; -0.213, -0.189, -0.038, -0.179, -0.215, -0.160, -0.288, -0.310,…\n$ jul    &lt;dbl&gt; -0.128, -0.215, -0.016, -0.059, -0.228, -0.268, -0.297, -0.544,…\n$ aug    &lt;dbl&gt; -0.233, -0.153, -0.195, -0.148, -0.163, -0.159, -0.305, -0.327,…\n$ sep    &lt;dbl&gt; -0.444, -0.108, -0.125, -0.409, -0.115, -0.339, -0.459, -0.393,…\n$ oct    &lt;dbl&gt; -0.452, -0.063, -0.216, -0.359, -0.188, -0.211, -0.384, -0.467,…\n$ nov    &lt;dbl&gt; -0.190, -0.030, -0.187, -0.256, -0.369, -0.212, -0.608, -0.665,…\n$ dec    &lt;dbl&gt; -0.268, -0.067, 0.083, -0.444, -0.232, -0.510, -0.440, -0.356, …\n$ annual &lt;dbl&gt; -0.375, -0.223, -0.224, -0.271, -0.246, -0.271, -0.352, -0.460,…\n\n\n\nPlot the estimated annual global mean temperature (GMT) anomaly from 1850 to 2015.\n\n\nUse scale_x_date to improve the \\(x\\)-axis\n\n\nPlot the GMT anomaly for each month on the same plot (as different lines).\n\n\nBefore starting this, you may need to use the pivot_ functionality to get this data in the right shape. Recall that ggplot2 expects “data point” per row.\n\n\nPlot the monthly GMT anomaly series as one long line (with a point for each month).\nNow focus only on July: plot the July GMT anomaly series. Use the runmed()\nfunction to add a second series to the plot giving the median July GMT anomaly of the previous 5 years. Is there evidence of an increasing warming trend?\nFor each year, identify the warmest month (as measured by GMT anomaly); create a histogram showing the probability a given month was the hottest (largest anomaly) in its year.\n\n\n\nMake sure your \\(x\\)-axis is in reasonable (chronological) order - not alphabetical.\nYou will need to use dplyr tools to find the warmest month in a given year."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab07.html#exercise-3-animated-graphics-1-hour",
    "href": "archive/AY-2024-FALL/labs/lab07.html#exercise-3-animated-graphics-1-hour",
    "title": "STA 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Exercise 3: Animated Graphics (1 hour)",
    "text": "Exercise 3: Animated Graphics (1 hour)\nIn this question, you will use the gganimate extension to ggplot2 to create animated graphics. We will use the famous gapminder data set from the gapminder package. Install the gganimate, gapminder, gifski, and av packages before attempting attempting this problem.\n\nFor background, watch Hans Rosling’s talk on human prosperity.\nCreate a scatter plot of the relationship between GDP and Life Expectancy in the year 1952.\n\n\nColor points by continent and use the size aesthetic to represent population.\nYou might want to put quantities on a log-scale.\n\n\nThere is an outlier country in this data with very high GDP.\n\n\nWhat is it?\nIdentify and remove it.\n\n\nUsing the transition_time function, make this an animated plot showing how this data changes over time.\nUsing the theme machinery, labels, etc. make this a “publication ready” plot.\n\n\nNote that you can use {frame_time} in the title to get a dynamically changing year.\n\n\nUse the country_colors data from the gapminder plot to color the points using Dr. Rosling’s perferred color scheme.\n\n\nThis is a different color scale than ggplot2 uses by default, so you will need to override the scale_color_* functionality.\nThe help page for ?country_colors will be helpful here."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab07.html#footnotes",
    "href": "archive/AY-2024-FALL/labs/lab07.html#footnotes",
    "title": "STA 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCVXR is actually an incredible piece of software and super-useful for developing and implementing statistical and machine learning techniques. We, sadly, will not explore it in this course.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html",
    "href": "archive/AY-2024-FALL/labs/lab05.html",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html#join-specifications",
    "href": "archive/AY-2024-FALL/labs/lab05.html#join-specifications",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Join Specifications",
    "text": "Join Specifications\ndplyr specifies joins using the join_by function. The output of the join_by function, also known as a “join specification” is a series of logical tests applied to pairs of rows. The results of these logical tests are used to identify “matches” between rows. Joins differ primarily on how they use the outputs of these logical tests to construct their output.\nThe simplest and most useful logical test to use in a join is an equality test. In dplyr, these are simply written as\n\njoin_by(left_name == right_name)\n\nThis type of test checks whether the value in the left_name column of the first (left) argument matches the value in the right_name column of the second (right) argument.\nFor example, if I wanted to join the origin column of flights table to the faa column of the airports table, I might use something like the following:\n\ninner_join(flights, airports, join_by(origin == faa))\n\n# A tibble: 336,776 × 26\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 18 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt;, lat &lt;dbl&gt;,\n#   lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n\nHere origin is taken to be a column from the first (left) table and faa is taken to be a column from the second (right) table. As with other dplyr functions, there is a bit of programming magic used to allow column names to be used as variables and interpreted correctly.\nFor the airport identifiers, we only need to match on the single unique ID. (We can assume the FAA assigns unique IDs to each airport.) In other circumstances, we need to combine several logical tests to get a true match.\nFor example, suppose we want to align our flights with the weather at their origin airport at scheduled take off time. Here, we’d need to combine the flights and weather table on many columns:\n\norigin to origin\nyear to year\nmonth to month\nday to day\nhour to hour\n\nIn this case, we’d pass 5 equality conditions to join_by:\n\ninner_join(flights, \n           weather, \n           join_by(origin == origin,\n                   year == year,\n                   month == month,\n                   day == day,\n                   hour == hour))\n\n# A tibble: 335,220 × 29\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 335,210 more rows\n# ℹ 21 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour.x &lt;dttm&gt;, temp &lt;dbl&gt;, dewp &lt;dbl&gt;,\n#   humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;,\n#   precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour.y &lt;dttm&gt;\n\n\nHere we look only at those rows which match on all 5 tests. In this way, join_by behaves like filter: it “passes” the intersection of positive results.\nNote that it is relatively common for matched columns to have the same name in both tables: to support this case, dplyr reads a single column name as “self-equality”. So the above code can be more concisely written as:\n\ninner_join(flights, \n           weather, \n           join_by(origin, \n                   year, \n                   month, \n                   day, \n                   hour))\n\n# A tibble: 335,220 × 29\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 335,210 more rows\n# ℹ 21 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour.x &lt;dttm&gt;, temp &lt;dbl&gt;, dewp &lt;dbl&gt;,\n#   humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;,\n#   precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour.y &lt;dttm&gt;\n\n\nI recommend against using this short-cut. It takes hardly more time to write your intent explicitly and it’s far more robust. Measure twice, cut once.\nUnfortunately, it is not easy to perform an “OR” in join_by. We may cover this below, time allowing.\nWe now turn to specific joins. All of these joins use the join_by operator but they construct results differently based on its output."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html#inner-joins",
    "href": "archive/AY-2024-FALL/labs/lab05.html#inner-joins",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Inner Joins",
    "text": "Inner Joins\nThe most common and most important join in data analysis is the inner_join. The inner join returns matches between two tables. Conceptually, the inner join constructs all possible pairs of rows between the two tables (so {r eval=FALSE} NROW(x) * NROW(y) total rows) and then filters down to those which pass the join_by test. In practice, more efficient algorithms are used to prevent wasteful computation.\nInner joins are used when seeking matches between two tables. They are particularly useful when both tables are “comprehensive” and we are sure that there are matches. For instance, we can use an inner_join to combine most of the tables in nycflights13 because they come from a comprehensive government data source. (E.g., No flights going to secret “unauthorized” airports.)\nLet’s start by asking what the average arrival delay of flights going to west coast airports is. We do not have enough information to answer this using the flights table alone. To identify west coast airports, let’s filter airports on tzone:\n\nwest_coast_airports &lt;- airports |&gt; filter(tzone == \"America/Los_Angeles\")\n\nWe can now join this to the original flights table to find only those flights with destination matches in west_coast_airports:\n\ninner_join(flights, west_coast_airports, join_by(dest == faa))\n\n# A tibble: 46,324 × 26\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      558            600        -2      924            917\n 2  2013     1     1      558            600        -2      923            937\n 3  2013     1     1      559            600        -1      854            902\n 4  2013     1     1      611            600        11      945            931\n 5  2013     1     1      628            630        -2     1016            947\n 6  2013     1     1      646            645         1     1023           1030\n 7  2013     1     1      651            655        -4      936            942\n 8  2013     1     1      655            700        -5     1037           1045\n 9  2013     1     1      658            700        -2     1027           1025\n10  2013     1     1      702            700         2     1058           1014\n# ℹ 46,314 more rows\n# ℹ 18 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt;, lat &lt;dbl&gt;,\n#   lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n\nHere, we have only a subset of our original flights table. From this, we can compute our relevant summary statistic:\n\ninner_join(flights, west_coast_airports, join_by(dest == faa)) |&gt;\n    summarize(mean(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(arr_delay, na.rm = TRUE)`\n                            &lt;dbl&gt;\n1                            1.28\n\n\nIs this any better than the following alternative approach:\n\ninner_join(flights, airports, join_by(dest == faa)) |&gt;\n    filter(tzone == \"America/Los_Angeles\") |&gt;\n    drop_na() |&gt;\n    summarize(mean(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(arr_delay, na.rm = TRUE)`\n                            &lt;dbl&gt;\n1                            1.28\n\n\nFormally, these are basically equivalent. (filter and inner_join commute). As usual, it’s a matter of communicating intent. Here the single line filter(tzone == \"America/Los_Angeles\") is simple enough it probably doesn’t need a separate variable. But if, instead of a one line operation, we performed a very complex set of filtering options, we may benefit from giving it a separate name as opposed to trying to shoe-horn the complex filtering into a pipeline.\nPerformance-wise, it is a bit better to perform filter before inner_join (Why? Think about the size of the result of each step.) but the difference is rarely material. Clarity of intent, not optimizing performance, should dictate the order in which you perform steps.\nBoth approaches are also equivalent to:\n\ninner_join(flights, \n           airports |&gt; filter(tzone == \"America/Los_Angeles\"), \n           join_by(dest == faa)) |&gt;\n    drop_na() |&gt;\n    summarize(mean(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(arr_delay, na.rm = TRUE)`\n                            &lt;dbl&gt;\n1                            1.28\n\n\nBut I find this sort of “filter inside join argument” to be terribly difficult to read: it mixes standard (inside-out) and piped (left to right) evaluation orders in a confusing manner. Avoid this!\nWork with your group to answer the following questions using inner_join.\n\nWhat is the name of the airline with the longest average departure delay?\nWhat is the name of the origin airport with the longest average departure delay?\nWhat is the name of the destination airport with the longest average departure delay?\nAre average delays longer for East-coast destinations or West-coast destinations?\nWhich plane (tailnum) flew the most times leaving NYC? Who manufactured it?\nWhich manufacturer has the most planes flying out of NYC airports?\nWhich manufacturer has the longest average flight?\nWhat model of plane has the smallest average delay leaving NYC?"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html#left-join",
    "href": "archive/AY-2024-FALL/labs/lab05.html#left-join",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Left Join",
    "text": "Left Join\nLeft joins are useful when you don’t want to dropped unmatched columns in one table. For instance, suppose we misplace some rows from our airlines table:\n\nairlines_major &lt;- airlines |&gt;\n    filter(carrier %in% c(\"AA\", \"DL\", \"UA\", \"WN\", \"B6\", \"AS\"))\n\nIf we inner join on airlines_major, we loose many of the rows in flights.\n\nNROW(flights)\n\n[1] 336776\n\ninner_join(flights, \n           airlines_major, \n           join_by(carrier == carrier)) |&gt;\n    NROW()\n\n[1] 207128\n\n\nSometimes this is what we want, but not always. If we instead use a left join, we keep all of the rows in flights:\n\nNROW(flights)\n\n[1] 336776\n\nleft_join(flights, \n          airlines_major, \n          join_by(carrier == carrier)) |&gt;\n    NROW()\n\n[1] 336776\n\n\nRows lacking a pair in airlines_major fill the missing columns with NA. This fits our mental model of missing values in R: in theory, these flights should have some carrier name, but given the data at hand, we don’t know what it is.\n\nNROW(flights)\n\n[1] 336776\n\nleft_join(flights, \n          airlines_major, \n          join_by(carrier == carrier)) |&gt;\n    filter(carrier %in% c(\"MQ\", \"OO\", \"VX\")) |&gt;\n    glimpse() # Look at 'name' column\n\nRows: 31,591\nColumns: 20\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 600, 602, 608, 624, 656, 658, 729, 749, 800, 805, 811, …\n$ sched_dep_time &lt;int&gt; 600, 605, 600, 630, 705, 700, 730, 710, 810, 815, 630, …\n$ dep_delay      &lt;dbl&gt; 0, -3, 8, -6, -9, -2, -1, 39, -10, -10, 101, -4, -5, -8…\n$ arr_time       &lt;int&gt; 837, 821, 807, 840, 1007, 1027, 1049, 939, 949, 1006, 1…\n$ sched_arr_time &lt;int&gt; 825, 805, 735, 830, 940, 1025, 1115, 850, 955, 1010, 83…\n$ arr_delay      &lt;dbl&gt; 12, 16, 32, 10, 27, 2, -26, 49, -6, -4, 137, -13, -13, …\n$ carrier        &lt;chr&gt; \"MQ\", \"MQ\", \"MQ\", \"MQ\", \"MQ\", \"VX\", \"VX\", \"MQ\", \"MQ\", \"…\n$ flight         &lt;int&gt; 4650, 4401, 3768, 4599, 4534, 399, 11, 3737, 4406, 4490…\n$ tailnum        &lt;chr&gt; \"N542MQ\", \"N730MQ\", \"N9EAMQ\", \"N518MQ\", \"N722MQ\", \"N627…\n$ origin         &lt;chr&gt; \"LGA\", \"LGA\", \"EWR\", \"LGA\", \"LGA\", \"JFK\", \"JFK\", \"EWR\",…\n$ dest           &lt;chr&gt; \"ATL\", \"DTW\", \"ORD\", \"MSP\", \"XNA\", \"LAX\", \"SFO\", \"ORD\",…\n$ air_time       &lt;dbl&gt; 134, 105, 139, 166, 233, 361, 356, 148, 80, 101, 118, 5…\n$ distance       &lt;dbl&gt; 762, 502, 719, 1020, 1147, 2475, 2586, 719, 427, 479, 5…\n$ hour           &lt;dbl&gt; 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 6, 8, 8, 8, 8, 18, 9, 9, …\n$ minute         &lt;dbl&gt; 0, 5, 0, 30, 5, 0, 30, 10, 10, 15, 30, 25, 35, 40, 50, …\n$ time_hour      &lt;dttm&gt; 2013-01-01 06:00:00, 2013-01-01 06:00:00, 2013-01-01 0…\n$ name           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nleft_joins are useful if we want to join two tables, but want to avoid dropping any rows from a ‘gold standard’ table."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html#outer-join",
    "href": "archive/AY-2024-FALL/labs/lab05.html#outer-join",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Outer Join",
    "text": "Outer Join"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html#advanced-join-specifications",
    "href": "archive/AY-2024-FALL/labs/lab05.html#advanced-join-specifications",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Advanced Join Specifications",
    "text": "Advanced Join Specifications"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html#cumulative-operators",
    "href": "archive/AY-2024-FALL/labs/lab05.html#cumulative-operators",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Cumulative Operators",
    "text": "Cumulative Operators"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html#rank",
    "href": "archive/AY-2024-FALL/labs/lab05.html#rank",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "*_rank",
    "text": "*_rank"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html#advanced-joins",
    "href": "archive/AY-2024-FALL/labs/lab05.html#advanced-joins",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Advanced Joins",
    "text": "Advanced Joins\n\ncross_join\n\n\nsemi_join\n\n\nanti_join\n\n\nnest_join"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html#bind_rows-and-bind_columns",
    "href": "archive/AY-2024-FALL/labs/lab05.html#bind_rows-and-bind_columns",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "bind_rows and bind_columns",
    "text": "bind_rows and bind_columns"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab05.html#footnotes",
    "href": "archive/AY-2024-FALL/labs/lab05.html#footnotes",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that some SQL engines use LEFT OUTER JOIN than LEFT JOIN. Because OUTER is a bit ambiguous, dplyr emphasizes full_ vs left_ in its function naming. Also note the convention of dplyr names - lower case, underscore separated - and that it differs from SQL syntax.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab04.html#rs-missing-data-model---na",
    "href": "archive/AY-2024-FALL/labs/lab04.html#rs-missing-data-model---na",
    "title": "STA 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "R’s Missing Data Model - NA",
    "text": "R’s Missing Data Model - NA\nLet’s begin with a simple question: what is the single most delayed flight in our data set:\n\n\n\n\n\n\n\n\nHmmmm…That’s odd. Certainly something has to be the maximum arrival delay - why did we get no rows back?\nLet’s look at this expression more closely: firstly, what happens if we simply fix a delay amount?\n\n\n\n\n\n\n\n\nThat’s fine. So perhaps the problem was in computing max(arr_delay).\n\n\n\n\n\n\n\n\nThat’s weird - what is this NA object?\nNA is R’s representation of missing data: this is not a NaN object you have seen from other languages. NaN represents invalid arithmetic output (Not-a-Number), e.g,\n\n\n\n\n\n\n\n\nNA is statistical missingness. The data exists - and is well defined - but we simply don’t know it. Like we said above, there is some most delayed flight, but we don’t know what it is.\nThe NA construct is a bit odd when you start with it - but it’s actually one of R‘s great strengths. Missingness matters in data analysis and R forces you to deal with it explicitly. The behavioral rules of NA are reasonably straightforward - NA is ’contagious’. Any calculation that takes at least one NA input usually has NA output. (This is not dissimilar to the “random in, random out” rule of functions of random variables) For example:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat last result may be a bit surprising - isn’t anything times zero just zero?\nThat’s true in ‘real’ math, but not actually true for computer (“floating-point”) math:\n\n\n\n\n\n\n\n\nHere, because 0 * NA could be 0 or NaN, the answer is still unknown and hence NA.\nThere are some rare operations where NA can be “over-ruled” but they are not super common:\n\n\n\n\n\n\n\n\nThis follows because both:\n\n\n\n\n\n\n\n\nso the value of NA doesn’t actually matter here.\nAlso note that not all NA values are ‘the same’:\n\n\n\n\n\n\n\n\nWhy is this the case? Well, suppose we rewrite this as:\n\n\n\n\n\n\n\n\nIs today the same temperature as tomorrow? If we don’t know either temperature, we can’t say!\nSimilarly,\n\n\n\n\n\n\n\n\n\nis.na and na.rm\nWhile it’s certainly helpful that R handles NA values so intelligently for us, it can also be a bit annoying. Eventually we want (non-NA) answers!\nWe generally deal with this in one of two ways:\n\nfiltering out NA values from our data set\nignoring NA values in our calculations.\n\nWe’ve already done a bit of the latter option - ignoring NA values in our calculations - so let’s review it first.\nMost base R functions have an na.rm optional argument to remove NA values. Returning to our motivating example:\n\n\n\n\n\n\n\n\nor indeed\n\n\n\n\n\n\n\n\nThat’s a horrendous (21+ hour) delay! But is it actually the “maximum” delay? It depends… we’ll come back to this example in a bit.\nNot all functions, however, provide a na.rm argument: in those cases, it’s our responsibility to remove the NA values ourselves.\nWe can do this using the is.na function: this takes in a vector of values and finds the NAs:\n\n\n\n\n\n\n\n\nIf we combine this with the filter operator, we now have an efficient way of removing NA values:\n\n\n\n\n\n\n\n\nFrom here, we can get back to our analysis of the most delayed flight:\n\n\n\n\n\n\n\n\nPoor folks!\nNote that I’m using glimpse here to ensure all columns are printed.\n\n\ndrop_na\ndplyr provides a drop_na function which removes any row that has an NA value in any column. It’s a bit of a blunt approach - do you really need remove a row in computing X just because it has an NA value in column Y? - but it can be useful for “quick and dirty” work. I recommend against using it without a thorough manual examination of your data first however.\n\n\nNA values in filter\nEarlier we saw that filter plays funny with NA values. It’s worth being explicit here\n\n\n\n\n\n\n\n\nfilter checks for TRUE conditions - not for “not FALSE”. Because of this, checks which result in NA lead to dropped rows. This means that most NA rows are automatically discarded when you start filtering.\nThis isn’t a bad default - but it’s one you should be aware of. For instance, in our motivating example:\nWe might use the following to compute the average arrival delay:\n\n\n\n\n\n\n\n\nbut this drops\n\n\n\n\n\n\n\n\nflights for which we have no arrival delay information. Of these,\n\n\n\n\n\n\n\n\nwe even have an arrival time but the delay itself is missing for some reason. Is it fair to exclude these flights or should we compute the delay ourselves? For flights that are missing arrival and departure times (i.e., cancelled flights), should we exclude them? Are they infinitely delayed? 24 hour delayed (assuming passengers were rebooked to the same flight on the next day)?\nThere’s no clear right-or-wrong answer to questions like this. It’s all context dependent: if you are the DOT trying to ensure good customer experience, a cancelled flight is very delayed; if you are instead a Boeing engineer looking to improve flight speeds, the cancelled flights simply aren’t useful to you.\nWhen faced with these challenges, data scientists often give the answer “defer to subject matter experts (SMEs)”. Unfortunately, we rarely have the resources to have a qualified SME at hand to answer ever little data analytic question we may have.\nI instead advocate for a strategy of reproducible transparency. Using tools like quarto, we can show our code and document the choices made. Then, when we share our results with an SME,"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab04.html#boolean-operators-and-filter",
    "href": "archive/AY-2024-FALL/labs/lab04.html#boolean-operators-and-filter",
    "title": "STA 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "Boolean Operators and filter",
    "text": "Boolean Operators and filter\nfilter() lets you use a logical test to extract specific rows from a data frame. To use filter(), pass it the data frame followed by one or more logical tests. filter() will return every row that passes each logical test.\nSo for example, we can use filter() to select every flight in flights that departed on January 1st:\n\n\n\n\n\n\n\n\nThe filter function is similar to the WHERE clause in SQL. As we will later see, it can also be used to implement the HAVING clause, when applied in conjunction with group_by.\nLike all dplyr functions, filter() returns a new data frame for you to save or use. It doesn’t overwrite the old data frame. If you want to save the output of filter(), you’ll need to use the assignment operator, &lt;-.\nRerun the command in the code chunk below, but first arrange to save the output to an object named jan1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood job! You can now see the results by running the name jan1 by itself. Or you can pass jan1 to a function that takes data frames as input.\nDid you notice that this code used the double equal operator, ==? == is one of R’s logical comparison operators. Comparison operators are key to making full use of filter(), so let’s take a closer look at them.\n\nLogical Comparisons\nR provides a suite of comparison operators that you can use to compare values: &gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal). Each creates a logical test. For example, is pi greater than three?\n\npi &gt; 3\n\n[1] TRUE\n\n\nWhen you place a logical test inside of filter(), filter applies the test to each row in the data frame and then returns the rows that pass, as a new data frame.\nOur code above returned every row whose month value was equal to one and whose day value was equal to one.\n\nWatch out!\nWhen you start out with R, the easiest mistake to make is to test for equality with = instead of ==. When this happens you’ll get an informative error:\n\n\n\n\n\n\n\n\nIf you give filter() more than one logical test, filter() will combine the tests with an implied “and.” In other words, filter() will return only the rows that return TRUE for every test. You can combine tests in other ways with Boolean operators…\n\n\n&, |, and !\nR uses Boolean operators to combine multiple logical comparisons into a single logical test. These include & (and), | (or), ! (not or negation), and xor() (exclusive or).\nBoth | and xor() will return TRUE if one or the other logical comparison returns TRUE. xor() differs from | in that it will return FALSE if both logical comparisons return TRUE. The name xor stands for exclusive or.\nStudy the diagram below to get a feel for how these operators work.\n\n\n\nIn the figure above, x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each command selects.\n\n\n\n\nCommon mistakes\nIn R, the order of operations doesn’t work like English. You can’t write filter(flights, month == 11 | 12), even though you might say “finds all flights that departed in November or December”. Be sure to write out a complete test on each side of a Boolean operator.\nHere are four more tips to help you use logical tests and Boolean operators in R:\n\nA useful short-hand for this problem is x %in% y. This will select every row where x is one of the values in y. We could use it to rewrite the code in the question above:\n\n\n\n\n\n\n\n\n\n\nSometimes you can simplify complicated subsetting by remembering De Morgan’s laws: !(x & y) is the same as !x | !y, and !(x | y) is the same as !x & !y. For example, if you wanted to find flights that weren’t delayed (on arrival or departure) by more than two hours, you could use either of the following two filters:\n\n\n\n\n\n\n\n\n\n\nAs well as & and |, R also has && and ||. Don’t use them with filter()! You’ll learn when you should use them later.\nWhenever you start using complicated, multipart expressions in filter(), consider making them explicit variables instead. That makes it much easier to check your work.\n\n\n\n\nExercises\n\nFilter Statements\nUsing filter and various Boolean operators, find all flights satisfying the following conditions.\n\nHad an arrival delay of two or more hours\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(arr_delay &gt; 120)\nflights |&gt; filter(arr_delay &gt; 120)\n\n\n\n\n\n\n\nFlew to Houston (IAH or HOU)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(dest %in% c(\"IAH\", \"HOU\"))\nflights |&gt; filter(dest %in% c(\"IAH\", \"HOU\"))\n\n\n\n\n\n\n\nWere operated by United (UA), American (AA), or Delta (DL)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\nflights |&gt; filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n\n\n\n\n\n\nDeparted in summer (June, July, or August)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(month &gt;= 6, month &lt;= 8)\nflights |&gt; filter(month &gt;= 6, month &lt;= 8)\n\n\n\n\n\n\n\nArrived more than two hours late, but didn’t leave late\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(arr_delay &gt; 120, dep_delay &lt;= 0)\nflights |&gt; filter(arr_delay &gt; 120, dep_delay &lt;= 0)\n\n\n\n\n\n\n\nWere delayed more than an hour, but made up more than 30 minutes in flight\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(dep_delay &gt; 60, (dep_delay - arr_delay) &gt; 30)\nflights |&gt; filter(dep_delay &gt; 60, (dep_delay - arr_delay) &gt; 30)\n\n\n\n\n\n\n\nDeparted between midnight and 6am (inclusive)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter((dep_time &lt;= 600) | (dep_time == 2400))\nflights |&gt; filter((dep_time &lt;= 600) | (dep_time == 2400))"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab04.html#grouped-operations",
    "href": "archive/AY-2024-FALL/labs/lab04.html#grouped-operations",
    "title": "STA 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "Grouped Operations",
    "text": "Grouped Operations\nIn this week’s preassignment, you also already saw the basics of the group_by operator for performing analyses on subgroups. The most common use of group_by is to modify summarize to perform group-wise summarization. We’ll next explore how it can be used to do group level filtering, similar to an SQL HAVING clause.\nAs before, let’s start by asking what is the average arrival delay (after removing NA values)?\n\n\n\n\n\n\n\n\nOk. But now suppose we want to know which carrier had flights that were later than average? We_could_ simply copy the value over into a new line of code:\n\n\n\n\n\n\n\n\nTo get carrier-wise statistics, we might try:\n\n\n\n\n\n\n\n\nThis works, but it requires us to keep the number 6.9 at hand, which is a bit inconvenient.\nWe next might be tempted to use a variable here to avoid hard-coding a specific value:\n\n\n\n\n\n\n\n\nThis is definitely better! If our data changes, we don’t have to worry about the number 6.9 being ‘out of date’. But it’s still maybe a bit clunky: we filter our data twice for NA values and have to repeat ourselves.\nLet’s try something else:\n\n\n\n\n\n\n\n\nThis creates a new column called mean delay. On its own, it’s not very interesting:\n\n\n\n\n\n\n\n\nNote the trick of using everything() inside a select statement to reorder columns.\nThe mean_delay column simply repeats the number 6.9 over and over. (Recall R’s recycling rules- we needed a long vector here, so the output of mean was repeated enough to fill the whole table.) But now we can work with this:\n\n\n\n\n\n\n\n\nand, if we want, we can get the carrier specific statistics:\n\n\n\n\n\n\n\n\nPretty nice! And when it matters - for very large data stored on a database - a little faster to boot!\nBefore going deeper down this path, what happens if we move the group_by earlier in our pipeline?\n\n\n\n\n\n\n\n\nDefinitely different! But why?\nTo see the difference, let’s compare the mean_delay column:\n\n\n\n\n\n\n\n\nWe now see here that the mean_delay is computed “group-wise”, so we’re not getting flights that are delayed compared to an average flight; we are instead counting flights that are delayed compared to an average flight on that airline. Put another way, we’re holding American Airlines (AA) and Delta (DL) flights to a higher standard than Jet Blue (B6) or ExpressJet (EV).\nAs always - the question you should ask yourself is not “is this the right thing” but “when is this the right thing?”. It’s simply a different question!\nRecall that group_by followed by a summarize removes one “layer” of grouping. If we use this group_by + mutate + filter construction, the result is still grouped, which can lead to weird bugs. To address this, it is sometimes easier to use the .by argument to mutate and filter which will modify the grouping for that command only.\n\n\n\n\n\n\n\n\nIt’s a matter of taste.\n\nHAVING clause\nRecall that a SQL HAVING clause applies group-level filtering based on some summary statistics: this is easy enough in dplyr.\nFor example, suppose we want the average flight delays of large airlines, which we can define as those with more than 10,000 departures in our data set.\nWe can compute this in two ways: directly, computing the number of flights and average delay for each airline.\n\n\n\n\n\n\n\n\nThis totally works, but now we’ve lost all the other flight-level information. An alternate approach is to compute counts group-wise and filter before averaging:\n\n\n\n\n\n\n\n\nThis has the advantage of being readily adaptable to other non-summarizing questions: for instance, of the delayed flights of the major carrier, how many were going to Houston?\n\n\n\n\n\n\n\n\nHere, we re-used the n column and so the old value of n was quietly replaced. This is probably ok with a simple variable name like n (which wasn’t all that interesting) but for “raw” data columns, you probably should avoid this.\nIn class, we’ll do more exercises based on group-specific filtering, both filtering on groups and filtering within groups. See if you can answer:\n\nWhat carrier has the lowest rate of delayed flights?\nWhat carrier has the highest chance of early arrivals?\nWhat carrier is most likely to “make up time in flight” after a delayed departure?\nWhich origin airport has the highest rate of delays?\nWhich month has the most flights?\nWhat is the furthest flight in this data?\nWhat is the shortest flight in this data?\nAre longer flights more likely to be delayed than short ones?\n\n\nThe readings in this tutorial follow R for Data Science, section 5.2. The exercises for filter were adapted from the official documentation of the learnr package."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab04.html#footnotes",
    "href": "archive/AY-2024-FALL/labs/lab04.html#footnotes",
    "title": "STA 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 2013 NYC version of this data has become a semi-standard teaching example, but the US Bureau of Transportation Statistics releases new versions of this data constantly. If you are interested in performing this type of analysis for a different set of airports or a different time period, check out the anyflights package. It’s very easy - but a bit slow - to get flight data from almost any US airport this way. If you want to develop your data cleaning skills, it’s a great exercise to parse the BTS website directly and compare your output with the anyflights package.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab03.html",
    "href": "archive/AY-2024-FALL/labs/lab03.html",
    "title": "STA 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab03.html#review-of-r",
    "href": "archive/AY-2024-FALL/labs/lab03.html#review-of-r",
    "title": "STA 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Review of R",
    "text": "Review of R\nIn this section, we will review some of the basic ‘built-in’ features of R. In the next section (Packages) we will discuss how to add to the “base-bones” functionality. When working with R, there are two interacting ‘subsystems’ in play:\n\nThe R language and interpreter: this is the part of R that is similar to python or C/C++. You will write R code in the R language and the R interpreter will run that code. The fact that all of these elements are called R is a bit confusing, but once you get the hang of things, the distinctions will melt away.\nR packages: When working in R, you do not have to start from scratch every time. Other programmers make sets of code available to you in the form of packages. For our purposes, a package can contain two things:\n\nPre-written functions to help you achieve some goal\nData sets Most of the time, the primary purpose of a package is sharing functions and code: there are easier ways to share data with the world.\n\n\nWhen you first downloaded R, you downloaded the interpreter and a set of base packages written by the “R Core Development Team”.\nRun the following code to see what your R environment looks like:\n\n\n\n\n\n\n\n\nCompare the output of running this here-in the browser-with what you get by running sessionInfo() on your machine.\nThere is lots of useful information here, including\n\nthe version of R being used\nthe operating system\nthe numerical linear algebra libraries (BLAS, LAPACK) used\nsystem language and time zone information\nloaded packages\n\nWhen asking for help, always include the output of the sessionInfo() command so that your helper can quickly know how your system is configured.\n\nPackages\nR code is distributed as packages, many of which come included with R by default. These are the base packages, and they are noted in your sessionInfo(). But we can do many more things with R using contributed (non-base) packages!\nThe most common platform for distributing R packages is CRAN, the Comprehensive R Archive Network, available at https://cran.r-project.org/. You have likely already visited this site to download R. The available.packages() function in R lists all packages currently on CRAN. We can see that there are many:\n\nNROW(available.packages())\n\n[1] 21847\n\n\nIf you want to use a contributed package, you need to do two things:\n\nDownload it from CRAN and install it onto your computer (one time)\nLoad it from your hard drive into R (every time you restart R)\n\nThe first step - download and install - can be completed using the install.packages() function. For example, to install the palmerpenguins package, I would run:\n\ninstall.packages(\"palmerpenguins\")\n\nThis will automatically download and install this package for me. R is helpful and also tries to automatically install all packages that a given package relies upon. Because of this, it is often sufficient to install the “last step” and trust R to handle the dependencies automatically. In this course, most of the packages we use can be automatically installed by installing the tidyverse package.\n\ninstall.packages(\"tidyverse\")\n\nNote that there really isn’t much in the tidyverse package we want, but it’s a useful proxy for a much larger set of packages.\nOnce a package is installed, we need to load it into R with the library function:\n\nlibrary(palmerpenguins)\n\nAfter doing this, we have access to the contents of the palmerpenguins package until we restart R.\n\nNote that the install.packages function wants you to quote its argument, but library does not. This is a weird historical quirk of R that you will trip up on many times before this course ends.\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\nFor example, palmerpenguins package provides a data set of penguin measurements. If we try to get the data set without loading palmerpenguins, we get an error message:\n\n\n\n\n\n\n\n\nAfter we install and load palermerpenguins, we are good to go:\n\n\n\n\n\n\n\n\nSo much tuxedo goodness!\nIn general, if you get a error message of the form Error: object 'X' not found, you should:\n\nMake sure you spelled X properly\nIf X comes from a package, make sure you library() that package.\n\nThere’s no harm to library()-ing a package multiple times; if you install.packages() a package that you have already loaded, you may need to restart R.\n\nAs mentioned last week, I strongly recommend never saving your workspace in R or RStudio. One of the things “saved” in a workspace is the list of loaded packages, so it becomes essentially impossible to reinstall a package properly.\n\n\n\nVariables and Assignment\nWhenever you type a “word” of R code, it must be one of three things:\n\nA reserved word: this is a small set of keywords that R keeps for its own use. These have special rules for their use that we’ll learn as we go along. The main ones are: if, else, for, in, while, function, repeat, break, and next.\nIf you use one of these words and get a weird error message, it’s likely because you aren’t respecting the special rules for these words.\nFor the nitty gritty, see the Reserved help page but feel free to skip this for now. The Control help page gives additional details.\n(When you run a help page in this tutorial, it looks a bit funny. Try running ?Reserved directly in RStudio for better formatting.)\nA “literal”. This is a word that represents “just the thing” without any additional indirection. The most common types of literals are:\n\nNumeric: e.g., 3, 42.0, or 1e-3\nString: e.g., 'a', \"beach\", or 'cream soda' There are a few rules for literals, but the most important is that strings begin and end with the same character, either a single quote or a double quote. When R sees a single quote, it will read everything until the next single quote as one string, even if there’s a double quote inside.\n\nTry some literals:\n\n\n\n\n\n\n\n\n\nWhat does the literal 0xF represent? (You don’t need to worry about why. This is a fancier literal than we will use in this class.) - A “variable name”. This is the most common sort of “word” in code. It is used to something without actually having to know what it is.\nWe can create variables using the “assignment” operator: &lt;-\n\n\n\n\n\n\n\n\nWhen you read this outloud, read &lt;- as “gets” so x &lt;- 3 becomes “x gets 3.”\nWhen we use the assignment operator on a variable, it overwrites the value of a variable silently and without warning\n\n\n\n\n\n\n\n\nWe also put expressions on the right hand side of an assignment:\n\n\n\n\n\n\n\n\nAlso note the trick we’ve used here a few times: a “plain” line of code without an assignment generally prints its value.\n\n\nComments\nWhen you include a # symbol, R will ignore everything after it. This is called a comment and you can use it to leave notes to yourself about what you are doing and why.\n\n\nVector Data Types\nA vector is a ordered array of the same sort of thing (e.g., all numbers or all strings). We can create vectors using the c command (short for concatenate).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the above example to c(1, \"a\", 3) and examine the output. What happened? Why?\n\n\n\nTo see the type (sort) of a vector, you can use the str command.\n\n\n\n\n\n\n\n\nstr(x) tells us about the structure of x. Here, we see that x is a numeric vector of length 3.\nR will try to do the right thing when doing arithmetic on vectors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen you give R vectors of different lenghts, it will “recycle” the shorter one to the length of the longer one.\n\n\n\n\n\n\n\n\nThis can be a double-edge sword when the two vectors don’t fit together so nicely:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow was the last element of x+y computed?\n\n\n\nHere we see also that R printed a warning message. A warning message is R’s way of saying “something is funny, but I can still do this” while it (successfully) implements your command. It’s here to help you, but sometimes can be safely ignored if you’re sure about what you’re doing.\nAn error is a “I can’t do this” message. When R encounters an error it stops and does not fully execute the command\n\n\n\n\n\n\n\n\nHere we get an error because there is no meaningful way to multiply a string by a number, unlike earlier where the recycling rule told R what to do, even if it was probably a bad idea."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab03.html#functions",
    "href": "archive/AY-2024-FALL/labs/lab03.html#functions",
    "title": "STA 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Functions",
    "text": "Functions\n\nFunctions\nIn many of these exercises, we have used commands that have the form NAME() with zero or more comma-separated elements in the parentheses.\nThis represents a function call. Specifically, the command func(x, y) calls the function named func with two arguments x and y.\nFunctions are the verbs of the programming world. They are how anything gets done. So far, we’ve only used some basic functions:\n\nc: the concatenate function\nprint: the print function\nstr: the structure function\nlist: the list making function\n\nBut there are tons of other useful ones!\nTry these out: - length on a vector - colnames on a data frame (like PlantGrowth) - toupper on a string (vector) - as.character on a numeric value\n\n\n\n\n\n\n\n\n\nArguments: Positional and Keyword\nThe inputs to a function are called the arguments. They come in two forms: - Positional - Keyword\nSo far we have only seen positional arguments. The function interprets them in an order that depends on they were given:\n\n\n\n\n\n\n\n\nHere paste combines two values into a string. We get different output strings depending on the order of the input.\nOther arguments can be passed as keyword arguments. Keyword arguments come with names that tell functions how to interpret them. For example, the paste function has an optional keyword argument sep that controls how the strings are combined.\n\n\n\n\n\n\n\n\nKeyword arguments typically have defaults so you don’t need to always provide them. For the paste function, the sep defaults to \" \".\n\n\n\nCreating Your Own Functions\nWhen you want to create your own function, you use a variant of the assignment structure\n\nmy_addition &lt;- function(x, y) {\n    x + y\n}\n\nLet’s break this into pieces:\n\nOn the left hand side of the assignment operator &lt;-, we see the function name. This works exactly the same as vector assignment.\nImmediately to the right of the assignment operator, we see the keyword function. This tells R that we are defining a function.\nAfter the word function, we see the “argument list”, i.e., the list of inputs to the function (comma separated). Here, we are not providing default values for any function.\nFinally, between the curly braces, we get the body of the function. This is actually the code defining a function’s behavior. You can do basically anything here! Define variables, do arithmetic, load packages, call other functions - it’s all valid. (In fact, you can even define a function within a function, but that’s sort of advanced.)\nThe last line of the body (here the only line) defines the return value of the function, i.e., its output.\n\nThis function will add two numbers together. Now that we’ve defined it, we can use it just like a built-in function:\n\nmy_addition(2, 4)\n\n[1] 6\n\n\nTip: You can see the code used to define any function by simply printing it: think of the code as being the “value” and the function name as the variable name. (This isn’t actually just a metaphor - it’s literally true!)\n\nDefault Arguments\nSometimes, we want functions to have default but changeable behvaior. This is default arguments come in. If the user provides a value, the function uses it, but otherwise the default is used.\nFor example,\n\nmake_bigger &lt;- function(x, by=2){\n    x + by\n}\n\nmake_bigger(3)\n\n[1] 5\n\nmake_bigger(3, by = 3)\n\n[1] 6\n\n\nHere by defaults to 2, but the user is required to supply x because it has no default.\n\nmake_bigger(by=3)\n\nError in make_bigger(by = 3): argument \"x\" is missing, with no default\n\n\nThere are lots of details in the mechanics - they even can be ‘dynamic’ using some tricks - but in general, they should “just work.”"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab03.html#control-flow",
    "href": "archive/AY-2024-FALL/labs/lab03.html#control-flow",
    "title": "STA 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Control Flow",
    "text": "Control Flow\nSo far, all of the code we have written executes linearly, one line at a time. To write complex programs, however, we sometimes need code to execute in other ways: e.g., going line by line through a complex data set running the same code (a “loop”) or doing different things depending on the value of a variable (a “conditional”). This brings us to the topic of control flow, or how a program gets executed.\n\nConditionals\nPerhaps the most common control flow operator is the “conditional” - the if operator. In R, the if operator looks like this:\n\nif(TEST){\n    # Some code goes here\n    # This gets run if `TEST` is true\n} else {\n    # Some code goes here\n    # This gets run if `TEST` if false\n}\n\nFor example\n\nx &lt;- 3\nif(x &gt; 0){\n    print(\"x is positive!\")\n} else {\n    print(\"x is negative\")\n}\n\n[1] \"x is positive!\"\n\n\nThe element inside the if (the test statement) should ideally be Boolean (TRUE/FALSE-ish) but R will make a reasonable guess if it isn’t.\nNote that you can omit the else part and the second set of braces that go with it, but the first set of braces, immediately after if(), should always be there.\n\n\n\n\n\n\n\n\nChange the value of x and see what happens. Next, modify this by adding an else statement to handle the case of odd numbers.\nNote that we’re using the %% operator here. If you haven’t seen it before, recall you can get help by typing\n\n?%%\n\nin R. In this case, %% is a modulo operator; that is, it is the “remainder” from division. (Do you see how it works here?)\nWe’ll practice using conditional operators below.\n\n\nLooping\nIn other ## Programming Exercises\nWrite functions to perform each of the following tasks.\n\nWrite a function that takes in a vector of numbers, calculates the length and maximum value of the vector, and prints that information to the screen in a formatted way.\n\n&gt; func_1(c(1, 2, 3, 5, 7))\nThe largest value in that list of 5 numbers is 7.\n\n&gt; func_1(c(1, 2, 5, 5))\nThe largest value in that list of 4 numbers is 5.\nTo make your output as attractive as possible, you might want to use the cat command instead of the print command.\n\nWrite a program that tests whether its (integer) outputs are leap years. Recall the leap year rules:\n\nA year is a leap year if it is divisible by 4\nBut it is not a leap year if it is divisible by 100\nUnless it is also divisible by 400\n\n\n&gt; leap_year(2023)\nFALSE\n&gt; leap_year(2024)\nTRUE\n&gt; leap_year(2100)\nFALSE\n&gt; leap_year(2000)\nTRUE\nRemember our discussion of the %\\% operator from class.\n\nWrite a function to greet your classmates with varying levels of enthusiasm. It should have three optional arguments:\n\nname. The name of the person to greet. Default \"friend\"\ntimes. The number of times to repeat the greeting.\nemphasis. A Boolean (TRUE/FALSE) value indicating whether the greeting should end with an exclamaition point. (Default FALSE)\n\n\n&gt; greetings()\nHello, friend\n&gt; greetings(name=\"Michael\")\nHello, Michael\n&gt; greetings(times=2)\nHello, friend\nHello, friend\n&gt; greetings(emphasis=TRUE)\nHello, friend!\n&gt; greetings(\"Michael\", 5, TRUE)\nHello, Michael!\nHello, Michael!\nHello, Michael!\nHello, Michael!\nHello, Michael!\n\nThe Riemann Zeta Function is a famous function in analytic number theory1 defined as \\[\\zeta(k) = 1 + \\left(\\frac{1}{2}\\right)^k + \\left(\\frac{1}{3}\\right)^k + \\dots = \\sum_{i=1}^{\\infty} i^{-k} \\] We cannot implement an infinite series in R, but we can get very close by taking a large number of terms in the series (e.g, the first 500,000). Implement the zeta function and show that \\(\\zeta(2) = \\frac{\\pi^2}{6}\\)\n\n&gt; zeta(2)\n[1] 1.644932\n&gt; zeta(3)\n[1] 1.202057\n&gt; zeta(4)\n[1] 1.082323\n&gt; all.equal(zeta(2), pi^2/6, tol=1e-4)\n[1] TRUE\n\nHero of Alexandria developed a method for computing square roots numerically. He showed that by performing the following update repeatedly, \\(x\\) will converge to \\(\\sqrt{n}\\): \\[x \\leftarrow \\frac{1}{2}\\left(x + \\frac{n}{x}\\right)\\] You can start with any positive \\(x\\), but \\(n/2\\) is a good choice.\nImplement this method to compute square roots. Use an optional keyword argument (default value 100) to control how many iterations are performed:\n\n&gt; hero_sqrt(100)\n[1] 1.644932\n&gt; hero_sqrt(3)^2\n[1] 3\n&gt; hero_sqrt(3, iter=2)\n[1] 1.732143\n&gt; hero_sqrt(3000)\n[1] 54.77226"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab03.html#footnotes",
    "href": "archive/AY-2024-FALL/labs/lab03.html#footnotes",
    "title": "STA 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nANL is basically the application of calculus techniques to prove properties of prime numbers: it’s a surprisingly powerful approach.↩︎"
  },
  {
    "objectID": "preassigns/pa07.html",
    "href": "preassigns/pa07.html",
    "title": "STA 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "",
    "text": "Due Date: 2025-03-19 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week we begin to explore statistical visualizations. Visualizations play several interrelated roles in statistical practice: we use visualization to explore new data sets, to see how well models fit to data, and to communicate the results of analyses to new audiences. Compared with the ‘point summary’ tools we have discussed to date, visualizations are far more flexible and more powerful: we can extract novel insights from data visualizations, but we can also deceive ourselves and others.\nAs you review this document, also watch how I iterate and refine each figure until I have something I’m finally happy with. This is quite typical of how working data scientists produce plots: you rarely know exactly what you want, particularly before you begin to explore your data. You should adapt a similar pattern of “take a sad plot and make it better” as you create plots for your mini-projects and, ultimately, your final project."
  },
  {
    "objectID": "preassigns/pa07.html#grammar-of-graphics",
    "href": "preassigns/pa07.html#grammar-of-graphics",
    "title": "STA 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nComputer graphics is a field of essentially infinite possibility - anything you dream can be represented in the digital domain. At one limit, we have the option to work on a “per pixel” basis, telling the computer exactly what to draw and where. Because this is clearly overwhelmingly monotonous, very little work is actually done at such fine detail and abstraction layers are provided to automate the low-level detail work. We will use an abstraction model known as the Grammar of Graphics.\nDesigned by Leland Wilkinson and popularized by Hadley Wickham, the Grammar of Graphics poses a set of rules for visualizing (tidy) data. It draws its name from the concept of linguistic grammar - the rules that dictate how basic elements (nouns, verbs, and adjectives) may and may not be combined into clear and meaningful sentences The Grammar of Graphics provides specifications for combining different plot elements (legends, data, axes, etc.) into clear and meaningful statistical graphics. Taking the metaphor too far, we might say that the Grammar of Graphics is the Strunk and White to the Ska that are “Infographics.”\nThe grammar of graphics has several interconnected components, which are combined to form a meaningful graphic. We assume that we have a “tidy” data set we want to visualize; recall that, by “tidy”, we mean that our data is\n\nOrganized in a rectangular array\nHomogenously-typed within a column\nOne observation per row\nOne value per cell\n\nGiven this form of tidy data, the grammar of graphics provides us the “parts of speach” necessary to convert tidy data to a visual representation. We’ll only cover the basic components here, leaving more advanced tools for class session:\n\nThe aesthetics are mappings between columns of the data and aspects of the visualization. For instance, “put the grade column on the \\(y\\) axis” or “use color to represent the course ID”.\nScales convert data values to the aesthetics: scales may be quite trivial, e.g. placing continuous values on the \\(x\\) axis in proper order, or more advanced, e.g., binning values and converting them to a sequence of perceptually-ordered colors. )\nGeometric elements or geoms specify how the data are represented on the page through the scales. geoms include basic representations, like points for a scatter plot or lines for a trend plot, as well as more complex objects like boundaries on a map.\nGuides provide interpretational assistance to the viewer. Most guides take the form of legends.\n\nThat’s all a bit abstract, so let’s put it into practice. For now, you shouldn’t worry so much about what each of these really mean; it’s just useful to have a rough sense of what “knob” you want to turn to modify plots."
  },
  {
    "objectID": "preassigns/pa07.html#getting-started-with-ggplot2",
    "href": "preassigns/pa07.html#getting-started-with-ggplot2",
    "title": "STA 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "Getting started with ggplot2",
    "text": "Getting started with ggplot2\nThe leading implementation of the grammar of graphics is the ggplot2 package in R (gg = Grammar of Graphics). It comes to us from Hadley Wickham and the tidyverse team, who also developed the dplyr and tidyr tools we have been using for the past several weeks.\nLet’s begin by using ggplot2 to explore our penguins data:\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nggplot(penguins)\n\n\n\n\n\n\n\n\nSomewhat underwhelming…\nggplot2, like many of the tools in this course, do exactly what we ask, and no more. Because we have not specified any of the Grammar of Graphics elements, we only get a blank canvas. Let’s now begin by adding an aesthetic to map some of the elements of our data to aspects of our plot.\nSpecifically, suppose we want to see how flipper length correlates with body mass. Let’s make a scatter plot with flipper length on the \\(x\\)-axis and body mass on the \\(y\\)-axis.1\n\nggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g))\n\n\n\n\n\n\n\n\nOk - this is perhaps a bit better. We can see that the columns flipper_lenth_mm and body_mass_g have been placed on the \\(x\\)- and \\(y\\)-axes as we wanted, but we still don’t see anything.\nWe need a geom to actually put “ink to paper”. The simplest geom is a point, useful for making scatter plots.\n\nggplot(penguins, \n       aes(x=flipper_length_mm, y=body_mass_g)) + \n    geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nPretty nifty! Before we go forward, note that ggplot2 adds elements to create ever more complex plots. This is different than dplyr where we “piped” data from one step to the next, refining it along the way.\nHow can we improve the plot above? Before anything else, let’s clean up the \\(x\\) and \\(y\\) axis labels. While the default behavior of showing variable names is helpful for exploratory data analysis, we never want to let variable names “leak” in plots we intend to share with others. We should instead use meaningful (and attractive) axis labels.\n\nggplot(penguins, \n       aes(x=flipper_length_mm, y=body_mass_g)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe can also take advantage of the theme mechanisms of ggplot2 to change the color of the “infrastructure” of our plot. The theme mechanism doesn’t change how the data itself is visualized, but it controls how things like the background, font sizing, etc behave. I tend to prefer the black and white theme over the grey default:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, y=body_mass_g)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdding Color to Depict Species\nOur data set three distinct species and the correlation between flipper and body size may vary across species. Let’s add some color to our plot: since color maps a data element (species) to a graphical aspect (color) we add it to our aesthetic mapping (aes).\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe see here that a few things happened automatically for us:\n\nThe color element was automatically propagated into geom_point. By default, any “top-level” aesthetics are automatically applied to any geom that can handle them.\nA legend was created.\nA color scale was chosen.\n\nThe geom_point help page tells us which aesthetics are required (\\(x, y\\)) and which are optional for the point geom. We couldn’t have gotten away without providing \\(x, y\\) coordinates, but until this point, we were just using the default (black) color.\nTo improve the look of the colors, we can choose a different color scale. I tend to like the colors of the Color Brewer project, though strictly speaking these are designed for use in maps, not scatter plots. You can access these in ggplot2 as follows:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", palette=2)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere, I’m using a type=\"qual\" (qualitative) palette because there is no inherent ordering to the penguin types. I like the “bolder” colors of the second palette in this set, but you can adjust the number to try different schemes.\nNext, let’s improve the look of the legend. As before, we see that it is by default titled with the variable name (species). We can provide a proper title instead:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nI still don’t like this legend on the side - it takes too much room, so let’s move it below the image instead. This involves changing a “non-data” element of the plot, so we go through the theme machinery. theme() allows an enormous number of possible changes, but here we want legend.position:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNot so bad!\n\n\nVisualizing Statistical Relationships\nRecall that our goal was to measure the correlation between Body Mass and Flipper Length. We can visualize this correlation on the plot by adding a regression line (recall that for univariate regression like this, the slope of the regression line is \\(\\hat{\\beta} = \\rho_{XY}\\frac{\\sigma_Y}{\\sigma_X}\\)).\nThis is a new geometric element, called a smoother. ggplot2 allows many possible smoothers, but let’s use the lm (linear model) version, which we specify by setting the method argument:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\") + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nBy default, ggplot2 is giving us confidence intervals around the linear trend. These are sometimes useful, but perhaps a bit crowded for now, so let’s turn them off:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\", \n                se=FALSE) + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis is nicer, but maybe still a bit crowded. It would be nicer if we could avoid the “overlaps” of the different species. Here, let’s break out a “small multiples” plot: this is, in essence, a group_by for plotting.\nIn ggplot2 speak, this is called a faceted plot:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\", \n                se=FALSE) + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNot too shabby. But now it’s a bit too hard to tell the lines from the points. Let’s override the color used:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\", \n                se=FALSE, \n                color=\"black\") + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThese plots give us some idea of the correlation, but what if we want actual numbers? We can’t do this in “plain” ggplot2, but now we can take advantage of the enormous number of ggplot2 extension packages. It turns out that the ggpmisc package supports what we need, so let’s download and install it:\n\nif(!require(\"ggpmisc\")) install.packages(\"ggpmisc\")\nlibrary(ggpmisc)\n\nNow we have access to the various geom_, scale_, etc objects from that package. We can now introduce a new category, stat_, that represents statistical transformations or modeling. Generally, these are applied “automagically” for us, as in geom_smooth, but here we need to build our regression models explicitly:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSee how, even though we’re using functionality from outside ggplot2, the structure of the “grammar” makes it easy for all these tools to work well together.\n\n\nFinal Polish\nWe are almost done, but every figure needs a bit of final polish. Firstly, we should add a title, using the ggtitle function. (R has a built-in title function but that won’t help us here)\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn good academic practice, we should always add a footnote citing the source of our data. The palmerpenguins site has appropriate source information:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\") + \n    labs(caption=\"Data provided by Dr. K. Gorman and the Palmer Station, Antarctica LTER\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAt this point, the bottom of our figure looks a bit crowded. To clear out some space, let’s remove the legend from the bottom, since it simply repeats the facet labels:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\") + \n    labs(caption=\"Data provided by Dr. K. Gorman and the Palmer Station, Antarctica LTER\") + \n    guides(color=\"none\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nCleaner and no loss of information. In designing good scientific graphics, the concept of a “ink-to-information” ratio is useful: if you can remove some ink without removing any (relevant) information, you should generally do so. This makes it easier for the reader to identify the important elements of the plot.\nTo make our point even clearer, it is sometimes useful to add a short “summary” to a plot like this:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\", \n            subtitle=\"Flipper Length and Body Mass are positively correlated across species\\nGentoo penguins exhibit the strongest relationship at 70% correlation\") + \n    labs(caption=\"Data provided by Dr. K. Gorman and the Palmer Station, Antarctica LTER\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis is not always a good idea - it requires “hard-coding” the insights for your reader and takes up some space. In scientific writing, I generally prefer to put this sort of summary in a figure caption, while I tend to “say the point” verbally if the figure is destined for a presentation.\nPersonally, I only use this sort of “here is the point” text if I expect a figure to “escape beyond” my presentation and need it to stand fully on its own.\n\n\nConclusions\nggplot2 provides an exceptionally powerful and flexible set of tools for creating statistical visualizations. We will explore it in more depth in class. For now, review the examples above and make sure you see how each plot is created “piecewise” from its various components.\nTo see more about what ggplot2 can do, check out the R Graphics Gallery. If you want to see the specifics of each ggplot2 function, check out the package reference page. To go further with ggplot2, you should also explore its extension gallery."
  },
  {
    "objectID": "preassigns/pa07.html#footnotes",
    "href": "preassigns/pa07.html#footnotes",
    "title": "STA 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI once had a boss who refused to use the terms \\(x\\) and \\(y\\) axis unless quantities called \\(x\\) and \\(y\\) were actually being plotted. Being Cantabrigian, he insisted I use the terms abscissa and ordinate. I will not inflict such pedantry in this course.↩︎"
  },
  {
    "objectID": "preassigns/pa04.html",
    "href": "preassigns/pa04.html",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "",
    "text": "Due Date: 2025-02-19 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week, we begin manipulating R’s most important structure the data.frame. While base R provides tools for working with data.frame objects, our primary focus will be on the tidyverse family of tools. These provide a unified and consistent set of tools for working with data objects."
  },
  {
    "objectID": "preassigns/pa04.html#what-is-a-data.frame",
    "href": "preassigns/pa04.html#what-is-a-data.frame",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "What is a data.frame?",
    "text": "What is a data.frame?\nLast week, we discussed vectors, one-dimensional collections of the same type of object. While vectors are an important computational tool, real data is rarely quite so simple: we collect multiple features (covariates) from each of our observations and these features may be of different type. For example, when performing a political survey, we may record the:\n\nName (character)\nAge (integer)\nGender (factor)1\nDate of Contact (Date)\nLevel of candidate support (double or numeric)\n\nof each respondant. It is natural to organize this in tabular (“spreadsheet”) form:\n\n\n\nName\nAge\nGender\nDate of Contact\nLevel of Support\n\n\n\n\nTimmy\n25\nM\n2024-09-13\n0.25\n\n\nTammy\n50\nF\n2024-06-20\n-1.35\n\n\nTaylor\n70\nX\n2024-08-15\n200\n\n\nTony\n40\nM\n2024-12-25\n0\n\n\nToni\n65\nF\n2024-11-28\n-4\n\n\n\nNote several important features of this data:\n\nEach row corresponds to one, and only one, sample\nEach column corresponds to one, and only one, feature\nThe values in each column are all of the same type\nThe order doesn’t matter: all important data is reflected in the values, not the presentation\n\nGenerally, data in this pattern will be called “tidy”. R represents this type of data as a data.frame object. For more on what it means for data to be “tidy”, read this paper.\n\nTibbles and the Tidyverse\nMany of the tools we will discuss in this class are from a set of related R packages, collectively known as the tidyverse. They are designed to i) read data from outside of R into tidy formats; ii) manipulate data from one tidy format to another; iii) communicate the results of tidy data analyses.\nWhile you can load these packages separately, they are used together so frequently that the tidyverse package exists to load them all simultaneously. I recommend you start most of your analyses with the command:\n\nlibrary(tidyverse)\n\nThis will automatically load the following packages:\n\nlubridate for date and time manipulation\nforcats for factor manipulation\nstringr for string manipulation\ndplyr for data frame manipulation\npurrr for functional programming\nreadr for tidy data import\ntidyr for tidy data manipulation\ntibble for data frame enhancement\nggplot2 for data visualization\n\nThis week, we are focusing on functionality from the dplyr package.\nYou may, from time to time, see reference to tibbles in R documentation. A tibble is a “souped-up” data frame with somewhat better default printing. For almost all purposes-and everywhere in this course- you can substitute tibble with data.frame without issue.\nBefore we get into dplyr, let’s make sure we have a data frame to play with. Let’s bring back our friends, the Palmer penguins:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "preassigns/pa04.html#subsetting-data-frames",
    "href": "preassigns/pa04.html#subsetting-data-frames",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Subsetting Data Frames",
    "text": "Subsetting Data Frames\nOur first task will be to subset data frames: that is, to select only some rows and columns and to return a smaller data frame than the one we started with.\n\nSubsetting Columns\nThe main dplyr function for column subsetting is select(). In its simplest form, we provide a set of column names we want to keep and everything else gets dropped.\n\nselect(penguins, species, island)\n\n# A tibble: 344 × 2\n   species island   \n   &lt;fct&gt;   &lt;fct&gt;    \n 1 Adelie  Torgersen\n 2 Adelie  Torgersen\n 3 Adelie  Torgersen\n 4 Adelie  Torgersen\n 5 Adelie  Torgersen\n 6 Adelie  Torgersen\n 7 Adelie  Torgersen\n 8 Adelie  Torgersen\n 9 Adelie  Torgersen\n10 Adelie  Torgersen\n# ℹ 334 more rows\n\n\nHere, we keep the species and island columns and remove the others.\nPause for a moment to note how the select function is structured: the first argument is the data frame on which we are operating; all following arguments control the resulting behavior. This is a common pattern in dplyr functions, designed to take advantage of another key R functionality, the pipe operator.\nR provides an operator |&gt; which “rewrites” code, so\n\nselect(penguins, species, island)\n\nand\n\npenguins |&gt; select(species, island)\n\nare exactly the same thing as far as R is concerned. You may well ask yourself why bother: the second “piped” operator is a bit longer and a bit harder to type. But just hold on - we’ll see this makes for far cleaner code in the long run.\nJust like base R, if we include - signs in our select statement, we get everything except a certain column:\n\npenguins |&gt; select(-bill_length_mm, -bill_depth_mm, -flipper_length_mm)\n\n# A tibble: 344 × 5\n   species island    body_mass_g sex     year\n   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n 1 Adelie  Torgersen        3750 male    2007\n 2 Adelie  Torgersen        3800 female  2007\n 3 Adelie  Torgersen        3250 female  2007\n 4 Adelie  Torgersen          NA &lt;NA&gt;    2007\n 5 Adelie  Torgersen        3450 female  2007\n 6 Adelie  Torgersen        3650 male    2007\n 7 Adelie  Torgersen        3625 female  2007\n 8 Adelie  Torgersen        4675 male    2007\n 9 Adelie  Torgersen        3475 &lt;NA&gt;    2007\n10 Adelie  Torgersen        4250 &lt;NA&gt;    2007\n# ℹ 334 more rows\n\n\nHere, we dropped three columns.\nThe select operator provides more advanced functionality which is useful for very complex data structures, but we don’t need to dive into that just yet.\n\n\nSubsetting Rows\nOften in data analysis, we want to focus on a subset of the entire population: e.g., we might want to know the fraction of women supporting a certain political candidate or the rate of a rare cancer among patients 65 or older. In this case, we need to select only those rows of our data that match some criterion. This brings us to the filter operator.\nfilter takes a logical vector and uses it to select rows of a data frame. Most commonly, this logical vector is created by performing some sort of tests on the values in the data frame. For example, if we want to select only the male penguins in our data set, we may write:\n\npenguins |&gt; filter(sex == \"male\")\n\n# A tibble: 168 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.3          20.6               190        3650\n 3 Adelie  Torgersen           39.2          19.6               195        4675\n 4 Adelie  Torgersen           38.6          21.2               191        3800\n 5 Adelie  Torgersen           34.6          21.1               198        4400\n 6 Adelie  Torgersen           42.5          20.7               197        4500\n 7 Adelie  Torgersen           46            21.5               194        4200\n 8 Adelie  Biscoe              37.7          18.7               180        3600\n 9 Adelie  Biscoe              38.2          18.1               185        3950\n10 Adelie  Biscoe              38.8          17.2               180        3800\n# ℹ 158 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nHere, note the use of the == operator for testing equality. A very common mistake is to use the single equals (assignment) operator inside of filter. Thankfully, dplyr will alert us with an error if we make this mistake:\n\npenguins |&gt; filter(sex = \"male\")\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `sex == \"male\"`?\n\n\nIf we supply multiple tests to filter, we get the intersection: that is, we get rows that pass all tests.\n\npenguins |&gt; filter(sex == \"male\", bill_length_mm &gt; 38)\n\n# A tibble: 156 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.3          20.6               190        3650\n 3 Adelie  Torgersen           39.2          19.6               195        4675\n 4 Adelie  Torgersen           38.6          21.2               191        3800\n 5 Adelie  Torgersen           42.5          20.7               197        4500\n 6 Adelie  Torgersen           46            21.5               194        4200\n 7 Adelie  Biscoe              38.2          18.1               185        3950\n 8 Adelie  Biscoe              38.8          17.2               180        3800\n 9 Adelie  Biscoe              40.6          18.6               183        3550\n10 Adelie  Biscoe              40.5          18.9               180        3950\n# ℹ 146 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIf we want the union-rows that satisfy any of the tests-we have to use the logical operators we previous applied to vectors:\n\npenguins |&gt; filter( (sex == \"male\") | (bill_length_mm &gt; 38))\n\n# A tibble: 292 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           39.3          20.6               190        3650\n 5 Adelie  Torgersen           38.9          17.8               181        3625\n 6 Adelie  Torgersen           39.2          19.6               195        4675\n 7 Adelie  Torgersen           42            20.2               190        4250\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 282 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIt’s a bit clunky, but thankfully, this is somewhat less common than checking that all conditions are satisfied.\ndplyr provides all sorts of useful helpers for creating test statements, e.g., the\n\nbetween\nnear\n\nfunctions.\nEven more useful than these, however, are the slice_*() functions which can be used to perform “top \\(k\\)” type operations. If we want the five largest Adelie penguins, we might try something like:\n\npenguins |&gt; filter(species == \"Adelie\") |&gt; slice_max(body_mass_g, n=5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Biscoe              43.2          19                 197        4775\n2 Adelie  Biscoe              41            20                 203        4725\n3 Adelie  Torgersen           42.9          17.6               196        4700\n4 Adelie  Torgersen           39.2          19.6               195        4675\n5 Adelie  Dream               39.8          19.1               184        4650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nslice_min() works similarly. slice_head and slice_tail get the first and last rows by sort order - in general, I recommend against using these. A key rule of data analysis is that the row order is not semantically meaningful, but it’s good to keep them in the back of your mind just in case. slice_sample can be used to select random subsets of data.\nAnother important subseting function is drop_na which will drop any rows with missing (NA) data.2 This is a good and useful tool, but before you apply it, always ask yourself “why is this data missing?” Understanding the abscence of data is often just as important as understanding the non-missing data. For example, is a student’s SAT score missing on a college application because they i) forgot to list it; ii) never took the SAT; or iii) took the test but chose to omit their score because they did poorly? Proper handling of missing data is often very problem-specific and very hard."
  },
  {
    "objectID": "preassigns/pa04.html#manipulating-and-creating-columns",
    "href": "preassigns/pa04.html#manipulating-and-creating-columns",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Manipulating and Creating Columns",
    "text": "Manipulating and Creating Columns\nA key task in data analysis is transforming data. As discussed in class, one of the guiding themes of R programming is data integrity and an important way R ensures this is by applying commands to the entire vector. In the data frame context, we apply commands to an entire column. The mutate function is dplyr’s main interface for column creation and manipulation.\nIn general, each argument to mutate takes a name = value pair: the name is the name of a column to be created from value. value can be an arbitrary function of other columns. If name corresponds to an existing column, that column is silently overwritten.\nFor example, if we want to convert penguin bill lengths from millimeters to inches, we might operator:\n\npenguins |&gt; mutate(bill_length_in = bill_length_mm / 25.4)\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_length_in &lt;dbl&gt;\n\n\nHere, note that the bill_length_mm column is retained - and all columns are kept! mutate only creates new columns; it won’t secretly drop them.\nA particularly common operator is changing the name of a column without changing its values. You can use mutate and select(-) for this, but rename provides essentially the same interface and its semantically clearer:\n\npenguins |&gt; rename(mass = body_mass_g)\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm  mass sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt; &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen           39.1          18.7               181  3750 male  \n 2 Adelie  Torgersen           39.5          17.4               186  3800 female\n 3 Adelie  Torgersen           40.3          18                 195  3250 female\n 4 Adelie  Torgersen           NA            NA                  NA    NA &lt;NA&gt;  \n 5 Adelie  Torgersen           36.7          19.3               193  3450 female\n 6 Adelie  Torgersen           39.3          20.6               190  3650 male  \n 7 Adelie  Torgersen           38.9          17.8               181  3625 female\n 8 Adelie  Torgersen           39.2          19.6               195  4675 male  \n 9 Adelie  Torgersen           34.1          18.1               193  3475 &lt;NA&gt;  \n10 Adelie  Torgersen           42            20.2               190  4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;"
  },
  {
    "objectID": "preassigns/pa04.html#operating-with-group-structure",
    "href": "preassigns/pa04.html#operating-with-group-structure",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Operating with Group Structure",
    "text": "Operating with Group Structure\nWe often want to summarize our data in useful ways: these can be simple summaries like mean (average) or standard deviation or more complex operations (trend lines). In the world of dplyr, the summarize function is used whenever we want to reduce multiple rows to a single data point. Note how this differs from filter: filter drops rows, summarize combines and reduces them.\nFor example, if we want to get the number of male penguins in our data set, we can use the n() function, which counts the number of rows:\n\npenguins |&gt; \n    filter(sex == \"male\") |&gt;\n    summarize(number = n())\n\n# A tibble: 1 × 1\n  number\n   &lt;int&gt;\n1    168\n\n\nLook at all those male penguins!\nThis is a relatively simple operation, but we can be a bit more complex: e.g., with the mean function:\n\npenguins |&gt; summarize(body_mass_avg = mean(body_mass_g))\n\n# A tibble: 1 × 1\n  body_mass_avg\n          &lt;dbl&gt;\n1            NA\n\n\nWait! Why didn’t that work?\nRecall that the penguins data had some missing (NA) values. When we ask R to compute the average, it can’t! Specifically, depending on the missing values, the mean could be anything, so R returns a missing (unknown) value for the mean as well. Many base R functions have this default behavior and have an optional flag for automatically removing NA values:\n\npenguins |&gt; summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  body_mass_avg\n          &lt;dbl&gt;\n1         4202.\n\n\nHere na.rm=TRUE means to remove all na values before computing the mean.\nThe summarize function is particularly powerful when applied groupwise: e.g., what is the average body mass by species? In dplyr world, this is a two-step operation:\n\npenguins |&gt; \n    group_by(species) |&gt;\n    summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE))\n\n# A tibble: 3 × 2\n  species   body_mass_avg\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Adelie            3701.\n2 Chinstrap         3733.\n3 Gentoo            5076.\n\n\nWe added the group_by operator here. Note that, on its own, group_by doesn’t really do anything:\n\npenguins |&gt; \n    group_by(species)\n\n# A tibble: 344 × 8\n# Groups:   species [3]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nWe have added some grouping metadata but the actual data does not get changed until the summarize step.\nWe can also group by more than one element:\n\npenguins |&gt; \n    group_by(species, sex) |&gt;\n    summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex    body_mass_avg\n  &lt;fct&gt;     &lt;fct&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Adelie    &lt;NA&gt;           3540 \n4 Chinstrap female         3527.\n5 Chinstrap male           3939.\n6 Gentoo    female         4680.\n7 Gentoo    male           5485.\n8 Gentoo    &lt;NA&gt;           4588.\n\n\nNote here that the result is still grouped and that only the last (sex) grouping was removed. That means that any future operations will be automatically grouped by species. If you want to remove all grouping structure, add the ungroup operator at the end:\n\npenguins |&gt; \n    group_by(species, sex) |&gt;\n    summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE)) |&gt;\n    ungroup()\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n  species   sex    body_mass_avg\n  &lt;fct&gt;     &lt;fct&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Adelie    &lt;NA&gt;           3540 \n4 Chinstrap female         3527.\n5 Chinstrap male           3939.\n6 Gentoo    female         4680.\n7 Gentoo    male           5485.\n8 Gentoo    &lt;NA&gt;           4588.\n\n\ngroup_by metadata is also useful when summary statistics are computed implicitly by other functions. E.g., if we want to get all penguins that are above average mass for their species, we might try the following:\n\npenguins |&gt; \n    group_by(species) |&gt;\n    filter(body_mass_g &gt;= mean(body_mass_g, na.rm=TRUE)) |&gt;\n    ungroup()\n\n# A tibble: 159 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           39.2          19.6               195        4675\n 4 Adelie  Torgersen           42            20.2               190        4250\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           42.5          20.7               197        4500\n 8 Adelie  Torgersen           46            21.5               194        4200\n 9 Adelie  Biscoe              35.9          19.2               189        3800\n10 Adelie  Biscoe              38.2          18.1               185        3950\n# ℹ 149 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nHere, R applies the summarization function mean groupwise for us.\nBefore we close out, let’s put this all together: what species has the largest difference in average body mass between the sexes?\nAnswer: Gentoo penguins have the largest sex difference in average body mass.\nBefore completing the Brightspace submission for this assignment, look up the source for this document on my GitHub (Hint: see the buttons on the sidebar) and see i) how I computed the answer; and ii) how I included it in the rendered text. This will be helpful as you begin preparing your first report."
  },
  {
    "objectID": "preassigns/pa04.html#footnotes",
    "href": "preassigns/pa04.html#footnotes",
    "title": "STA 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRecall a factor is a vector with a fixed set of possible values, often used to represent categorical data. Here, we follow the NY DMV and allow M, F, and X values for sex, but, in general, representation of sex and gender in databases is a tricky problem. See this essay for a list of some of the complexity of real people. (This essay follows in a longer tradition of “the world is much more complicated than you would believe” essays: names, time, addresses. People - and the world we create - are infinitely complex.↩︎\nWe will say much more about R’s missing data model in class this week.↩︎"
  },
  {
    "objectID": "preassigns/pa05.html",
    "href": "preassigns/pa05.html",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "",
    "text": "Due Date: 2025-02-26 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nLast week, we considered single-table verbs: these are appropriate for asking complex questions of a nicely formatted data frame. Sadly, we are rarely provided data frames suitable for every question we might seek to answer. Instead, we typically need to combine information from multiple sources. For instance, if we want to examine the relationship between demographics and electoral results, we will need to combine information from the US Census Bureau and local elections administrators. Or, if we want to investigate the relationship between a company’s financial status and its stock performance, we might need to combine information from multiple databases."
  },
  {
    "objectID": "preassigns/pa05.html#basic-joins",
    "href": "preassigns/pa05.html#basic-joins",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Basic Joins",
    "text": "Basic Joins\nThe basic operator of combining different tables is the join, patterned after SQL. Each join operates using some notion of “match” between tables. In the best case, this is done using a unique identifier - one universal and consistent name for each entity. Sadly, such perfect identifiers rarely exist. For instance, companies change their names and their ticker symbols somewhat regularly (e.g., Facebook becoming Meta)\nThe simplest join is the inner_join, which returns rows which match between the two tables:\n\nlibrary(dplyr)\nband_members\n\n# A tibble: 3 × 2\n  name  band   \n  &lt;chr&gt; &lt;chr&gt;  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\n\n\nband_instruments\n\n# A tibble: 3 × 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\n\nIn this case, the name column forms a unique ID, so we can use it for our join.\n\ninner_join(band_members, band_instruments)\n\nJoining with `by = join_by(name)`\n\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nWe see here that R automatically performed the join using the common column (name): if we want to be clearer, let’s specify the join element ourselves:\n\ninner_join(band_members, band_instruments, join_by(name))\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nHere join_by is a helper function that can be used to specify the form of the join used. In some contexts, the “common” column has different names in the two tables, so we can use a more explicit call to join_by:\n\nband_instruments2\n\n# A tibble: 3 × 2\n  artist plays \n  &lt;chr&gt;  &lt;chr&gt; \n1 John   guitar\n2 Paul   bass  \n3 Keith  guitar\n\n\nNote that this is the same as band_instruments, but with the name column changed to artist.\n\ninner_join(band_members, band_instruments2, join_by(name == artist))\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nI like to always use this final - most explicit - form, even when the column names are the same between the two tables (join_by(name == name)).\nLet’s look more closely at the result here: we return a table with 3 columns and two rows:\n\ninner_join(band_members, band_instruments, join_by(name == name))\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nThe three columns are pretty easy to understand:\n\nname is the (shared) name column from each table\nband comes from the band_members table\nplays comes from the band_instruments table.\n\nThe two rows are a bit trickier: each of our input tables had three rows, but there were only two “overlaps” so that’s what we get back from an inner_join. Specifically, we drop Mick [Jagger] from band_members because he doesn’t appear in band_instrumentsand we drop Keith [Richards] from band_instruments because he doesn’t appear in band_members.\nIn brief, an inner join is an intersection join. We only get rows back which have a match in both tables.\nOther join operators have complimentary behaviors: the full join (also sometimes called an outer join) is basically a union join. We get back all rows from both tables, regardless of whether a match has been found. But what happens with those unmatched rows?\n\nfull_join(band_members, band_instruments, join_by(name == name))\n\n# A tibble: 4 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n\nR fills in the “blanks” with NA values. Here, we can assume Mick [Jagger] plays an instrument, but it is unknown to R here.\nFinally, we have the intermediate left join, which keeps all rows from one table whether or not they have a match:\n\nleft_join(band_members, band_instruments, join_by(name == name))\n\n# A tibble: 3 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\n\nHere we keep Mick because he is in band_members, even though he is missing from band_instruments. Conversely, we drop Keith because he isn’t in band_members (even though he is in band_instruments).\nR also provides a right_join, but it’s not really different: it’s just a “flipped” left_join: left_join(x, y) == right_join(y, x).\nThe following image1 summarizes the different types of joins:\n\nThe anti_join returns elements that appear in one data set, but not the other. It’s rarer, but occasionally useful."
  },
  {
    "objectID": "preassigns/pa05.html#joins-with-repeats",
    "href": "preassigns/pa05.html#joins-with-repeats",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Joins with Repeats",
    "text": "Joins with Repeats\nIn the previous examples, we have seen joins that have a “one-to-one” (inner) or possibly “one-to-none” (full, left) structure. In many circumstances, we find ourselves with a “one-to-many” type structure, even when both data sets are “tidy”. This typically occurs because different data sets have different models of what a “unit” is. For example, consider a hypothetical instructor who has i) a table with student names and contact information; and ii) a table with grades on different assignments.\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 3\n)\n\ngrades &lt;- tribble(\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    3,           \"B\",            50,\n    1,           \"C\",            95,\n    2,           \"C\",            50,\n    3,           \"C\",            80\n)\n\nWhat happens if we join these?\n\ninner_join(students, grades, join_by(id == student_id))\n\n# A tibble: 9 × 5\n  name     email                id assignment_id grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1 A               100\n2 Bernard  bernard@cuny.edu      1 B                95\n3 Bernard  bernard@cuny.edu      1 C                95\n4 Hunter   hunter@cuny.edu       2 A                95\n5 Hunter   hunter@cuny.edu       2 B                80\n6 Hunter   hunter@cuny.edu       2 C                50\n7 John Jay john.jay@cuny.edu     3 A                80\n8 John Jay john.jay@cuny.edu     3 B                50\n9 John Jay john.jay@cuny.edu     3 C                80\n\n\n(Note here that we need the explicit join_by since the column names don’t match between the two tables: id in students gets joined to student_id. This pattern of id in table tbl getting joined to tbl_id elsewhere is quite common in database design.)\nWe get repeats of the student rows: for each valid student-grade pair, we have a row. From here, we can compute final grades:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    group_by(name, email, id) |&gt;\n    summarize(final_avg = mean(grade)) |&gt;\n    mutate(final_grade = \n               case_when(final_avg &gt; 90 ~ \"A\", \n                         final_avg &gt; 80 ~ \"B\", \n                         final_avg &gt; 70 ~ \"C\", \n                         final_avg &gt; 60 ~ \"D\", \n                         TRUE ~ \"F\")) # In a case_when, TRUE == \"else\"\n\n`summarise()` has grouped output by 'name', 'email'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 5\n# Groups:   name, email [3]\n  name     email                id final_avg final_grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n1 Bernard  bernard@cuny.edu      1      96.7 A          \n2 Hunter   hunter@cuny.edu       2      75   C          \n3 John Jay john.jay@cuny.edu     3      70   D          \n\n\nIn this case, everything works well. But let’s try a slightly trickier case, with some students who never fail to submit certain assignments.\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 3\n)\n\ngrades &lt;- tribble(\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    1,           \"C\",            95,\n    3,           \"C\",            80\n)\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    group_by(name, email, id) |&gt;\n    summarize(final_avg = mean(grade)) |&gt;\n    mutate(final_grade = \n               case_when(final_avg &gt; 90 ~ \"A\", \n                         final_avg &gt; 80 ~ \"B\", \n                         final_avg &gt; 70 ~ \"C\", \n                         final_avg &gt; 60 ~ \"D\", \n                         TRUE ~ \"F\"))\n\n`summarise()` has grouped output by 'name', 'email'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 5\n# Groups:   name, email [3]\n  name     email                id final_avg final_grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n1 Bernard  bernard@cuny.edu      1      96.7 A          \n2 Hunter   hunter@cuny.edu       2      87.5 B          \n3 John Jay john.jay@cuny.edu     3      80   C          \n\n\nWhy did the final grades go up after we deleted rows?\n\ninner_join(students, \n           grades, \n           join_by(id == student_id))\n\n# A tibble: 7 × 5\n  name     email                id assignment_id grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1 A               100\n2 Bernard  bernard@cuny.edu      1 B                95\n3 Bernard  bernard@cuny.edu      1 C                95\n4 Hunter   hunter@cuny.edu       2 A                95\n5 Hunter   hunter@cuny.edu       2 B                80\n6 John Jay john.jay@cuny.edu     3 A                80\n7 John Jay john.jay@cuny.edu     3 C                80\n\n\nThe “missing” assignments for Hunter and John Jay aren’t reported as zeros - they are just ignored! And hence R takes an average over the two assignments where these students did well, not all three assignments. We’ll talk about one way to fix this below, but for now I’m just flagging it as a possible issue that can come up with missing data and joins. (Here the rows were missing, so it’s harder to catch than a plain NA; better data management would have included a “0” row instead of deleting them, but we don’t always get to assume super well-organized data.)\nSo far, our join results have been relatively straightforward because we have had ‘good’ unique identifiers. If we find ourselves in a situation where we lack unique IDs, things can go wrong quickly:\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 2  # Accidentally repeat an ID\n)\ngrades &lt;- tribble( # Back to the complete data\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    3,           \"B\",            50,\n    1,           \"C\",            95,\n    2,           \"C\",            50,\n    3,           \"C\",            80\n)\n\nfull_join(students, \n          grades, \n          join_by(id == student_id))\n\nWarning in full_join(students, grades, join_by(id == student_id)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 2 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 12 × 5\n   name     email                id assignment_id grade\n   &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 Bernard  bernard@cuny.edu      1 A               100\n 2 Bernard  bernard@cuny.edu      1 B                95\n 3 Bernard  bernard@cuny.edu      1 C                95\n 4 Hunter   hunter@cuny.edu       2 A                95\n 5 Hunter   hunter@cuny.edu       2 B                80\n 6 Hunter   hunter@cuny.edu       2 C                50\n 7 John Jay john.jay@cuny.edu     2 A                95\n 8 John Jay john.jay@cuny.edu     2 B                80\n 9 John Jay john.jay@cuny.edu     2 C                50\n10 &lt;NA&gt;     &lt;NA&gt;                  3 A                80\n11 &lt;NA&gt;     &lt;NA&gt;                  3 B                50\n12 &lt;NA&gt;     &lt;NA&gt;                  3 C                80\n\n\nIn this case, R is kind enough to warn us that a “many-to-many” join has happened (joining multiple students to one grade and multiple grades to one student). This is a very good warning and it highlights a true error here. If faced with data like this, you may not be able to address it with fixing the underlying data, but at least you know something has gone awry."
  },
  {
    "objectID": "preassigns/pa05.html#compound-joins",
    "href": "preassigns/pa05.html#compound-joins",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Compound Joins",
    "text": "Compound Joins\nOften, data lack a unique identifier, but you can piece one together with several columns: that is, taken on its own, no column is unique, but the tuples formed by comining several columns are unique, e.g., data with year, month, and day columns.\n\nrevenues &lt;- tribble(\n    ~year, ~month, ~day, ~revenue,\n    2024,  09,     22,   100,\n    2024,  09,     23,   200,\n    2024,  10,     22,   200,\n    2024,  10,     22,   200,\n    2025,  09,     22,   500\n    )\n\nexpenses &lt;- tribble(\n    ~year, ~month, ~day, ~expenses,\n    2024,  09,     22,   -200,\n    2024,  09,     23,   -200,\n    2024,  10,     22,   -200,\n    2024,  10,     23,   -200,\n    2025,  09,     22,   -300\n    )\n\nIn this case, a simple join on any one column goes astray:\n\ninner_join(revenues, expenses, join_by(day == day))\n\nWarning in inner_join(revenues, expenses, join_by(day == day)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 14 × 7\n   year.x month.x   day revenue year.y month.y expenses\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1   2024       9    22     100   2024       9     -200\n 2   2024       9    22     100   2024      10     -200\n 3   2024       9    22     100   2025       9     -300\n 4   2024       9    23     200   2024       9     -200\n 5   2024       9    23     200   2024      10     -200\n 6   2024      10    22     200   2024       9     -200\n 7   2024      10    22     200   2024      10     -200\n 8   2024      10    22     200   2025       9     -300\n 9   2024      10    22     200   2024       9     -200\n10   2024      10    22     200   2024      10     -200\n11   2024      10    22     200   2025       9     -300\n12   2025       9    22     500   2024       9     -200\n13   2025       9    22     500   2024      10     -200\n14   2025       9    22     500   2025       9     -300\n\n\nNote the warning!\nIn this scenario, we should really “tidy” up the data by combining the date information, which is spread across three columns, into a single column, but we have the alternative option of a compound join:\n\ninner_join(revenues, expenses, \n           join_by(day == day, \n                   month == month, \n                   year == year))\n\n# A tibble: 5 × 5\n   year month   day revenue expenses\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1  2024     9    22     100     -200\n2  2024     9    23     200     -200\n3  2024    10    22     200     -200\n4  2024    10    22     200     -200\n5  2025     9    22     500     -300\n\n\nHere, as with filter, the list of conditions looks for an intersection: we want all three parts of the date to match."
  },
  {
    "objectID": "preassigns/pa05.html#pivots",
    "href": "preassigns/pa05.html#pivots",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Pivots",
    "text": "Pivots\nFinally, we may want to re-arrange the output of a join. Returning to our grades example from above:\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 3\n)\n\ngrades &lt;- tribble(\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    3,           \"B\",            50,\n    1,           \"C\",            95,\n    2,           \"C\",            50,\n    3,           \"C\",            80\n)\n\ngrade_book &lt;- inner_join(students, \n                         grades, \n                         join_by(id == student_id))\n\ngrade_book\n\n# A tibble: 9 × 5\n  name     email                id assignment_id grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1 A               100\n2 Bernard  bernard@cuny.edu      1 B                95\n3 Bernard  bernard@cuny.edu      1 C                95\n4 Hunter   hunter@cuny.edu       2 A                95\n5 Hunter   hunter@cuny.edu       2 B                80\n6 Hunter   hunter@cuny.edu       2 C                50\n7 John Jay john.jay@cuny.edu     3 A                80\n8 John Jay john.jay@cuny.edu     3 B                50\n9 John Jay john.jay@cuny.edu     3 C                80\n\n\nThis isn’t really how we like to see gradebooks: a “wider” format, with a column for each assignment, may be more preferable. In this case, we want to use the pivot_wider column from the tidyr package.\npivot_wider takes a few key arguments:\n\nid_cols which columns that (taken together) uniquely identify a row in the final table\nnames_from: where should we get the column names of the new table\nvalues_from: where should we get the values of the new table\n\nThis is maybe easier by example:\n\ngrade_book |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from=assignment_id,\n                values_from=grade)\n\n# A tibble: 3 × 6\n  name     email                id     A     B     C\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95\n2 Hunter   hunter@cuny.edu       2    95    80    50\n3 John Jay john.jay@cuny.edu     3    80    50    80\n\n\nThis pivot trick is particularly useful for finding missing rows, like those that tripped us up earlier:\n\ngrades &lt;- tribble( # Implicit missing values\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    1,           \"C\",            95,\n    3,           \"C\",            80\n)\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade)\n\n# A tibble: 3 × 6\n  name     email                id     A     B     C\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95\n2 Hunter   hunter@cuny.edu       2    95    80    NA\n3 John Jay john.jay@cuny.edu     3    80    NA    80\n\n\nHere, our missing values are now explicit!\nWe can also explicitly fill the NA with a value of our choice, here 0:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0)\n\n# A tibble: 3 × 6\n  name     email                id     A     B     C\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95\n2 Hunter   hunter@cuny.edu       2    95    80     0\n3 John Jay john.jay@cuny.edu     3    80     0    80\n\n\nThere is also an inverse operator pivot_longer which takes a wide table (like this) and makes it longer.\nTo complete our grade book example, we might want to take the average across the three grade columns:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0) |&gt;\n    group_by(name) |&gt;\n    mutate(final_avg = mean(c_across(A:C)))\n\n# A tibble: 3 × 7\n# Groups:   name [3]\n  name     email                id     A     B     C final_avg\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95      96.7\n2 Hunter   hunter@cuny.edu       2    95    80     0      58.3\n3 John Jay john.jay@cuny.edu     3    80     0    80      53.3\n\n\nNote here that we need to use a mutate since our final grade book has the same number of rows before and after we add the final average column. The c_across column here is a variant of the standard c function used to combine scalars: here we’re creating a new length-3 vector of the student’s three grades and passing it to the mean function.\nWhat is group_by(name) doing here? See what happens without it:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0) |&gt;\n    mutate(final_avg = mean(c_across(A:C)))\n\n# A tibble: 3 × 7\n  name     email                id     A     B     C final_avg\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95      69.4\n2 Hunter   hunter@cuny.edu       2    95    80     0      69.4\n3 John Jay john.jay@cuny.edu     3    80     0    80      69.4\n\n\nRecall that mean is a summarization function - it will combine data from across rows if no grouping structure is present. Since we want seperate averages for each student, we need a group_by. In this case, name is a unique identifier for each student, so we can group on it. We also have the rowwise() helper, which automatically creates group structure with each group a separate row. If you don’t have a clean unique identifier, or just can’t think of one easily, this is sometimes a useful helper.\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0) |&gt;\n    rowwise() |&gt;\n    mutate(final_avg = mean(c_across(A:C)))\n\n# A tibble: 3 × 7\n# Rowwise: \n  name     email                id     A     B     C final_avg\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95      96.7\n2 Hunter   hunter@cuny.edu       2    95    80     0      58.3\n3 John Jay john.jay@cuny.edu     3    80     0    80      53.3\n\n\nIn class, we will explore joins in more detail by combining the flights data with plane, airport, and weather factors.\nPlease now go fill out the weekly quiz on Brightspace."
  },
  {
    "objectID": "preassigns/pa05.html#footnotes",
    "href": "preassigns/pa05.html#footnotes",
    "title": "STA 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAdapted from Data Carpentry↩︎"
  },
  {
    "objectID": "preassigns/pa03.html",
    "href": "preassigns/pa03.html",
    "title": "STA 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "",
    "text": "Due Date: 2025-02-12 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nIt is now time for us to start programming in R properly. In this week’s pre-assignment, we’re going to focus on three basic elements of programming in R:\nBefore we get into these however, let’s introduce the feedback mechanism used throughout this pre-assignment. Throughout this page, you will encounter blocks like the below:\nWait for the exercise to fully load (the blue dot next to Run Code will disappear) and then try giving correct and incorrect solutions.\nThese little code-blocks throughout this pre-assignment will be used to give similar feedback. You can always hit Show Solution to get the correct answer. The feedback engine is a bit overly picky at times, so if your answer is substantially similar to the official solution, I wouldn’t worry too much.\nYou will see the R output sometimes has a [1] before it. Don’t worry about that until you get to the section on vectorized semantics below.\nBlocks that aren’t listed as “Exercise” are interactive snippets. Feel free to adjust the code to check your understanding."
  },
  {
    "objectID": "preassigns/pa03.html#calculator-math-with-r",
    "href": "preassigns/pa03.html#calculator-math-with-r",
    "title": "STA 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "Calculator Math with R",
    "text": "Calculator Math with R\nLet’s start by using R as a calculator. R implements all the basic operations of arithmetic:\n\na + b: (binary) addition \\(a + b\\)\na - b: (binary) subtraction \\(a - b\\)\n*: (binary) multiplication \\(ab\\)\n/: (binary) division \\(a/b\\)\n-b: (unary) negation \\(-b\\)\n\nYou can type integers and decimals in the usual manner:\n\n\n\n\n\n\n\n\nCompute \\(5! = 5 * 4 * 3 * 2 * 1\\) using R:\n\n\n\n\n\n\n\n\n\nThe blanks should be filled with 3 and 1\n\n\n5 * 4 * 3 * 2 * 1\n5 * 4 * 3 * 2 * 1\n\n\n\n\n\n\nExponentials (powers) can be implemented with either a double star ** or a carrot ^:\n\n\n\n\n\n\n\n\nI tend to prefer the carrot ^ as its one fewer character.\nIn general, R respects the standard “PEMDAS” order of operations:\n\nParentheses\nExponentiation\nMultiplication and Division\nAddition and Subtraction\n\nSo we can compute \\(3 * (2 + 1)\\) as:\n\n\n\n\n\n\n\n\n\nExercises\nCompute the following algebraic expressions using R:\n\n\\[3 * 2^2\\]\n\n\n\n\n\n\n\n\n\n\n\n\n3 * 2^2\n3 * 2^2\n\n\n\n\n\n\n\n\\[(3 * 2)^2\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(3 * 2)^2\n(3 * 2)^2\n\n\n\n\n\n\n\n\\[3 + 2 - 1 + 4\\]\n\n\n\n\n\n\n\n\n\n\n\n\n3 + 2 - 1 + 4\n3 + 2 - 1 + 4\n\n\n\n\n\n\n\n\\[3 + 2 - (1 + 4)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n3 + 2 - (1 + 4)\n3 + 2 - (1 + 4)\n\n\n\n\n\n\n\nExecution in RStudio\nNow, redo these exercises in the RStudio Console. At each step, type the relevant code next to the &gt; prompt and hit enter to execute the command.\nR has greedy execution. When you hit enter, R tries its best to execute the whole line of code. If you enter an incomplete line of code, e.g., 3 +, R will change the &gt; prompt to a + prompt, indicating there is more to be done.\nCompare\n\n\n\n\n\n\n\n\nwith\n\n\n\n\n\n\n\n\nIn the first example, 3- is not a complete mathematical statement, so R knew there had to be more code and continued to await input. In the second, 3 is a perfectly valid (if very simple) mathematical command on its own, so R simply executes it as is.\nContinuation prompts from dangling math are quite rare, but you will often find yourself in this scenario if you let parentheses become mismatched. If you are ever stuck and can’t figure out how to appease R, simply type Cntrl-C to “interrupt” the command and get back to the standard prompt."
  },
  {
    "objectID": "preassigns/pa03.html#function-calls",
    "href": "preassigns/pa03.html#function-calls",
    "title": "STA 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "Function Calls",
    "text": "Function Calls\nR comes built in with a quite robust mathematical library. You can in general call a function like this:\n\n\n\n\n\n\n\n\n(R also comes with the mathematical constant \\(\\pi\\) pre-loaded.)\nIn general a function call is a “name” immediately followed by parentheses. If a function takes input or arguments, the input is located between the parentheses, separated by commas.\nSo above, cos(pi) implements the math \\(\\cos(\\pi)\\).\nUseful built-in functions are:\n\nsin - in radians\ncos - in radians\nexp - base \\(e\\) exponential\nlog - by default this is the natural logarithm (\\(\\ln\\))\nsqrt\nabs - absolute value\nfactorial - \\(n! = n * (n - 1) * (n - 2) * \\dots * 3 * 2 * 1\\)\n\nUse the built-in functions to compute \\(5!\\):\n\n\n\n\n\n\n\n\n\n\n\nfactorial(5)\nfactorial(5)\n\n\n\n\n\n\n\nExercises\nUsing these built-in functions, compute the following arithmetic expressions:\n\n\\[\\cos^2(\\pi / 4)\\]\n\n\n\n\n\n\n\n\n\n\n\n\ncos(pi/4)^2\ncos(pi/4)^2\n\n\n\n\n\n\n\n\\[e^{\\log(\\pi) + 3}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nexp(log(pi) + 3)\nexp(log(pi) + 3)"
  },
  {
    "objectID": "preassigns/pa03.html#vectorized-semantics",
    "href": "preassigns/pa03.html#vectorized-semantics",
    "title": "STA 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "Vectorized Semantics",
    "text": "Vectorized Semantics\nA distinguishing feature of R is its vectorized semantics. By default, R wants to operate on collections of data - not individual values (scalars). You’ve seen some evidence of this already. Whenever you run a bit of math, R puts [1] at the front of the output. This is R helping you count the number of elements in the solution; it’s been pretty trivial so far, as all our calculations have returned a single number. But this is about to change!\nThe easiest vectors to create in R are sequences: e.g., the list of numbers from 1 to 10:\n\n\n\n\n\n\n\n\nHere, our output is a vector of 10 elements. R still tells us where the vector starts (at the first element) but nothing else. Change this code to create the first 100 elements and read R’s output. Do you understand the output?\nWhen R starts a new line of output, it tells you where you are in the vector. In RStudio, type letters to see the built-in vector of letters; this is a nice example of how the position information can be helpful in sorting through printed output.\nBy default, R operates on vectors elementwise:\n\n\n\n\n\n\n\n\nHere the sqrt function is applied to each element separately.\nWhen two vectors are combined, the operation also works elementwise:\n\n\n\n\n\n\n\n\n(Note that the sequence operator (:) has higher precedence than most arithmetic so this does “what you’d expect.”)\nThings get weird if the two vectors are of different lenghts:\n\n\n\n\n\n\n\n\nUnder the hood, R “recycles” 3 to be a vector of length 5 and then operators elementwise. That is, R computes\n\n3 + 1\n3 + 2\n3 + 3\n3 + 4\n3 + 5\n\nand combines the results.\nThis in general gives useful results, but the results can be quite alarming if combining vectors of unaligned size:\n\n\n\n\n\n\n\n\nThankfully, R gives a warning that something weird happened. (This might seem annoying, but warnings are great! They help you find likely errors before anything too bad happens. Most experienced programmers wish R had more built-in warnings.)\nIt’s worth distinguishing warnings from errors. Errors occur when R absolutely cannot and will not execute your command:\n\n\n\n\n\n\n\n\nIn this case, it is impossible to add the number 3 to a letter, so R throws an error.\nWarnings are hints of possible problems, but do not prevent execution. When dealing with external software and packages, you will often get warnings about old versions of software. These are encouraging you to update, but unless you see an error, things probably worked out ok.\nSome of R’s built-in functions can be used to “summarize” a vector down to a single scalar (or, more precisely, a length 1 vector). These include sum, max, min, and mean. For example, we can compute the sum of the first 100 numbers as follows:\n\n\n\n\n\n\n\n\nApocryphally, a young C.F. Gauss did this calculation in his head to the great surprise of his school teacher. We might not have Gauss’s skills at arithmetic, but we can do quite a lot with R.\nFor example, the famous “Bessel problem” of mathematics is to compute\n\\[ \\sum_{n=1}^{\\infty} \\frac{1}{n^2} = 1 + \\frac{1}{2^2} + \\frac{1}{3^2} + \\dots\\]\nEuler showed, somewhat remarkably, that the answer is \\(\\pi^2 / 6\\). We won’t repeat Euler’s analysis here, but let’s confirm it using R.\n\n\n\n\n\n\n\n\nPretty good alignment!\nDo you understand everything that happened here? If so, you’re ready for next week’s class. Go ahead and fill out this week’s Brightspace quiz."
  },
  {
    "objectID": "preassigns/pa09.html",
    "href": "preassigns/pa09.html",
    "title": "STA 9750 Week 9 Pre Assignment: Flat-File Data Ingest",
    "section": "",
    "text": "Due Date: 2025-04-02 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nWe now turn to the next “unit” of this course - acquiring data and preparing it for further analysis. In what follows, our goal will be go get data to a “tidy” format suitable for use with tidyverse tools like dplyr and ggplot2. Unfortunately, this problem becomes a rather difficult one. While every “tidy” data set shares certain characteristics, non-tidy data, by definition, won’t. Still - we are analysts and data scientists - so we soldier on and begin to work with the nasty and difficult data the real world often presents us with.\nWe are spending two weeks on this topics of “getting data into R”. Roughly, this will break into two sections:\nIn this pre-assignment, we’re focusing on the first part of the first section: reading data from a file into R. This will be relatively fast - we’ve already done some of it this semester - but it’s an essential skill set."
  },
  {
    "objectID": "preassigns/pa09.html#what-is-a-file",
    "href": "preassigns/pa09.html#what-is-a-file",
    "title": "STA 9750 Week 9 Pre Assignment: Flat-File Data Ingest",
    "section": "What is a File?",
    "text": "What is a File?\nBefore we work on getting data from files into R, it’s worth taking a moment to review what a file actually is. Modern technology often tries to hide the “true nature” of things behind ever increasing stacks of accounts and cloud services, but we’re going back to basics.\nA file is a long series of bits, with a well-defined beginning and end.\nThat’s it - files aren’t required to have extensions, formats, or anything else. Computers associated each file with a name, organized into a file system, but this all lives outside the file.\nThat general definition - lots of bits - is not particularly helpful for sharing of content and data, so files are conventionally shared with formats. A set of rules that tell programs how to interpret the bits of a file. Essentially, a file format is a social convention that exists “around” the file, enabling us to work with it productively and easily.\nA huge number of file formats exist - images, movies, text documents, financial transaction records, data bases - and you thankfully do not need to know them all. We can generally divide these into two buckets:\n\nPlain text formats. These are formats which are designed to be easily readable across a wide range of tools. This flexibility makes plain text formats particularly popular among tech-types, as users are not forced into using a piece of (typically expensive) software.[^1]\nMost of the files we have seen in this course are plain-text: qmd files, csv files, R files, etc. We have edited them primarily in RStudio, but you could just as easily access them in tools like VS Code, Eclipse, Jupyter, or even rather primitive tools like Notepad.\nBinary formats. These are typically specialized formats used to encode complex data formats. Image formats, a common example of binary formats, are necessarily more complex than anything we could represent in text. Because of this complexity, binary formats tend to require specialized software to read and edit. For popular formats, suitable software can be found on almost any machine: e.g., almost any computer you will use comes with some PDF-reading software pre-installed, even if you later choose to upgrade it to a more advanced tool.\n\nSome files blend these two elements, e.g., an HTML page with a mix of plain text and an embedded image, but it’s still a useful distinction.\nGiven all that, how do our computers actually know what to do with a file? We open and close files all day and rarely have to tell our computers what software to use or how to read the file.\nThere are two general conventions, one more popular in Unix-type systems (MacOS and Linux) and one more popular on Windows-like machines.\nMost file formats - and especially most binary formats - begin with a “magic string” that clearly states what format is used to encode the file. By reading this, the operating system can tell what type of file format is used and call the appropriate software. For instance, PDF files must start with the string %PDF - 1.4. When your computer reads this, it knows it has a found a PDF and calls the appropriate software.\nThis type of convention is powerful, though it sometimes struggles with very rare formats if the destination machine has never been programmed to handle that type of file. It has the advantage that the magic string is inside the file. If the file is transferred without corruption, it’s impossible to “misplace” the file format information.\nThe other convention used to identify file formats is the extension. A file extension is a set of a few letters following a period at the end of the file name. We often adopt these extensions into our everyday discourse, e.g., a “PDF” file or a “GIF”. It is important to note that the extension is not part of the file and that it is a second piece of information in addition to the file contents. As such, this is a second place where things can be corrupted and mistakes can be made. This isn’t a huge problem - despite our best efforts, computers are pretty reliable - but it is something to be aware of. Importantly, we can change the extension without ever editing the file itself; this is a double-edged sword.\nIn an attempt to be more user-friendly, certain operating systems - especially Microsoft Windows - are quite dogmatic about the use of file extensions, sometimes even going so far as to “hide” the extension from the user. If you’ve ever seen a file with a name like “doc.html.html”, it’s almost always because a user tried to add or change an extension, but Windows hid the “true” extension away from them. (If you’re on a Windows machine, I recommend setting Windows to always show the extensions to avoid this type of mistake.)\nExtension-based formatting is popular and, when it works well, can be quite efficient, but it’s worth re-emphasizing: the truth is in the bits, not the name. Pragmatically, the true test of whether a file is in a format is whether software designed to read that format can handle it."
  },
  {
    "objectID": "preassigns/pa09.html#so-what",
    "href": "preassigns/pa09.html#so-what",
    "title": "STA 9750 Week 9 Pre Assignment: Flat-File Data Ingest",
    "section": "So What?",
    "text": "So What?\nAt this point, you should probably be asking yourself why any of this matters. The key is to understand that a format is just a set of rules for interpreting the bits of a file. To read a file into R, we only then need to find a function that knows how to map that format into R.\nPeople love R, and many such “reader” functions exist. We have already seen\n\nreadr::read_csv\nsf::read_sf\n\nseveral times in this course. But there are many others. The readr package provides functions for reading standard “plain-text” formats, including:\n\nreadr::read_csv (“comma separated values”)\nreadr::read_tsv (“tab separated values”)\nreadr::read_delim (read files with an arbitrary delimiter, e.g., semi-colon separated values)\nreadr::read_fwf (read files where columns have a fixed width)\n\nSome of these functions are quite advanced, and can even automatically download and decompress files from web services, if you pass it a URL instead of a (local) file name.[^2]\nThe haven package provides tools for reading data formats generated by other statistical software:\n\nhaven::read_stata\nhaven::read_spss\nhaven::read_sas\n\nIn general, if you are reading a not-entirely bespoke file format into R, there is almost always a function for doing so. If you come across a format for your course project that you are struggling to read, ask the course staff."
  },
  {
    "objectID": "preassigns/pa09.html#reading-in-action",
    "href": "preassigns/pa09.html#reading-in-action",
    "title": "STA 9750 Week 9 Pre Assignment: Flat-File Data Ingest",
    "section": "Reading in Action",
    "text": "Reading in Action\nWe have already read many files in this course, and when all goes well, the process is quite seamless:\n\nlibrary(readr)\ncars &lt;- read_csv(\"https://github.com/tidyverse/readr/raw/main/inst/extdata/mtcars.csv\")\n\nRows: 32 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (11): mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf the data provider produces well-formatted files, you usually do not require any more thought than identifying the right read function (possibly from a package which you need to install and load as well) and finding the file path. Unfortunately, data providers are rarely as perfect as we might hope.\nCon-Ed, NYC’s electric utility, provides electric usage data in 15-minute intervals. My usage data for 2024-10-24 looks something like this:\n\n\n\nName,MICHAEL WEYLANDT\nAddress,ADDRESS REDACTED\nAccount Number,ACCOUNT NUMBER REDACTED\nService,Service 1\n\nTYPE,DATE,START TIME,END TIME,USAGE (kWh),NOTES\nElectric usage,2024-10-24,00:00,00:14,0.08,\nElectric usage,2024-10-24,00:15,00:29,0.08,\nElectric usage,2024-10-24,00:30,00:44,0.08,\nElectric usage,2024-10-24,00:45,00:59,0.08,\nElectric usage,2024-10-24,01:00,01:14,0.09,\nElectric usage,2024-10-24,01:15,01:29,0.08,\nElectric usage,2024-10-24,01:30,01:44,0.08,\nElectric usage,2024-10-24,01:45,01:59,0.1,\nElectric usage,2024-10-24,02:00,02:14,0.11,\nElectric usage,2024-10-24,02:15,02:29,0.11,\nElectric usage,2024-10-24,02:30,02:44,0.1,\nElectric usage,2024-10-24,02:45,02:59,0.08,\nElectric usage,2024-10-24,03:00,03:14,0.08,\n\n\nIt’s clear that this file is “csv-ish”, but is not a standard CSV file. ConEd includes a “header” of questionable utility. While we could edit this file by hand to only include the “data parts”, we can also adjust our file reading code.\nThe default read.csv function in R fails on this file:\n\nread.csv(\"mw_coned_20241024.csv\")\n\nError in read.table(file = file, header = header, sep = sep, quote = quote, : more columns than column names\n\n\nThis message is a bit opaque, but it’s essentially saying that there are more columns in the middle of the file (where the data “should” be) than there are names in the first (blank) line of the file.\nThe improved read_csv function from the readr package tries harder and seems to succeed, but it also fails:\n\nlibrary(readr)\nread_csv(\"mw_coned_20241024.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 38 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, MICHAEL WEYLANDT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 38 × 2\n   Name           `MICHAEL WEYLANDT`                        \n   &lt;chr&gt;          &lt;chr&gt;                                     \n 1 Address        ADDRESS REDACTED                          \n 2 Account Number ACCOUNT NUMBER REDACTED                   \n 3 Service        Service 1                                 \n 4 TYPE           DATE,START TIME,END TIME,USAGE (kWh),NOTES\n 5 Electric usage 2024-10-24,00:00,00:14,0.08,              \n 6 Electric usage 2024-10-24,00:15,00:29,0.08,              \n 7 Electric usage 2024-10-24,00:30,00:44,0.08,              \n 8 Electric usage 2024-10-24,00:45,00:59,0.08,              \n 9 Electric usage 2024-10-24,01:00,01:14,0.09,              \n10 Electric usage 2024-10-24,01:15,01:29,0.08,              \n# ℹ 28 more rows\n\n\nIf you look at this closely, it thinks we only have a two column file. But the data clearly has several more columns than that.\nThe warning message here is helpful: it encourages us to use the problems() function to get more information on possible issues:\n\nlibrary(readr)\ncon_ed &lt;- read_csv(\"mw_coned_20241024.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 38 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, MICHAEL WEYLANDT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproblems(con_ed)\n\n# A tibble: 35 × 5\n     row   col expected  actual    file                                         \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;                                        \n 1     5     6 2 columns 6 columns /Users/weylandt/sta9750/mw_coned_20241024.csv\n 2     6     6 2 columns 6 columns /Users/weylandt/sta9750/mw_coned_20241024.csv\n 3     7     6 2 columns 6 columns /Users/weylandt/sta9750/mw_coned_20241024.csv\n 4     8     6 2 columns 6 columns /Users/weylandt/sta9750/mw_coned_20241024.csv\n 5     9     6 2 columns 6 columns /Users/weylandt/sta9750/mw_coned_20241024.csv\n 6    10     6 2 columns 6 columns /Users/weylandt/sta9750/mw_coned_20241024.csv\n 7    11     6 2 columns 6 columns /Users/weylandt/sta9750/mw_coned_20241024.csv\n 8    12     6 2 columns 6 columns /Users/weylandt/sta9750/mw_coned_20241024.csv\n 9    13     6 2 columns 6 columns /Users/weylandt/sta9750/mw_coned_20241024.csv\n10    14     6 2 columns 6 columns /Users/weylandt/sta9750/mw_coned_20241024.csv\n# ℹ 25 more rows\n\n\nWe see here that R expected only two columns, based on the top section of the file, but every row of “substance” beyond the ConEd-special header actually has six columns. Note that R is giving us a warning here - it could conceivably continue, here by smashing together the extra columns - so it does so. This type of warning is not uncommon in reading malformatted data. It is your responsibility as an analyst to investigate the warnings and see if they are true signs of trouble or false positives. A particularly common warning, which you have already encountered in this course, is when data sets use a non-standard way to encode NA values.\nIn this case, there’s actually an easy fix - we just want to skip those first rows for the header:\n\nlibrary(readr)\ncon_ed &lt;- read_csv(\"mw_coned_20241024.csv\", skip = 6)\n\nRows: 34 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): TYPE\ndbl  (1): USAGE (kWh)\nlgl  (1): NOTES\ndate (1): DATE\ntime (2): START TIME, END TIME\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe see lots of useful information in this read out. Our data has\n\n34 rows\n6 columns\nComma delimited fields\nThe columns types are:\n\nCharacter (1)\nDouble / Numeric (1)\nLogical (1)\nDate (1)\nTime (2)\n\n\nHere the logical column (NOTES) is a bit of a red herring; it’s all empty for this file, so R interprets as the simplest data type that fits,\nFrom here, I’m going to turn off printing of the data import message to keep this file readable, but you should always at least quickly eyeball them when importing a new data set.\nAt this point, we’re ready to proceed with tidying up our data. The USAGE (kWh) column is clearly the most important to us, but R doesn’t like column names with spaces and punctuation, so let’s go ahead and manually rename it:\n\nlibrary(readr); library(dplyr)\ncon_ed &lt;- read_csv(\"mw_coned_20241024.csv\", skip = 6) |&gt;\n    rename(usage = `USAGE (kWh)`)\n\nRecall that we have to use double back ticks to surround “weird” names. We can also drop the unused NOTES column and clean up the start and end time columns as well.\n\nlibrary(readr); library(dplyr)\ncon_ed &lt;- read_csv(\"mw_coned_20241024.csv\", skip = 6) |&gt;\n    rename(usage = `USAGE (kWh)`, \n           start_time = `START TIME`,\n           end_time = `END TIME`) |&gt;\n    select(-NOTES)\n\nWe now have nice tidy data that we can use to make a plot:\n\nlibrary(readr); library(dplyr); library(ggplot2)\nread_csv(\"mw_coned_20241024.csv\", skip = 6) |&gt;\n    rename(usage = `USAGE (kWh)`, \n           start_time = `START TIME`,\n           end_time = `END TIME`) |&gt;\n    select(-NOTES) |&gt;\n    ggplot(aes(x=start_time, y=usage)) + \n    geom_line()\n\n\n\n\n\n\n\n\nCan you see i) when my phone finished charting overnight; and ii) when I started to make breakfast on my electric stovetop?\n\n\n\n\n\n\nConEd Practice\n\n\n\nIf you have your own ConEd account, download your Energy Usage data file by hand and import it into R. Can you find patterns of your daily life in this data?\n\n\nAfter finishing this document, complete the Weekly Pre-Assignment Quiz on Brightspace."
  },
  {
    "objectID": "miniprojects/mini01.html",
    "href": "miniprojects/mini01.html",
    "title": "STA 9750 Mini-Project #01: Welcome to the Commission to Analyze Taxpayer Spending (CATS)",
    "section": "",
    "text": "Released to Students: 2025-02-13\nInitial Submission: 2025-03-05 11:45pm ET on GitHub and Brightspace\n\nPeer Feedback:\n\nPeer Feedback Assigned: 2025-03-06 on GitHub\nPeer Feedback Due: 2025-03-12 11:45pm ET on GitHub\n\n\n\nEstimated Time to Complete: 9 Hours\nEstimated Time for Peer Feedback: 1 Hour"
  },
  {
    "objectID": "miniprojects/mini01.html#welcome-to-mini-projects",
    "href": "miniprojects/mini01.html#welcome-to-mini-projects",
    "title": "STA 9750 Mini-Project #01: Welcome to the Commission to Analyze Taxpayer Spending (CATS)",
    "section": "Welcome to STA 9750 Mini Projects!",
    "text": "Welcome to STA 9750 Mini Projects!\nIn the STA 9750 Mini-Projects, you will perform basic data analyses intended to model best practices for your course final project. (Note, however, that these are mini-projects; your final course project is expected to be far more extensive than any single MP.)\nFor purposes of MPs, we are dividing the basic data analytic workflow into several major stages:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\nIn this course, our primary focus is on the first four stages: you will take other courses that develop analytical and modeling techniques for a variety of data types. As we progress through the course, you will eventually be responsible for the first four steps. Specifically, you are responsible for the following stages of each mini-project:\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn early stages of the course, such as this MP, I will ‘scaffold’ much of the analysis for you, leaving only those stages we have discussed in class for you to fill in. As the course progresses, the mini-projects will be more self-directed and results less standardized.\nRubric\nSTA 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nLess than half of the instructor-provided tasks were successfully completed.\nReport exhibits particularly creative insights beyond instructor specifications.\n\n\nFormatting & Display\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation. Table has a suitable caption.\nColumn names and digits are well-chosen, but formatting could be improved.\nBad column names (opaque variable names or other undefined acronyms)\nUnfiltered ‘data dump’ instead of curated table.\nNo tables.\nReport includes one or more high-quality graphics (created using R).\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nAutomatic (10/10). Out of scope for this mini-project\n\n\n\n\nReport modifies instructor-provided import code to use additional columns or data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp01.qmd so the rendered document can be found at docs/mp01.html in the student’s repository and served at the URL:1\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp01.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/michaelweylandt/STA9750-2025-SPRING/issues/new .\n\nTitle the issue STA 9750 &lt;GITHUB_ID&gt; MiniProject #01 and fill in the following text for the issue:\n\nHi @michaelweylandt!\n\nI've uploaded my work for MiniProject #**01** - check it out!\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp01.html\n\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation."
  },
  {
    "objectID": "miniprojects/mini01.html#mini-project",
    "href": "miniprojects/mini01.html#mini-project",
    "title": "STA 9750 Mini-Project #01: Welcome to the Commission to Analyze Taxpayer Spending (CATS)",
    "section": "Mini-Project #01: Welcome to the Commission to Analyze Taxpayer Spending (CATS)",
    "text": "Mini-Project #01: Welcome to the Commission to Analyze Taxpayer Spending (CATS)\nCongratulations! You have just been appointed as a senior technical analyst working with New York City’s new Commission to Analyze Taxpayer Spending (CATS). As a technical analyst, you are tasked with helping the Commissioners understand New York City’s expenses and identifying opportunities to spend taxyper monies more effectively. Specifically, the Commission chair, Mr. Keno Slum, has asked you to analyze the City payroll and to identify instances in which senior agency officials make significantly more than rank-and-file city employees.\nIn this mini-project, you will analyze City payroll data and write a report highlighting possible savings to be submitted to the CATS Commissioners. In this mini-project, you will:\n\nBegin to work with NYC Open Data\n\nPractice Use of dplyr for analysis of tabular data\nPractice Use of quarto and Reproducible Research Tools for Effective Communication of Data Analysis Results\n\n\n\n\n\n\n\nWriting Requirements\n\n\n\nRecall that you are evaluated on writing and communication in these Mini-Projects. You are required write a report in the prescribed style, here an internal policy briefing ‘white paper’. A submission that performs the instructor-specified tasks, but does not write and give appropriate context and commentary will score very poorly on the relevant rubric elements.\nIn particular, if a submission is not in “white paper” style, peer evaluators should judge it to have “Good” quality Written Communication (at best) as key findings are not conveyed appropriately.\nQuarto’s cold folding functionality is useful for “hiding” code so that it doesn’t break the flow of your writing.\nYou can also make use of Quarto’s contents shortcode to present code and findings in an order other than how the code should be executed. This is particularly useful if you want to include a figure or table in an “Executive Summary” at the top of your submission.\n\n\nAcquiring Payroll Data\nThe following code will download the city payroll data and create a file nyc_payroll_export.csv in a data/mp01 directory. If this doesn’t work for whatever reason, you can download the data directly from NYC OpenData, though you will need to make sure it is in a suitable location and format for use in this mini-project.2\n\nif(!file.exists(\"data/mp01/nyc_payroll_export.csv\")){\n    dir.create(\"data/mp01\", showWarnings=FALSE, recursive=TRUE)\n    \n    ENDPOINT &lt;- \"https://data.cityofnewyork.us/resource/k397-673e.json\"\n    \n    if(!require(\"httr2\")) install.packages(\"httr2\")\n    library(httr2)\n    \n    if(!require(\"jsonlite\")) install.packages(\"jsonlite\")\n    library(jsonlite)\n    \n    if(!require(\"dplyr\")) install.packages(\"dplyr\")\n    library(dplyr)\n    \n    if(!require(\"readr\")) install.packages(\"readr\")\n    library(readr)\n    \n    BATCH_SIZE &lt;- 50000\n    OFFSET     &lt;- 0\n    END_OF_EXPORT &lt;- FALSE\n    ALL_DATA &lt;- list()\n    \n    while(!END_OF_EXPORT){\n        cat(\"Requesting items\", OFFSET, \"to\", BATCH_SIZE + OFFSET, \"\\n\")\n        \n        req &lt;- request(ENDPOINT) |&gt;\n                  req_url_query(`$limit`  = BATCH_SIZE, \n                                `$offset` = OFFSET)\n        \n        resp &lt;- req_perform(req)\n        \n        batch_data &lt;- fromJSON(resp_body_string(resp))\n        \n        ALL_DATA &lt;- c(ALL_DATA, list(batch_data))\n        \n        if(NROW(batch_data) != BATCH_SIZE){\n            END_OF_EXPORT &lt;- TRUE\n            \n            cat(\"End of Data Export Reached\\n\")\n        } else {\n            OFFSET &lt;- OFFSET + BATCH_SIZE\n        }\n    }\n    \n    ALL_DATA &lt;- bind_rows(ALL_DATA)\n    \n    cat(\"Data export complete:\", NROW(ALL_DATA), \"rows and\", NCOL(ALL_DATA), \"columns.\")\n    \n    write_csv(ALL_DATA, \"data/mp01/nyc_payroll_export.csv\")\n}\n\nYou do not (yet) need to understand the code above, so please use the course discussion board if you have trouble getting it working.\n\n\n\n\n\n\nTask 1: Data Acquisition\n\n\n\nUsing the code above, acquire the latest NYC Payroll Data.\n\n\n\n\n\n\n\n\nDo Not git add Data Files\n\n\n\nMake sure that git is set to ignore data files, such as the one created above. Check the git pane in RStudio and make sure that nyc_payroll_export.csv does not appear. (If you set up your .gitignore file correctly in MP#00, it should already be ignored.) If it is appearing, you may need to edit your .gitignore file.\nRemoving a large data file from git is possible, but difficult. Don’t get into a bad state!\n\n\nImporting Data into R and Preparing for Analysis\nBefore we can analyze this data, we need to get it into R and in a suitable format. The read_csv function from the readr package can be used to read csv files into R (imagine that!). The above download code will handle most of the irregularities in the city data,3 so we only need to make a few changes before beginning our analysis. In particular, let’s change some of the string columns to more conventional punctuation. The str_to_title function from the stringr package isn’t perfect (name capitalization rules can be rather idiosyncratic), but it will get us pretty close.\n\n\n\n\n\n\nTask 2: Data Import and Preparation\n\n\n\nRead your data into R using the read_csv function from the readr package. Before continuing, use a mutate command and the str_to_title function from the stringr package to convert the following columns to more conventional formatting.\n\nAgency Name\nLast Name\nFirst Name\nWork Location (Borough)\nTitle / Job Description\nLeave Status\n\n\n\nIt is good practice to always visually check your data to make sure it is properly formatting. You can use the glimpse function for a quick look here. We will use more refined formatting below.\nInitial Exploration\nIdentifying Individual Records\nAny time you are analyzing a new data set, you should begin by making sure you understand what the relevant columns are. In this case, we have basic descriptions of each column from the data page. We seem, however, to lack a few things you would normally want in a data set like this: in particular, we don’t have a unique ID for employees (a “primary key” in database parlance). This will make it harder for us to track individuals across years or across agencies (if they have two jobs at different agencies in the same fiscal year); we can approximate a unique identifier with First+Middle+Last, but it won’t be perfect.\nLet’s begin by reviewing the career of the current mayor, Eric L. Adams. Use a combination of filter, rename, arrange to create a table like the following. (You may also need to use group_by and summarize.)\n\n\nFiscal Year\nPosition\nAgency\nTotal Salary\n\n\n\n2020\nGame Keeper\nParks and Recreation\n15000\n\n\n2021\nFish Sorter\nEnvironmental Protection\n25000\n\n\n2022\nMayor\nOffice of the Mayor\n35000\n\n\n2023\nHamburgler\nMcDonald’s on 23rd\n20000\n\n\n\n(Obviously, these numbers and details are not correct.) Note that the Mayoral term does not line up with the City’s fiscal year, so you can choose how best to combine years in which Mr. Adams held more than one job. Once you have created the career table for Mr. Adams, you can use the DT package to create an attractive visualization of your results.\nFor example,\n\nlibrary(DT)\nlibrary(scales)\n\ntbl_txt &lt;- \"\nFiscal Year, Position, Agency, Total Salary\n2020,Game Keeper, Parks and Recreation, 15000\n2021, Fish Sorter, Environmental Protection, 25000\n2022, Mayor, Office of the Mayor, 35000\n2023, Hamburgler, McDonald's on 23rd,20000\"\n\nread_csv(tbl_txt) |&gt; \n    mutate(`Total Salary` = dollar(`Total Salary`)) |&gt;\n    datatable(options=list(searching=FALSE, \n                           paging=FALSE,\n                           info=FALSE))\n\n\n\n\n\nThe DT package wraps the Javascript datatables library to to provide interactive tables. This library has many options to control formatting. Later in this course, you will also counter the gt package to create complex tables natively in R.\n\n\n\n\n\n\nTask 3: Create an Employee Salary Table\n\n\n\nCreate an employee salary table for Eric L. Adams similar to that shown above, but with real salary and employment records.\n\n\nCalculating Aggregate Salaries\nFor high-ranking officials like Mayor Adams, their total compensation is a fixed salary. For other NYC employees, total compensation is computed using their hourly wage and the total hours worked (both regular and overtime). As a general rule, overtime is paid at 1.5x premium, so an employee who worked 40 hours of scheduled time and 20 hours of overtime with a base pay of $25 per hour, will be paid:\n\\[ \\$25 * (40 + 20 * 1.5) = \\$1750 \\]\nOther employees are paid a “day rate” - that is, a fixed amount per day worked. For purposes of this exercise, you can covert hours worked to days at a fixed rate of 7.5 hours per day. That is, if an employee is paid $100 per day and reports 1500 hours worked, you can estimate their pay as:\n\\[ \\$100 * \\frac{1500}{7.5} = \\$20,000 \\]\n\n\n\n\n\n\nTask 3: Calculation of Total Compensation\n\n\n\nUse these calculations to compute actual total compensation for each employee record in our data set. You will need to use a case_when() function inside a mutate() command to handle different pay structures.\n\n\nNow that we have computed total compensation for each city employee, we are ready to begin our analysis of the city payroll. Before we go further, answer some general questions about this data set to make sure you are comfortable with it.\n\n\n\n\n\n\nTask 4: Instructor-Provided Questions\n\n\n\nAnswer the following questions about city payroll data. In your final write-up, include these in the form of a “quick facts” bullet as part of your introduction.\n\nWhich job title has the highest base rate of pay? (If needed, assume a standard 2000 hour work year and no overtime.)\nWhich individual & in what year had the single highest city total payroll (regular and overtime combined)?\nWhich individual worked the most overtime hours in this data set?\nWhich agency has the highest average total annual payroll (base and overtime pay per employee)?\nWhich agency has the most employees on payroll in each year?\nWhich agency has the highest overtime usage (compared to regular hours)?\nWhat is the average salary of employees who work outside the five boroughs? (That is, whose work_location_borough is not one of the five counties.)\nHow much has the city’s aggregate payroll grown over the past 10 years?\n\n\n\n\n\n\n\n\n\nAvoid Using the ‘Paid’ Columns\n\n\n\nOur data set includes columns regular_gross_paid, total_ot_paid and total_other_paid that seem like they could be used for this mini-project. If you look closely, you will see that these amounts do not match what you might calculate by hand. This is because these include adjustments for i) use of deferred compensation plans; ii) pre-tax benefit withholding; iii) various ad hoc adjustments negotiated as part of collective bargaining agreements. Understanding all of these is far beyond the scope of this project, so you should use the simplified calculations described above.\n\n\nPolicy Analysis\nCATS has asked you to analyze three possible policy changes to analyze their impact on overall spending. Your supervisor has suggested two policies and has asked you to create a third for analysis. For each policy,\n\ncompute its impact on city payroll, i.e., determine how the total payroll expenses would have changed if that policy had been in place historically;\ndetermine any other staffing adjustments required to implement that policy, e.g., hiring more employees; and\nmake a recommendation to the CATS commissioners on whether this policy should be adopted.\n\nPolicy I: Capping Salaries at Mayoral Level\nMany governments require that the no subordinate employee be paid more than the chief executive (mayor, governor, president). CATS is considering recommending that the city adopt such a policy. To analyze it:\n\nCompute the total mayor pay for each fiscal year and identify employees who made more than this amount in the same fiscal year.\nDetermine total savings if these employees’ compensation were capped at the mayor’s salary.\nIdentify which agencies and job titles (if any) would bear the brunt of this policy.\n\n\n\n\n\n\n\nTask 5: Analyze the Impact of Capping Salaries at Mayoral Level\n\n\n\nAnalyze the impact of capping total aggregate compensation for any employee at the level of the mayor’s annual salary. Make a recommendation to CATS based on your findings.\n\n\nPolicy II: Increasing Staffing to Reduce Overtime Expenses\nA major driver of payroll expenditures is the “1.5x” premium associated with overtime work. That is, it may be cheaper to hire two employees to each work 30 hours to complete a task than to hire one employee to work 40 regular hours and 20 overtime hours to complete the same task.4 The CATS Commissioners are considering urging certain city agencies to hire more employees and to reduce the amount of overtime used.5\nAnalyze the potential upside of increasing employment to reduce overtime.\n\nFor each combination of agency and job title, identify the total number of overtime hours worked and see how many full-time employees it would take to replace that much overtime.\nFor each combination of agency and job title, calculate the total savings possible by converting all overtime hours to regular time hours for (new employees).\nDetermine the aggregate savings possible by agency. This will let the CATS Commission recommend the agencies where this hiring action would have the largest benefit.\n\nNote that much of this analysis must be done on an agency + job title basis. If, e.g., NYPD is paying lots of overtime to Sargents, hiring additional IT Specialists will not help. Similarly, hiring additional IT Specialists into NYPD will not help reduce the need for IT Specialists at the NYC Housing Authority. After completing this analysis at the agency + job title basis, you can aggregate (over job titles) to the agency level, as this is the most likely level at which hiring policy adjustments can be made.\n\n\n\n\n\n\nTask 6: Analyze the Potential Savings of Hiring Additional Employees to Reduce Overtime Expenses\n\n\n\nAnalyze the impact of authorizing agencies to hire more employees with the intent to reduce overtime expenses. Make sure to report the total potential savings, the total number of employees needed, and the agencies and job titles where such a change in policy would have the largest impact. The CATS Commissioners are particularly interested in a job title analysis as it may be hard to hire additional employees in certain positions.\n\n\nPolicy III: Create Your Own Policy Proposal\n\n\n\n\n\n\nTask 7: Create and Analyze an Additional Policy Proposal\n\n\n\nCreate your own policy proposal and analyze it in the same manner as the previous two policies. You are encouraged to draw upon real-world proposals to reduce city payroll."
  },
  {
    "objectID": "miniprojects/mini01.html#deliverable-policy-analysis-white-paper",
    "href": "miniprojects/mini01.html#deliverable-policy-analysis-white-paper",
    "title": "STA 9750 Mini-Project #01: Welcome to the Commission to Analyze Taxpayer Spending (CATS)",
    "section": "Deliverable: Policy Analysis “White Paper”",
    "text": "Deliverable: Policy Analysis “White Paper”\nWrite up your findings in the form of a “white paper” to be shared with the CATS Commissioners.6 Your submission should include all three policy analyses as well as sufficient background and context for a reader who is not already familiar with NYC pay structures. Note any assumptions or limitations of your analysis and explain how they might introduce error into your projections. You are encouraged to add images and figures as necessary, as well as linking to prior published analyses of NYC payroll."
  },
  {
    "objectID": "miniprojects/mini01.html#extra-credit",
    "href": "miniprojects/mini01.html#extra-credit",
    "title": "STA 9750 Mini-Project #01: Welcome to the Commission to Analyze Taxpayer Spending (CATS)",
    "section": "Extra Credit",
    "text": "Extra Credit\nThere is no extra credit available on this mini-project beyond that specified in the rubric above. In particular, peer evaluators are authorized to give up to 8 points (total across all categories) for\n\nReports that are particularly well-written and include, at a minimum, citations to existing analyses and policy proposal documents raised by local think-tanks and political candidates. Such citations are made stronger by a comparison of results and outcomes: you shouldn’t just say Think-Tank A made a similar proposal, but you should also see if your results differ with theirs and, if so, report any source(s) of the difference.\nPolicy proposals that are particularly creative and effective. When awarding extra credit, peer evaluators should be sure to assess the political feasibility of proposals. A proposal to, e.g., eliminate all public schools would certainly reduce the city’s spending on teachers but stands no chance of actually being implemented. A good policy proposal is both effective and effectible.\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "miniprojects/mini01.html#footnotes",
    "href": "miniprojects/mini01.html#footnotes",
    "title": "STA 9750 Mini-Project #01: Welcome to the Commission to Analyze Taxpayer Spending (CATS)",
    "section": "Footnotes",
    "text": "Footnotes\n\nThroughout this section, replace &lt;GITHUB_ID&gt; with your GitHub ID from Mini-Project #00, making sure to remove the angle brackets. Note that the automated course infrastructure will be looking for precise formatting, so follow these instructions closely.↩︎\nFor some reason, this data takes up to seven or eight minutes to download from the city website, even though it’s really not that complicated or massive. Don’t fret.↩︎\n\nIn particular, if you use the CSV export, the names of the columns in the data file will be “non-syntactic.” This means they are tricky to use in R and require use of backticks throughout your code. You will want to change these column names before proceeding. The rename function can be useful here.\n\nDATA &lt;- DATA |&gt;\n  rename(column_name=`Column Name`, \n         other_column=`Other Column`)\n\nNote that you will need to change all column names in this fashion.↩︎\n\nA commonly-raised criticism of public sector unions, such as those covering NYC employees, is that they make it difficult to hire additional employees, thereby directing more overtime hours to their (current) membership. I do not know of any reputable analysis supporting or refuting this claim in the context of NYC, though I would be interested in reading one if you come across one in your background reading.↩︎\nA recent NYT article describes recent struggles of the NYPD to increase staffing and to decrease the use of overtime pay. It’s not as simple as one might hope… (As a Baruch student, you have a free subscription to the New York Times; find registration details here.)↩︎\nThis recent white paper from the Rockefeller Institute regarding updating NYS’ public school funding formula is a good example of the genre. Clearly your report does not need to be this long or this detailed.↩︎"
  },
  {
    "objectID": "miniprojects/mini02.html",
    "href": "miniprojects/mini02.html",
    "title": "STA 9750 Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "",
    "text": "Released to Students: 2025-02-27\nInitial Submission: 2025-03-26 11:45pm ET on GitHub and Brightspace\n\nPeer Feedback:\n\nPeer Feedback Assigned: 2025-03-27 on GitHub\nPeer Feedback Due: 2025-04-02 11:45pm ET on GitHub\n\n\n\nEstimated Time to Complete: 9 Hours\nEstimated Time for Peer Feedback: 1 Hour"
  },
  {
    "objectID": "miniprojects/mini02.html#introduction",
    "href": "miniprojects/mini02.html#introduction",
    "title": "STA 9750 Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Introduction",
    "text": "Introduction\nWelcome to Mini-Project #02. In this project, you will take on the role of Executive Director of the Green Transit Alliance for Investigation of Variance (GTA IV). GTA IV will be giving a series of awards for the “greenest” public transit agencies. Your final submission for this mini-project should take the form of a press release, announcing the winners of the various GTA IV awards and giving the key statistics that lead to their selection.\nFor this project, you will use “multi-table” operations such as joins and pivots, to combine data from different sources to generate novel insights not attainable from any individual table. Our analysis will bring in data from two federal sources, the National Transit Database and the Energy Information Administration.\nStudent Responsbilities\nRecall our basic analytic workflow and table of student responsibilities:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn this mini-project, you are mainly responsible for data alignment and basic statistical analyses. While not the main focus of this mini-project, you are also expected to provide basic data visualizations to support your findings; the grading rubric, below, emphasizes the dplyr tools used in this project, but reports without any visualization will be penalized severely. Note that data visualization will play a larger role in Mini-Project #03.\nAs before, I will provide code to automatically download and read in the data used for this project. Also note that, as compared with Mini-Project #01, I am providing significantly less ‘scaffolding’: students are more responsible for directing their own analyses.\nRubric\nSTA 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.\nCode completes all instructor-provided tasks satisfactorially.\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three or ore instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nReport exhibits particularly creative insights drawn from thorough student-initiated analyses.\n\n\nFormatting & Display\n\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation.\nFigures are ‘publication-quality’, with suitable axis labels, well-chosen structure, attractive color schemes, titles, subtitles, and captions, etc.\n\n\nTables are well-formatted, but still have room for improvement.\nFigures are above ‘exploratory-quality’, but do not reach full ‘publication-quality’.\n\n\nTables lack significant ‘polish’ and need improvement in substance (filtering and down-selecting of presented data) or style.\nFigures are suitable to support claims made, but are ‘exploratory-quality’, reflecting minimal effort to customize and ‘polish’ beyond ggplot2 defaults.\n\n\nUnfiltered ‘data dump’ instead of curated table.\nBaseline figures that do not fully support claims made.\n\nReport lacks basic tables; OR report lacks basic figures.\nReport includes one or more high-quality graphics (created using R) using tools beyond static basic ggplot2. These can be created using extensions toggplot2 or speciality packages for interactive graphics.\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nAutomatic (10/10). Out of scope for this mini-project\n\n\n\n\nReport modifies instructor-provided import code to use additional columns or data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp02.qmd so the rendered document can be found at docs/mp02.html in the student’s repository and served at the URL:1\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp02.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/michaelweylandt/STA9750-2025-SPRING/issues/new .\n\nTitle the issue STA 9750 &lt;GITHUB_ID&gt; MiniProject #02 and fill in the following text for the issue:\n\nHi @michaelweylandt!\n\nI've uploaded my work for MiniProject #**02** - check it out!\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp02.html\n\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation.\n\nHi @michaelweylandt!\n\nI've uploaded my work for MiniProject #02 - check it out!\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp02.html"
  },
  {
    "objectID": "miniprojects/mini02.html#mini-project",
    "href": "miniprojects/mini02.html#mini-project",
    "title": "STA 9750 Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "text": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems\nAmong the commonly cited benefits of robust public transit is a reduction in environmental impact. In this mini-project, we will explore US Public Transit systems to assess their environmental efficiency. Our analysis will use a variety of data sources to i) determine how many riders are served by different transit systems; ii) determine how far each public transit system transports an average rider; and iii) investigate the effective emissions associated with each form of transit.\nData Acquisition\nWe begin by downloading and importing relevant data sets. For this mini-project, we will download the following details:\n\nEIA State Electricity Profiles, which we will use to estimate the environmental impact of the electricity used to run certain transit systems.\n\n\n\n\n\n\n\nTask 1: Data Import\n\n\n\nIncorporate the instructor provided code below into your report. As you do so, make sure to look for possible “keys” on which you will be able to join datasets together.\n\n\nState Electricity Profiles\nFor purposes of this analysis, we will assume that all electricity generation in a state is fungible. That is, we will assign the average generation emissions to all electric transit in a state, even if the transit authority officially has a “green supply” agreement in place.\nThe following code will parse the EIA SEP summary pages and create a “tidy” table for your use:\n\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(dplyr)\nensure_package(stringr)\nensure_package(tidyr)\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\n\nget_eia_sep &lt;- function(state, abbr){\n    state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n    \n    dir_name &lt;- file.path(\"data\", \"mp02\")\n    file_name &lt;- file.path(dir_name, state_formatted)\n    \n    dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n    \n    if(!file.exists(file_name)){\n        BASE_URL &lt;- \"https://www.eia.gov\"\n        REQUEST &lt;- request(BASE_URL) |&gt; \n            req_url_path(\"electricity\", \"state\", state_formatted)\n    \n        RESPONSE &lt;- req_perform(REQUEST)\n    \n        resp_check_status(RESPONSE)\n        \n        writeLines(resp_body_string(RESPONSE), file_name)\n    }\n    \n    TABLE &lt;- read_html(file_name) |&gt; \n        html_element(\"table\") |&gt; \n        html_table() |&gt;\n        mutate(Item = str_to_lower(Item))\n    \n    if(\"U.S. rank\" %in% colnames(TABLE)){\n        TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n    }\n    \n    CO2_MWh &lt;- TABLE |&gt; \n        filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n        pull(Value) |&gt; \n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    PRIMARY &lt;- TABLE |&gt; \n        filter(Item == \"primary energy source\") |&gt; \n        pull(Rank)\n    \n    RATE &lt;- TABLE |&gt;\n        filter(Item == \"average retail price (cents/kwh)\") |&gt;\n        pull(Value) |&gt;\n        as.numeric()\n    \n    GENERATION_MWh &lt;- TABLE |&gt;\n        filter(Item == \"net generation (megawatthours)\") |&gt;\n        pull(Value) |&gt;\n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    data.frame(CO2_MWh               = CO2_MWh, \n               primary_source        = PRIMARY,\n               electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n               # * 1000 kWh to MWH \n               generation_MWh        = GENERATION_MWh, \n               state                 = state, \n               abbreviation          = abbr\n    )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\nThis produces the following table:\n\nensure_package(scales)\nensure_package(DT)\n\nEIA_SEP_REPORT |&gt; \n    select(-abbreviation) |&gt;\n    arrange(desc(CO2_MWh)) |&gt;\n    mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n           electricity_price_MWh = dollar(electricity_price_MWh), \n           generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n    rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n           `Primary Source of Electricity Generation`=primary_source, \n           `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n           `Total Generation Capacity (MWh)`= generation_MWh, \n           State=state) |&gt;\n    datatable()\n\n\n\n\n\n \nHere, we have collected the effective emissions per MWh[^mwh] as well as price and total state-wide generation capacity. We will use this data to compare the emissions of different transit modalities. We next turn to the National Transit Database to get information on various public transit authorities. Before we do so, however, let’s explore the SEP data a bit first.\n\n\n\n\n\n\nTask 2: Initial Analysis of SEP Data\n\n\n\nAnswer the following questions using the EIA_SEP_REPORT data:\n\nWhich state has the most expensive retail electricity?\nWhich state has the ‘dirtiest’ electricity mix?\nOn average, how many pounds of CO2 are emitted per MWh of electricity produced in the US? (Note that you will need to use a suitably weighted average here.)\nWhat is the rarest primary energy source in the US? What is the associated cost of electricity and where is it used?\nMy home state, Texas, has a reputation as being the home of “dirty fossil fuels” while NY has a reputation as a leader in clean energy. How many times cleaner is NY’s energy mix than that of Texas?\n\n\n\n2023 Annual Database Energy Consumption\nWe first download the 2023 Annual Database Energy Consumption report. This report is shared as a Excel spreadsheet (.xlsx filetype), so we use the readxl package to import it to R.\n\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n    DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                  destfile=NTD_ENERGY_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\n\n\n\n\n\n\nSelection of Download Methods\n\n\n\nR provides several different internal tools to download data files from the internet. These can be accessed more-or-less seamlessly by changing the method argument of the download.file function.\nDepending on your operating system, different methods may work better than others. Above, I used the \"curl\" method as that seemed to be the most robust on my computer. If this doesn’t work, for you, I would recommend trying the following:\n\nmethod = \"internal\"\nmethod = \"wget\"\nmethod = \"libcurl\"\nmethod = \"wininet\"\n\nIf none of these work, download the file by hand and save it as data/mp02/2023_ntd_energy.xlsx. It is worth making sure that you are able to download data files through R as this will be important in your course project (and in life generally).\n\n\nNext, we do some basic clean-up:\n\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n    x &lt;- if_else(x == \"-\", NA, x)\n    replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n    select(-c(`Reporter Type`, \n              `Reporting Module`, \n              `Other Fuel`, \n              `Other Fuel Description`)) |&gt;\n    mutate(across(-c(`Agency Name`, \n                     `Mode`,\n                     `TOS`), \n                  to_numeric_fill_0)) |&gt;\n    group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n    summarize(across(where(is.numeric), sum), \n              .groups = \"keep\") |&gt;\n    mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n    filter(ENERGY &gt; 0) |&gt;\n    select(-ENERGY) |&gt;\n    ungroup()\n\nThe resulting table is almost ready for us to work with:\n\n# Display 10 random rows\nslice_sample(NTD_ENERGY , n=10)\n\n# A tibble: 10 × 16\n   `NTD ID` Mode  `Agency Name`       `Bio-Diesel` `Bunker Fuel` `C Natural Gas`\n      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                      &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n 1       23 MG    City of Seattle                0             0               0\n 2    40021 MB    City of Albany                 0             0          124607\n 3    50519 MB    Minnesota Valley T…       863323             0               0\n 4    10048 RB    Connecticut Depart…            0             0               0\n 5    60008 LR    Metropolitan Trans…            0             0               0\n 6    20113 MB    Regional Transit S…            0             0               0\n 7    50005 MB    City of Madison                0             0               0\n 8    40041 DR    Hillsborough Area …            0             0               0\n 9    50040 MB    Ann Arbor Area Tra…       229209             0               0\n10    90033 SR    City of Tucson                 0             0               0\n# ℹ 10 more variables: `Diesel Fuel` &lt;dbl&gt;, `Electric Battery` &lt;dbl&gt;,\n#   `Electric Propulsion` &lt;dbl&gt;, Ethanol &lt;dbl&gt;, Methonal &lt;dbl&gt;, Gasoline &lt;dbl&gt;,\n#   Hydrogen &lt;dbl&gt;, Kerosene &lt;dbl&gt;, `Liquified Nat Gas` &lt;dbl&gt;,\n#   `Liquified Petroleum Gas` &lt;dbl&gt;\n\n\nHere, the first 3 columns (NTD ID, Mode, and Agency Name) identify the operating agency and the relevant mode of transportation while the remaining columns denote how much of each type of fuel that agency used during 2023. The non-electric sources are all quoted in gallons, while Electric Battery and Electric Propulsion are quoted in kWh used.\nThe Mode column is important: it is used to distinguish multiple transport types operated the same agency, e.g. MTA subways and buses in NYC. The two letter codes used by the NTD, however, are not immediately interpretable.\n\n\n\n\n\n\nTask 3: Recoding the Mode column\n\n\n\nFirst, find the unique Mode codes in our data using the distinct function. Next, examine the NTD website and find the interpretations of these codes. Complete the following snippet to recode the Mode column.\n\n## This code needs to be modified\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n    mutate(Mode=case_when(\n        Mode == \"HR\" ~ \"Heavy Rail\", \n        ...\n        ...\n        TRUE ~ \"Unknown\"))\n\n\n\n2023 Annual Database Service by Agency\nNext, we download the 2023 Service by Agency report, from which we will extract characteristics of typical passenger trips on each transit service.\n\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n    DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                  destfile=NTD_SERVICE_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\nAs before, we need to clean up the data a bit before we can use it:\n\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n    mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n    rename(Agency = agency, \n           City   = max_city, \n           State  = max_state,\n           UPT    = sum_unlinked_passenger_trips_upt, \n           MILES  = sum_passenger_miles) |&gt;\n    select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n    filter(MILES &gt; 0)\n\nHere UPT is the number of total Unlinked Passenger Trips during 2023; that is, the number of distinct (non-transfer) trips taken. E.g., if you transfer from the 7 to the 6 to get to Baruch. You ride two segments, but only take 1 UPT to get to Baruch (2 total UPT if you go home the same way).\nLet’s explore this data a bit to get familiar with it.\n\n\n\n\n\n\nTask 4: Explore NTD Service Data\n\n\n\nAnswer the following questions using the NTD_SERVICE data.\n\nWhich transit service has the most UPT annually?\nWhat is the average trip length of a trip on MTA NYC?\n\nWhich transit service in NYC has the longest average trip length?\n(NB: You need to be careful with the City column here. Certain MTA services are officially located in “New York City” while others are located in Brooklyn.)\n\nWhich state has the fewest total miles travelled by public transit?\nAre all states represented in this data? If no, which ones are missing? The state.name and state.abb objects we used above may be useful here.\n\n\n\nWe’re now ready to start putting these datasets together and using them to identify America’s greenest transit agencies.\nAnalysis\nCalculate Total Emissions\nCompute the total emissions associated with each Agency + Mode pair.\n\n\n\n\n\n\nTask 5: Calculate Emissions\n\n\n\nYou will need to join the three tables (NTD_SERVICE, NTD_ENERGY, and EIA_SEP_REPORT) together. You should create a table with the following format:\n\nEach row should be an Agency + Mode Pair\nEach row should include the state in which the Agency is located (per the NTD_SERVICE table)\nEach row should include all fuel sources from the NTD_ENERGY table\nEach row should include the EIA estimated CO2 emissions per MWh from the EIA_SEP_REPORT table.\n\nOnce this table is created, use a (long-ish) arithmetic formula within a mutate to compute the total emissions for that row. Note that you will need additional data to convert combustion fuels to CO2 emissions; these conversions can be found on the EIA website. (You may hard code these values in your submission. Note that the fuel names used by the EIA do not exactly match those reported by the NTD; you will need to align these manually to best effort.)\n\n\nRelate Total Emissions and Passenger Service\nNow that you have computed the total emissions for each Agency + Mode pair, it is time to normalize these by the actual size of the transit agency. E.g., the NYC Subway almost certainly has higher total emissions than local bus service in Ithaca, NY, but it also serves many more riders, so the per capita or per transit emissions may be lower.\n\n\n\n\n\n\nTask 6: Normalize Emissions to Transit Usage\n\n\n\nUsing the total emissions calculated above, compute the emissions:\n\nPer UPT; and\nPer passenger mile\n\nWhich agencies are most efficient on these bases? You may want to filter out extremely small agencies and to create “small”, “medium”, and “large” agencies so that you can give multiple awards.\nNote that transit usage is reported at the Agency level, so you may need to sum emissions across multiple modes.\n\n\nIdenitify ‘Greenest’ Transit Agencies\nWith the above analysis, determine the winners of (at least) three GTA IV awards:\n\nGreenest Transit Agency\nMost Emissions Avoided\nA third award of your creation\nA “Worst Of” award of your creation\n\nThe Greenest Transit Agency award can be determined using your analysis above.\nFor the “Most Emissions Avoided” award, you should first compute the total emissions if all transit usage were replaced with driving individual vehicles. Then use US CAFE standards to convert miles to gallons of motor fuel used and convert motor fuel to total emissions as per above. Compare this to the actual emissions associated with that agency to determine emissions avoided.\nFor the third award, determine an additional metric on which a transit agency can be considered to be highly green (e.g., highest percentage of electrification) and then determine the appropriate winner.\nFinally, for the fourth award, “name and shame” an agency which does not have particularly ‘green’ operations. As with the third award, you can determine a metric you think is best.\nPer discussion above, you may choose to break these down into multiple categories (or honorable mention awards) to recognize smaller agencies.\n\n\n\n\n\n\nTask 7: Determine Award Winners\n\n\n\nDesign and implement metrics for the four GTA IV Green Transit Awards and determine winners."
  },
  {
    "objectID": "miniprojects/mini02.html#deliverable-transit-award-press-release",
    "href": "miniprojects/mini02.html#deliverable-transit-award-press-release",
    "title": "STA 9750 Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Deliverable: Transit Award “Press Release”",
    "text": "Deliverable: Transit Award “Press Release”\nAfter completing your analysis, prepare a “press release” announcing the winners in each category. For each award given, your press release should include at a minimum:\n\nA short description of the way in which the relevant metric was calculated.\nThe winning agency and/or transit mode and the value of their metric.\nA ‘reference’ value that makes the winner look particularly impressive, e.g. how did the median agency do on that same metric?\n\n\n\n\n\n\n\nTask 8: Visualization\n\n\n\nFor at least two of the four awards, create a suitable visualization demonstrating the ‘green-ness’ of the award winners, both in absolute terms and relative to other agencies and/or private modes of transporation.\nThese should be included in your press release as media supplements that can be used by, e.g., a local news report in their nightly broadcast. As such, these graphics should:\n\nHave professional-quality (“publication ready”) formatting;2\n\nHave low-clutter / high-information density;\nBe self-contained and accessible to a general reader without significant additional description retired.\nInclude citations to original data sources used\n\n\n\nAs with Mini-Project #01, you are required to present your findings in this format. Because the code implementing this analysis does not fit naturally in the press release format, you may find it easier to style it as an Appendix and to include the press release writing as an Executive Summary at the top of your submission."
  },
  {
    "objectID": "miniprojects/mini02.html#extra-credit-opportunities",
    "href": "miniprojects/mini02.html#extra-credit-opportunities",
    "title": "STA 9750 Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Extra Credit Opportunities",
    "text": "Extra Credit Opportunities\nPeer evaluators are authorized to give up to 4 points of extra credit (total across all categories) for\n\nPress release writing that is particularly funny, punny, or otherwise enjoyable to read; and\nAward criteria that are particularly creative and insight-generating. When awarding extra credit, peer evaluators should be sure to assess the quality of the insight generated by these additional awards: finding that, e.g., the NYC subway is highly efficient on two different but highly correlated metrics is not a remarkable insight.\n\nAdditionally, up to 4 points of extra credit may be given for code that automatically reads the EIA Fuel Type Emissions estimates automatically instead of hard-coding values. Note that full credit may be given for analysis which hard-codes these EPA estimates: reading and manipulating web data like this is not required until the end of this course. [^mwh]: 1 Megawatt-hour (MWh) is 1,000 kilowatt-hours (kWh) or roughly enough to power 3 NYC appartments for one month. Electricity bills are typically cited in kWh; to convert the per MWh price reported here to a per kWh price divide by 1000.\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "miniprojects/mini02.html#footnotes",
    "href": "miniprojects/mini02.html#footnotes",
    "title": "STA 9750 Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Footnotes",
    "text": "Footnotes\n\nThroughout this section, replace &lt;GITHUB_ID&gt; with your GitHub ID from Mini-Project #00, making sure to remove the angle brackets. Note that the automated course infrastructure will be looking for precise formatting, so follow these instructions closely.↩︎\nMake sure to set fonts and figure dimensions large enough to be legible on a large screen.↩︎"
  },
  {
    "objectID": "miniprojects.html",
    "href": "miniprojects.html",
    "title": "STA 9750 - Mini Projects",
    "section": "",
    "text": "In lieu of traditional homework, STA 9750 has a series of mini-projects designed to achieve several interlocking goals:\nEach Mini-Project will be submitted via GitHub, an industry-standard code management platform, as both raw analysis code and as a HTML document hosted on GitHub pages.\nAfter each Mini-Project is submitted, 2-3 peer reviewers will be assigned to give feedback and to assign an initial grade following an instructor provided rubric. This feedback will be given via GitHub Issues.\nIn order to ensure good peer feedback, the peer feedback will be evaluated by the instructor in a “meta-review” worth a small fraction of the overall grade.\nIf you believe your mini-project has received inaccurate peer feedback, please contact the instructor directly within 48 hours of the peer feedback deadline. No student-initiated requests for re-grading will be accepted after that time, though the instructor may re-grade the work during the meta-review stage."
  },
  {
    "objectID": "miniprojects.html#footnotes",
    "href": "miniprojects.html#footnotes",
    "title": "STA 9750 - Mini Projects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs of now, GitHub does not allow pre-filling a comment body via URL, so I can’t provide a helper script to template the peer review comment for you.↩︎"
  },
  {
    "objectID": "labs/lab08.html",
    "href": "labs/lab08.html",
    "title": "STA 9750 Week 8 In-Class Activity: More ggplot2 + Tools for Interactive Data Analysis",
    "section": "",
    "text": "Week 8 Slides"
  },
  {
    "objectID": "labs/lab08.html#spatial-visualization-with-ggplot2",
    "href": "labs/lab08.html#spatial-visualization-with-ggplot2",
    "title": "STA 9750 Week 8 In-Class Activity: Advanced ggplot2",
    "section": "Spatial Visualization with ggplot2",
    "text": "Spatial Visualization with ggplot2\nToday, we’re going to dive deeper into ggplot2, with a focus on visualizing spatial data. Spatial data can be quite complex, but we can get pretty far with the simple features (sf) paradigm. Also, if we focus on relatively small regions, we can avoid the complexities that come from Earth not being planar.\n\nKey to this approach is a shapefile. A shapefile, conventionally stored with a shp extension, gives precise coordinates outlining spatial regions. By plotting a polygon with those points at its boundaries, we can visualize spatial regions.\nThe sf package includes a shp file for the counties of North Carolina:\n\nlibrary(sf)\nnc &lt;- read_sf(system.file(\"shape/nc.shp\", package=\"sf\"))\nnc\n\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n# A tibble: 100 × 15\n    AREA PERIMETER CNTY_ CNTY_ID NAME  FIPS  FIPSNO CRESS_ID BIR74 SID74 NWBIR74\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 0.114      1.44  1825    1825 Ashe  37009  37009        5  1091     1      10\n 2 0.061      1.23  1827    1827 Alle… 37005  37005        3   487     0      10\n 3 0.143      1.63  1828    1828 Surry 37171  37171       86  3188     5     208\n 4 0.07       2.97  1831    1831 Curr… 37053  37053       27   508     1     123\n 5 0.153      2.21  1832    1832 Nort… 37131  37131       66  1421     9    1066\n 6 0.097      1.67  1833    1833 Hert… 37091  37091       46  1452     7     954\n 7 0.062      1.55  1834    1834 Camd… 37029  37029       15   286     0     115\n 8 0.091      1.28  1835    1835 Gates 37073  37073       37   420     0     254\n 9 0.118      1.42  1836    1836 Warr… 37185  37185       93   968     4     748\n10 0.124      1.43  1837    1837 Stok… 37169  37169       85  1612     1     160\n# ℹ 90 more rows\n# ℹ 4 more variables: BIR79 &lt;dbl&gt;, SID79 &lt;dbl&gt;, NWBIR79 &lt;dbl&gt;,\n#   geometry &lt;MULTIPOLYGON [°]&gt;\n\n\nWe represent this data in “tidy” format, with each row being a county. The “magic” is in the geometry column:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nnc |&gt; select(NAME, AREA, PERIMETER, geometry)\n\nSimple feature collection with 100 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n# A tibble: 100 × 4\n   NAME         AREA PERIMETER                                          geometry\n   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;                                &lt;MULTIPOLYGON [°]&gt;\n 1 Ashe        0.114      1.44 (((-81.47276 36.23436, -81.54084 36.27251, -81.5…\n 2 Alleghany   0.061      1.23 (((-81.23989 36.36536, -81.24069 36.37942, -81.2…\n 3 Surry       0.143      1.63 (((-80.45634 36.24256, -80.47639 36.25473, -80.5…\n 4 Currituck   0.07       2.97 (((-76.00897 36.3196, -76.01735 36.33773, -76.03…\n 5 Northampton 0.153      2.21 (((-77.21767 36.24098, -77.23461 36.2146, -77.29…\n 6 Hertford    0.097      1.67 (((-76.74506 36.23392, -76.98069 36.23024, -76.9…\n 7 Camden      0.062      1.55 (((-76.00897 36.3196, -75.95718 36.19377, -75.98…\n 8 Gates       0.091      1.28 (((-76.56251 36.34057, -76.60424 36.31498, -76.6…\n 9 Warren      0.118      1.42 (((-78.30876 36.26004, -78.28293 36.29188, -78.3…\n10 Stokes      0.124      1.43 (((-80.02567 36.25023, -80.45301 36.25709, -80.4…\n# ℹ 90 more rows\n\n\nThe geometry column is of type MULTIPOLYGON, essentially a list of GPS coordinates.\nIn this course, we’ll mainly just use these for plotting, though it is possible to do quite sophisticated analyses here.\nggplot2’s geom_sf makes it easy to visualize spatial data. It has one required aesthetic, fittingly named geometry:\n\nlibrary(ggplot2)\nggplot(nc, aes(geometry = geometry)) + geom_sf()\n\n\n\n\n\n\n\n\nNote that ggplot2 is doing some work behind the scenes to keep the aspect ratio reasonable and not “stretching” North Carolina north-south.\nThis is nice enough, if we like maps for maps’ sake, but we can do more. The BIR74 is the number of children born in that county between July 1, 1974 and June 30, 1978. We can use that to set the fills for each county.\n\nlibrary(ggplot2)\nggplot(nc, aes(geometry = geometry, \n               fill = BIR74)) + geom_sf()\n\n\n\n\n\n\n\n\nThis type of plot is called a chloropleth plot, coming from the Greek for “area-multitude”. It is commonly used to visualize areal geospatial data - that is quantities associated with spatial regions - as opposed to point data.\nBuilding a chloropleth plot is not too hard, if you have an appropriate shape file. Thankfully, governments are in the business of knowing exactly what the boundaries of their territories are and providing shape files. Typically, these shape files are distributed in a “zip” format, with other geodata files that we won’t use in this course.1\n\n\n\n\n\n\nAdding Points on Chloropleths\n\n\n\n\n\nWe can add the City of Charlotte, the largest city of North Carolina on this map:\n\ncharlotte &lt;- data.frame(\n    x = -80 - 50/60 - 35/60^2, # West = negative\n    y = +35 + 13/60 + 38/60^2  # North = positive\n)\n\nggplot() + \n    geom_sf(data=nc, \n            aes(geometry = geometry, \n                fill = BIR74)) + \n    geom_point(data=charlotte, \n               aes(x=x, y=y),\n               color=\"red4\")\n\n\n\n\n\n\n\n\nNote that I pass different data sets and aes mappings to each layer here, so I do them inside the individual geom_s instead of in the global ggplot() call."
  },
  {
    "objectID": "labs/lab08.html#exercises-1---geospatial-visualizations",
    "href": "labs/lab08.html#exercises-1---geospatial-visualizations",
    "title": "STA 9750 Week 8 In-Class Activity: More ggplot2 + Tools for Interactive Data Analysis",
    "section": "Exercises 1 - Geospatial Visualizations",
    "text": "Exercises 1 - Geospatial Visualizations\n\nDownload the New York City Council Districts (Clipped to Shoreline) shapefiles. Unzip the download and read the shape file, nycc.shp, using the sf::read_sf function.\nUse ggplot2::geom_sf to create a basic map of NYC’s city council districts. This should just be a plot of the outlines of each district (recognizable as NYC, but otherwise showing no data).\nUsing the nyc_demos.csv data file, create a chloropleth map of NYC, where the color variable represents the 2010 under 5-year-old population of each district.\nNote that you will need to use some dplyr *_join functions to combine the shape data with the demographic data before passing it to ggplot2.2 Which parts of NYC have the fewest young children? Does this seem right?\nUnder the “one person one vote” principle, the number of adult residents in each council district should be roughly equal. Create a chloropleth plot showing how many more/fewer adult residents each district has than the average district. Are any districts (significantly) over/under-represented on the NY city council?\nTo do this, you will need to first 1) determine the number of voting age residents in each district; 2) compute the average number of voting age residents per district; 3) see how each district compares to the average (above/below and by how much).\nCreate a facet plot where each facet is a chloropleth indicating the percentage of residents in each district identifying as a member of each census-designated racial categories.3 Note that the nyc_demos.csv file contains total counts by race, so you will need to normalize by overall population to get percentages.\nTo get the data into suitable form for plotting, you will need to pivot the data into a longer format using the pivot_longer function from the tidyr package.\nCreate a visualization to compare the ratio of rental vs owner-owned housing per district with the age demographics of this district. What do you find? Is this what you would have expected?"
  },
  {
    "objectID": "labs/lab08.html#limitations-and-extensions-of-chloropleths",
    "href": "labs/lab08.html#limitations-and-extensions-of-chloropleths",
    "title": "STA 9750 Week 8 In-Class Activity: More ggplot2 + Tools for Interactive Data Analysis",
    "section": "Limitations and Extensions of Chloropleths",
    "text": "Limitations and Extensions of Chloropleths\nChloropleth plots are rightly popular but they have a major limitation: humans are not uniformly distributed across space! Smaller regions may have higher populations (cf Connecticut vs Montana) and administrative regions are not the same size. Because human perception is naturally drawn to larger regions, we often need to adjust the color schemes used to compensate for area effects. This process can be powerful, but it is a bit difficult to get right.\nFor instance, let’s see what happens if we create a chloropleth of the SID74 data, the number of children who die of SIDS during our sample period in North Carolina:\n\nggplot(nc, aes(geometry = geometry, fill = SID74)) + \n    geom_sf()\n\n\n\n\n\n\n\n\nHere, Mecklenberg County (middle, bottom) sticks out because it has a large population, not because SIDS was particularly more common in that county. We can modify our plot to show the SIDS rate, rather than raw counts:\n\nnc |&gt;\n    mutate(sids_rate_74 = SID74 / BIR74) |&gt;\n    ggplot(aes(geometry = geometry, \n               fill = sids_rate_74)) + \n    geom_sf()\n\n\n\n\n\n\n\n\nWe see less variation here, but there is still some. Note that this type of data – rate estimation from rare counts – can be somewhat tricky to analyze, but that’s not the primary focus of this class so we’ll leave it here.\nThis plot still isn’t perfect however: Sampson county is the largest county in NC, but it has a relatively small population. That means that the largest area - and hence the place our eyes will most immediately look - is assigned to a relatively unimportant county. Relatedly, Anson County is a small population county, but it appears to be a “hot-spot”. This could be true, but it is more likely a noise-effect resulting from a small population.\nTo address these issues, we can use a cartogram which will “adjust” the map so that area maps to a relevant quantity. This is quite complex mathematically, but the cartogram package handles it reasonably transparently for us:\n\nlibrary(cartogram)\nnc |&gt; \n    mutate(sids_rate_74 = SID74 / BIR74) |&gt;\n    st_transform(26916) |&gt; \n    cartogram_cont(weight=\"BIR74\") |&gt; \n    ggplot(aes(geometry=geometry, \n               fill = sids_rate_74)) + \n    geom_sf()\n\n\n\n\n\n\n\n\nHere the st_transform function species what projection to equilibrate area in. I’m not an expert on this, but the internet seems to suggest 26916 is not a bad default.\nLooking at this plot, we see that counties are now adjusted to population, or really birth count and Anson county - while still high - is clearly down-weighted in accordance with its population."
  },
  {
    "objectID": "labs/lab08.html#exercises-2---cartograms-of-nyc",
    "href": "labs/lab08.html#exercises-2---cartograms-of-nyc",
    "title": "STA 9750 Week 8 In-Class Activity: More ggplot2 + Tools for Interactive Data Analysis",
    "section": "Exercises 2 - Cartograms of NYC",
    "text": "Exercises 2 - Cartograms of NYC\n\nModify your plot of NYC to adjust city council districts by area.\nNext, modify your analysis to use the Dorling cartogram (cartogram_dorling) which gives a more regular cartogram representation."
  },
  {
    "objectID": "labs/lab08.html#footnotes",
    "href": "labs/lab08.html#footnotes",
    "title": "STA 9750 Week 8 In-Class Activity: More ggplot2 + Tools for Interactive Data Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn MP#03, you will learn how to extract the shp file from a zip archive automatically, but for now you can do so “by hand” by opening the zip file as if it were a regular file.↩︎\nIf you get an error reading “stat_sf() requires the following missing aesthetics: geometry”, make sure you include geometry=geometry in your aes (5 points)↩︎\nThese are c(\"White Hispanic\", \"Black Hispanic\", \"Asian and Pacific Islander Nonhispanic\", \"Other Nonhispanic\", \"Two or More Races Nonhispanic\", \"Hispanic Origin\"). Note that the census has changed these over time and the categories for the 2020 and 2030 censuses will be different.↩︎"
  },
  {
    "objectID": "labs/lab07.html",
    "href": "labs/lab07.html",
    "title": "STA 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "",
    "text": "Update Slides: Slides 07\nThis week, we’re going to break into project groups and do three ggplot2 exercises of increasing difficulty. As you work through these with your teammates, be sure to reflect on what plots and what tools you will need to best present your mini-project and course project findings."
  },
  {
    "objectID": "labs/lab07.html#exercise-1-basic-ggplot2-15-minutes",
    "href": "labs/lab07.html#exercise-1-basic-ggplot2-15-minutes",
    "title": "STA 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Exercise 1: Basic ggplot2 (15 minutes)",
    "text": "Exercise 1: Basic ggplot2 (15 minutes)\nIn this exercise, you will create ggplot2 graphics to analyze the diamonds data from the ggplot2 package. This data contains pricing and measurements for 50,000 diamonds sold in the US. (Note that these prices are rather out of date.) Before beginning this exercise, you might want to read about the “4 C’s of Diamonds” commonly used to measure quality.\n\nMake a scatter plot of price vs carat and facet it by cut.\nUse geom_smooth to see how the price-carat relationship changes by color.\nCreate a frequency polygon plot of price, broken out by different diamond cuts.\nCreate a scatter plot of color by clarity. Why is this plot not useful?\n\nStretch Goal: Make a better plot to visualize this relationship using the ggmosaic package."
  },
  {
    "objectID": "labs/lab07.html#exercise-2-trend-analysis-with-ggplot2-30-minutes",
    "href": "labs/lab07.html#exercise-2-trend-analysis-with-ggplot2-30-minutes",
    "title": "STA 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Exercise 2: Trend Analysis with ggplot2 (30 minutes)",
    "text": "Exercise 2: Trend Analysis with ggplot2 (30 minutes)\nThe Carbon Dioxide Information and Analysis Center studies the effect of carbon dioxide on global and local temperature trends. A key tool in their analysis is the temperature “anomaly”. An anomaly is the difference between observed temperature (in a world with anthropogenic atmospheric CO2) and ‘natural’ temperature (from a world without anthropogenic gases). Note that these anomalies require significant analysis to compute and are not “simple observational” data.\nPoliticians have adopted the tools of temperature anomaly to set national and international emissions targets, e.g., the 2 Degree Target. Note that 2 degrees is calculated as a global average: in practice, some regions will experience a much larger change in temperature, while others may experience a smaller change or even a negative change.\nThe CVXR package includes the cdiac dataset, capturing CDIAC’s estimated global temperature anomalies from 1850 to 2015. In this question, you will explore these estimated anomalies. Note that you may need to install the CVXR package before beginning this question.1\n\ninstall.packages(\"CVXR\")\n\n\nlibrary(CVXR)\nlibrary(tidyverse)\ndata(cdiac)\nglimpse(cdiac)\n\nRows: 166\nColumns: 14\n$ year   &lt;int&gt; 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 186…\n$ jan    &lt;dbl&gt; -0.702, -0.303, -0.308, -0.177, -0.360, -0.176, -0.119, -0.512,…\n$ feb    &lt;dbl&gt; -0.284, -0.362, -0.477, -0.330, -0.280, -0.400, -0.373, -0.344,…\n$ mar    &lt;dbl&gt; -0.732, -0.485, -0.505, -0.318, -0.284, -0.303, -0.513, -0.434,…\n$ apr    &lt;dbl&gt; -0.570, -0.445, -0.559, -0.352, -0.349, -0.217, -0.371, -0.646,…\n$ may    &lt;dbl&gt; -0.325, -0.302, -0.209, -0.268, -0.230, -0.336, -0.119, -0.567,…\n$ jun    &lt;dbl&gt; -0.213, -0.189, -0.038, -0.179, -0.215, -0.160, -0.288, -0.310,…\n$ jul    &lt;dbl&gt; -0.128, -0.215, -0.016, -0.059, -0.228, -0.268, -0.297, -0.544,…\n$ aug    &lt;dbl&gt; -0.233, -0.153, -0.195, -0.148, -0.163, -0.159, -0.305, -0.327,…\n$ sep    &lt;dbl&gt; -0.444, -0.108, -0.125, -0.409, -0.115, -0.339, -0.459, -0.393,…\n$ oct    &lt;dbl&gt; -0.452, -0.063, -0.216, -0.359, -0.188, -0.211, -0.384, -0.467,…\n$ nov    &lt;dbl&gt; -0.190, -0.030, -0.187, -0.256, -0.369, -0.212, -0.608, -0.665,…\n$ dec    &lt;dbl&gt; -0.268, -0.067, 0.083, -0.444, -0.232, -0.510, -0.440, -0.356, …\n$ annual &lt;dbl&gt; -0.375, -0.223, -0.224, -0.271, -0.246, -0.271, -0.352, -0.460,…\n\n\n\nPlot the estimated annual global mean temperature (GMT) anomaly from 1850 to 2015.\n\n\nUse scale_x_date to improve the \\(x\\)-axis\n\n\nPlot the GMT anomaly for each month on the same plot (as different lines).\n\n\nBefore starting this, you may need to use the pivot_ functionality to get this data in the right shape. Recall that ggplot2 expects “data point” per row.\n\n\nPlot the monthly GMT anomaly series as one long line (with a point for each month).\nNow focus only on July: plot the July GMT anomaly series. Use the runmed()\nfunction to add a second series to the plot giving the median July GMT anomaly of the previous 5 years. Is there evidence of an increasing warming trend?\nFor each year, identify the warmest month (as measured by GMT anomaly); create a histogram showing the probability a given month was the hottest (largest anomaly) in its year.\n\n\n\nMake sure your \\(x\\)-axis is in reasonable (chronological) order - not alphabetical.\nYou will need to use dplyr tools to find the warmest month in a given year."
  },
  {
    "objectID": "labs/lab07.html#exercise-3-animated-graphics-1-hour",
    "href": "labs/lab07.html#exercise-3-animated-graphics-1-hour",
    "title": "STA 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Exercise 3: Animated Graphics (1 hour)",
    "text": "Exercise 3: Animated Graphics (1 hour)\nIn this question, you will use the gganimate extension to ggplot2 to create animated graphics. We will use the famous gapminder data set from the gapminder package. Install the gganimate, gapminder, gifski, and av packages before attempting attempting this problem.\n\nFor background, watch Hans Rosling’s talk on human prosperity.\nCreate a scatter plot of the relationship between GDP and Life Expectancy in the year 1952.\n\n\nColor points by continent and use the size aesthetic to represent population.\nYou might want to put quantities on a log-scale.\n\n\nThere is an outlier country in this data with very high GDP.\n\n\nWhat is it?\nIdentify and remove it.\n\n\nUsing the transition_time function, make this an animated plot showing how this data changes over time.\nUsing the theme machinery, labels, etc. make this a “publication ready” plot.\n\n\nNote that you can use {frame_time} in the title to get a dynamically changing year.\n\n\nUse the country_colors data from the gapminder plot to color the points using Dr. Rosling’s perferred color scheme.\n\n\nThis is a different color scale than ggplot2 uses by default, so you will need to override the scale_color_* functionality.\nThe help page for ?country_colors will be helpful here."
  },
  {
    "objectID": "labs/lab07.html#footnotes",
    "href": "labs/lab07.html#footnotes",
    "title": "STA 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCVXR is actually an incredible piece of software and super-useful for developing and implementing statistical and machine learning techniques. We, sadly, will not explore it in this course.↩︎"
  },
  {
    "objectID": "labs/lab05.html",
    "href": "labs/lab05.html",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "labs/lab05.html#join-specifications",
    "href": "labs/lab05.html#join-specifications",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Join Specifications",
    "text": "Join Specifications\ndplyr specifies joins using the join_by function. The output of the join_by function, also known as a “join specification” is a series of logical tests applied to pairs of rows. The results of these logical tests are used to identify “matches” between rows. Joins differ primarily on how they use the outputs of these logical tests to construct their output.\nThe simplest and most useful logical test to use in a join is an equality test. In dplyr, these are simply written as\n\njoin_by(left_name == right_name)\n\nThis type of test checks whether the value in the left_name column of the first (left) argument matches the value in the right_name column of the second (right) argument.\nFor example, if I wanted to join the origin column of flights table to the faa column of the airports table, I might use something like the following:\n\ninner_join(flights, airports, join_by(origin == faa))\n\n# A tibble: 336,776 × 26\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 18 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt;, lat &lt;dbl&gt;,\n#   lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n\nHere origin is taken to be a column from the first (left) table and faa is taken to be a column from the second (right) table. As with other dplyr functions, there is a bit of programming magic used to allow column names to be used as variables and interpreted correctly.\nFor the airport identifiers, we only need to match on the single unique ID. (We can assume the FAA assigns unique IDs to each airport.) In other circumstances, we need to combine several logical tests to get a true match.\nFor example, suppose we want to align our flights with the weather at their origin airport at scheduled take off time. Here, we’d need to combine the flights and weather table on many columns:\n\norigin to origin\nyear to year\nmonth to month\nday to day\nhour to hour\n\nIn this case, we’d pass 5 equality conditions to join_by:\n\ninner_join(flights, \n           weather, \n           join_by(origin == origin,\n                   year == year,\n                   month == month,\n                   day == day,\n                   hour == hour))\n\n# A tibble: 335,220 × 29\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 335,210 more rows\n# ℹ 21 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour.x &lt;dttm&gt;, temp &lt;dbl&gt;, dewp &lt;dbl&gt;,\n#   humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;,\n#   precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour.y &lt;dttm&gt;\n\n\nHere we look only at those rows which match on all 5 tests. In this way, join_by behaves like filter: it “passes” the intersection of positive results.\nNote that it is relatively common for matched columns to have the same name in both tables: to support this case, dplyr reads a single column name as “self-equality”. So the above code can be more concisely written as:\n\ninner_join(flights, \n           weather, \n           join_by(origin, \n                   year, \n                   month, \n                   day, \n                   hour))\n\n# A tibble: 335,220 × 29\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 335,210 more rows\n# ℹ 21 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour.x &lt;dttm&gt;, temp &lt;dbl&gt;, dewp &lt;dbl&gt;,\n#   humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;,\n#   precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour.y &lt;dttm&gt;\n\n\nI recommend against using this short-cut. It takes hardly more time to write your intent explicitly and it’s far more robust. Measure twice, cut once.\nUnfortunately, it is not easy to perform an “OR” in join_by. We may cover this below, time allowing.\nWe now turn to specific joins. All of these joins use the join_by operator but they construct results differently based on its output."
  },
  {
    "objectID": "labs/lab05.html#inner-joins",
    "href": "labs/lab05.html#inner-joins",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Inner Joins",
    "text": "Inner Joins\nThe most common and most important join in data analysis is the inner_join. The inner join returns matches between two tables. Conceptually, the inner join constructs all possible pairs of rows between the two tables (so {r eval=FALSE} NROW(x) * NROW(y) total rows) and then filters down to those which pass the join_by test. In practice, more efficient algorithms are used to prevent wasteful computation.\nInner joins are used when seeking matches between two tables. They are particularly useful when both tables are “comprehensive” and we are sure that there are matches. For instance, we can use an inner_join to combine most of the tables in nycflights13 because they come from a comprehensive government data source. (E.g., No flights going to secret “unauthorized” airports.)\nLet’s start by asking what the average arrival delay of flights going to west coast airports is. We do not have enough information to answer this using the flights table alone. To identify west coast airports, let’s filter airports on tzone:\n\nwest_coast_airports &lt;- airports |&gt; filter(tzone == \"America/Los_Angeles\")\n\nWe can now join this to the original flights table to find only those flights with destination matches in west_coast_airports:\n\ninner_join(flights, west_coast_airports, join_by(dest == faa))\n\n# A tibble: 46,324 × 26\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      558            600        -2      924            917\n 2  2013     1     1      558            600        -2      923            937\n 3  2013     1     1      559            600        -1      854            902\n 4  2013     1     1      611            600        11      945            931\n 5  2013     1     1      628            630        -2     1016            947\n 6  2013     1     1      646            645         1     1023           1030\n 7  2013     1     1      651            655        -4      936            942\n 8  2013     1     1      655            700        -5     1037           1045\n 9  2013     1     1      658            700        -2     1027           1025\n10  2013     1     1      702            700         2     1058           1014\n# ℹ 46,314 more rows\n# ℹ 18 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt;, lat &lt;dbl&gt;,\n#   lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n\nHere, we have only a subset of our original flights table. From this, we can compute our relevant summary statistic:\n\ninner_join(flights, west_coast_airports, join_by(dest == faa)) |&gt;\n    summarize(mean(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(arr_delay, na.rm = TRUE)`\n                            &lt;dbl&gt;\n1                            1.28\n\n\nIs this any better than the following alternative approach:\n\ninner_join(flights, airports, join_by(dest == faa)) |&gt;\n    filter(tzone == \"America/Los_Angeles\") |&gt;\n    drop_na() |&gt;\n    summarize(mean(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(arr_delay, na.rm = TRUE)`\n                            &lt;dbl&gt;\n1                            1.28\n\n\nFormally, these are basically equivalent. (filter and inner_join commute). As usual, it’s a matter of communicating intent. Here the single line filter(tzone == \"America/Los_Angeles\") is simple enough it probably doesn’t need a separate variable. But if, instead of a one line operation, we performed a very complex set of filtering options, we may benefit from giving it a separate name as opposed to trying to shoe-horn the complex filtering into a pipeline.\nPerformance-wise, it is a bit better to perform filter before inner_join (Why? Think about the size of the result of each step.) but the difference is rarely material. Clarity of intent, not optimizing performance, should dictate the order in which you perform steps.\nBoth approaches are also equivalent to:\n\ninner_join(flights, \n           airports |&gt; filter(tzone == \"America/Los_Angeles\"), \n           join_by(dest == faa)) |&gt;\n    drop_na() |&gt;\n    summarize(mean(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(arr_delay, na.rm = TRUE)`\n                            &lt;dbl&gt;\n1                            1.28\n\n\nBut I find this sort of “filter inside join argument” to be terribly difficult to read: it mixes standard (inside-out) and piped (left to right) evaluation orders in a confusing manner. Avoid this!\nWork with your group to answer the following questions using inner_join.\n\nWhat is the name of the airline with the longest average departure delay?\nWhat is the name of the origin airport with the longest average departure delay?\nWhat is the name of the destination airport with the longest average departure delay?\nAre average delays longer for East-coast destinations or West-coast destinations?\nWhich plane (tailnum) flew the most times leaving NYC? Who manufactured it?\nWhich manufacturer has the most planes flying out of NYC airports?\nWhich manufacturer has the longest average flight?\nWhat model of plane has the smallest average delay leaving NYC?"
  },
  {
    "objectID": "labs/lab05.html#left-join",
    "href": "labs/lab05.html#left-join",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Left Join",
    "text": "Left Join\nLeft joins are useful when you don’t want to dropped unmatched columns in one table. For instance, suppose we misplace some rows from our airlines table:\n\nairlines_major &lt;- airlines |&gt;\n    filter(carrier %in% c(\"AA\", \"DL\", \"UA\", \"WN\", \"B6\", \"AS\"))\n\nIf we inner join on airlines_major, we loose many of the rows in flights.\n\nNROW(flights)\n\n[1] 336776\n\ninner_join(flights, \n           airlines_major, \n           join_by(carrier == carrier)) |&gt;\n    NROW()\n\n[1] 207128\n\n\nSometimes this is what we want, but not always. If we instead use a left join, we keep all of the rows in flights:\n\nNROW(flights)\n\n[1] 336776\n\nleft_join(flights, \n          airlines_major, \n          join_by(carrier == carrier)) |&gt;\n    NROW()\n\n[1] 336776\n\n\nRows lacking a pair in airlines_major fill the missing columns with NA. This fits our mental model of missing values in R: in theory, these flights should have some carrier name, but given the data at hand, we don’t know what it is.\n\nNROW(flights)\n\n[1] 336776\n\nleft_join(flights, \n          airlines_major, \n          join_by(carrier == carrier)) |&gt;\n    filter(carrier %in% c(\"MQ\", \"OO\", \"VX\")) |&gt;\n    glimpse() # Look at 'name' column\n\nRows: 31,591\nColumns: 20\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 600, 602, 608, 624, 656, 658, 729, 749, 800, 805, 811, …\n$ sched_dep_time &lt;int&gt; 600, 605, 600, 630, 705, 700, 730, 710, 810, 815, 630, …\n$ dep_delay      &lt;dbl&gt; 0, -3, 8, -6, -9, -2, -1, 39, -10, -10, 101, -4, -5, -8…\n$ arr_time       &lt;int&gt; 837, 821, 807, 840, 1007, 1027, 1049, 939, 949, 1006, 1…\n$ sched_arr_time &lt;int&gt; 825, 805, 735, 830, 940, 1025, 1115, 850, 955, 1010, 83…\n$ arr_delay      &lt;dbl&gt; 12, 16, 32, 10, 27, 2, -26, 49, -6, -4, 137, -13, -13, …\n$ carrier        &lt;chr&gt; \"MQ\", \"MQ\", \"MQ\", \"MQ\", \"MQ\", \"VX\", \"VX\", \"MQ\", \"MQ\", \"…\n$ flight         &lt;int&gt; 4650, 4401, 3768, 4599, 4534, 399, 11, 3737, 4406, 4490…\n$ tailnum        &lt;chr&gt; \"N542MQ\", \"N730MQ\", \"N9EAMQ\", \"N518MQ\", \"N722MQ\", \"N627…\n$ origin         &lt;chr&gt; \"LGA\", \"LGA\", \"EWR\", \"LGA\", \"LGA\", \"JFK\", \"JFK\", \"EWR\",…\n$ dest           &lt;chr&gt; \"ATL\", \"DTW\", \"ORD\", \"MSP\", \"XNA\", \"LAX\", \"SFO\", \"ORD\",…\n$ air_time       &lt;dbl&gt; 134, 105, 139, 166, 233, 361, 356, 148, 80, 101, 118, 5…\n$ distance       &lt;dbl&gt; 762, 502, 719, 1020, 1147, 2475, 2586, 719, 427, 479, 5…\n$ hour           &lt;dbl&gt; 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 6, 8, 8, 8, 8, 18, 9, 9, …\n$ minute         &lt;dbl&gt; 0, 5, 0, 30, 5, 0, 30, 10, 10, 15, 30, 25, 35, 40, 50, …\n$ time_hour      &lt;dttm&gt; 2013-01-01 06:00:00, 2013-01-01 06:00:00, 2013-01-01 0…\n$ name           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nleft_joins are useful if we want to join two tables, but want to avoid dropping any rows from a ‘gold standard’ table."
  },
  {
    "objectID": "labs/lab05.html#outer-join",
    "href": "labs/lab05.html#outer-join",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Outer Join",
    "text": "Outer Join"
  },
  {
    "objectID": "labs/lab05.html#advanced-join-specifications",
    "href": "labs/lab05.html#advanced-join-specifications",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Advanced Join Specifications",
    "text": "Advanced Join Specifications"
  },
  {
    "objectID": "labs/lab05.html#cumulative-operators",
    "href": "labs/lab05.html#cumulative-operators",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Cumulative Operators",
    "text": "Cumulative Operators"
  },
  {
    "objectID": "labs/lab05.html#rank",
    "href": "labs/lab05.html#rank",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "*_rank",
    "text": "*_rank"
  },
  {
    "objectID": "labs/lab05.html#advanced-joins",
    "href": "labs/lab05.html#advanced-joins",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Advanced Joins",
    "text": "Advanced Joins\n\ncross_join\n\n\nsemi_join\n\n\nanti_join\n\n\nnest_join"
  },
  {
    "objectID": "labs/lab05.html#bind_rows-and-bind_columns",
    "href": "labs/lab05.html#bind_rows-and-bind_columns",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "bind_rows and bind_columns",
    "text": "bind_rows and bind_columns"
  },
  {
    "objectID": "labs/lab05.html#footnotes",
    "href": "labs/lab05.html#footnotes",
    "title": "STA 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that some SQL engines use LEFT OUTER JOIN than LEFT JOIN. Because OUTER is a bit ambiguous, dplyr emphasizes full_ vs left_ in its function naming. Also note the convention of dplyr names - lower case, underscore separated - and that it differs from SQL syntax.↩︎"
  },
  {
    "objectID": "labs/lab04.html#rs-missing-data-model---na",
    "href": "labs/lab04.html#rs-missing-data-model---na",
    "title": "STA 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "R’s Missing Data Model - NA",
    "text": "R’s Missing Data Model - NA\nLet’s begin with a simple question: what is the single most delayed flight in our data set:\n\n\n\n\n\n\n\n\nHmmmm…That’s odd. Certainly something has to be the maximum arrival delay - why did we get no rows back?\nLet’s look at this expression more closely: firstly, what happens if we simply fix a delay amount?\n\n\n\n\n\n\n\n\nThat’s fine. So perhaps the problem was in computing max(arr_delay).\n\n\n\n\n\n\n\n\nThat’s weird - what is this NA object?\nNA is R’s representation of missing data: this is not a NaN object you have seen from other languages. NaN represents invalid arithmetic output (Not-a-Number), e.g,\n\n\n\n\n\n\n\n\nNA is statistical missingness. The data exists - and is well defined - but we simply don’t know it. Like we said above, there is some most delayed flight, but we don’t know what it is.\nThe NA construct is a bit odd when you start with it - but it’s actually one of R‘s great strengths. Missingness matters in data analysis and R forces you to deal with it explicitly. The behavioral rules of NA are reasonably straightforward - NA is ’contagious’. Any calculation that takes at least one NA input usually has NA output. (This is not dissimilar to the “random in, random out” rule of functions of random variables) For example:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat last result may be a bit surprising - isn’t anything times zero just zero?\nThat’s true in ‘real’ math, but not actually true for computer (“floating-point”) math:\n\n\n\n\n\n\n\n\nHere, because 0 * NA could be 0 or NaN, the answer is still unknown and hence NA.\nThere are some rare operations where NA can be “over-ruled” but they are not super common:\n\n\n\n\n\n\n\n\nThis follows because both:\n\n\n\n\n\n\n\n\nso the value of NA doesn’t actually matter here.\nAlso note that not all NA values are ‘the same’:\n\n\n\n\n\n\n\n\nWhy is this the case? Well, suppose we rewrite this as:\n\n\n\n\n\n\n\n\nIs today the same temperature as tomorrow? If we don’t know either temperature, we can’t say!\nSimilarly,\n\n\n\n\n\n\n\n\n\nis.na and na.rm\nWhile it’s certainly helpful that R handles NA values so intelligently for us, it can also be a bit annoying. Eventually we want (non-NA) answers!\nWe generally deal with this in one of two ways:\n\nfiltering out NA values from our data set\nignoring NA values in our calculations.\n\nWe’ve already done a bit of the latter option - ignoring NA values in our calculations - so let’s review it first.\nMost base R functions have an na.rm optional argument to remove NA values. Returning to our motivating example:\n\n\n\n\n\n\n\n\nor indeed\n\n\n\n\n\n\n\n\nThat’s a horrendous (21+ hour) delay! But is it actually the “maximum” delay? It depends… we’ll come back to this example in a bit.\nNot all functions, however, provide a na.rm argument: in those cases, it’s our responsibility to remove the NA values ourselves.\nWe can do this using the is.na function: this takes in a vector of values and finds the NAs:\n\n\n\n\n\n\n\n\nIf we combine this with the filter operator, we now have an efficient way of removing NA values:\n\n\n\n\n\n\n\n\nFrom here, we can get back to our analysis of the most delayed flight:\n\n\n\n\n\n\n\n\nPoor folks!\nNote that I’m using glimpse here to ensure all columns are printed.\n\n\ndrop_na\ndplyr provides a drop_na function which removes any row that has an NA value in any column. It’s a bit of a blunt approach - do you really need remove a row in computing X just because it has an NA value in column Y? - but it can be useful for “quick and dirty” work. I recommend against using it without a thorough manual examination of your data first however.\n\n\nNA values in filter\nEarlier we saw that filter plays funny with NA values. It’s worth being explicit here\n\n\n\n\n\n\n\n\nfilter checks for TRUE conditions - not for “not FALSE”. Because of this, checks which result in NA lead to dropped rows. This means that most NA rows are automatically discarded when you start filtering.\nThis isn’t a bad default - but it’s one you should be aware of. For instance, in our motivating example:\nWe might use the following to compute the average arrival delay:\n\n\n\n\n\n\n\n\nbut this drops\n\n\n\n\n\n\n\n\nflights for which we have no arrival delay information. Of these,\n\n\n\n\n\n\n\n\nwe even have an arrival time but the delay itself is missing for some reason. Is it fair to exclude these flights or should we compute the delay ourselves? For flights that are missing arrival and departure times (i.e., cancelled flights), should we exclude them? Are they infinitely delayed? 24 hour delayed (assuming passengers were rebooked to the same flight on the next day)?\nThere’s no clear right-or-wrong answer to questions like this. It’s all context dependent: if you are the DOT trying to ensure good customer experience, a cancelled flight is very delayed; if you are instead a Boeing engineer looking to improve flight speeds, the cancelled flights simply aren’t useful to you.\nWhen faced with these challenges, data scientists often give the answer “defer to subject matter experts (SMEs)”. Unfortunately, we rarely have the resources to have a qualified SME at hand to answer ever little data analytic question we may have.\nI instead advocate for a strategy of reproducible transparency. Using tools like quarto, we can show our code and document the choices made. Then, when we share our results with an SME,"
  },
  {
    "objectID": "labs/lab04.html#boolean-operators-and-filter",
    "href": "labs/lab04.html#boolean-operators-and-filter",
    "title": "STA 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "Boolean Operators and filter",
    "text": "Boolean Operators and filter\nfilter() lets you use a logical test to extract specific rows from a data frame. To use filter(), pass it the data frame followed by one or more logical tests. filter() will return every row that passes each logical test.\nSo for example, we can use filter() to select every flight in flights that departed on January 1st:\n\n\n\n\n\n\n\n\nThe filter function is similar to the WHERE clause in SQL. As we will later see, it can also be used to implement the HAVING clause, when applied in conjunction with group_by.\nLike all dplyr functions, filter() returns a new data frame for you to save or use. It doesn’t overwrite the old data frame. If you want to save the output of filter(), you’ll need to use the assignment operator, &lt;-.\nRerun the command in the code chunk below, but first arrange to save the output to an object named jan1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood job! You can now see the results by running the name jan1 by itself. Or you can pass jan1 to a function that takes data frames as input.\nDid you notice that this code used the double equal operator, ==? == is one of R’s logical comparison operators. Comparison operators are key to making full use of filter(), so let’s take a closer look at them.\n\nLogical Comparisons\nR provides a suite of comparison operators that you can use to compare values: &gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal). Each creates a logical test. For example, is pi greater than three?\n\npi &gt; 3\n\n[1] TRUE\n\n\nWhen you place a logical test inside of filter(), filter applies the test to each row in the data frame and then returns the rows that pass, as a new data frame.\nOur code above returned every row whose month value was equal to one and whose day value was equal to one.\n\nWatch out!\nWhen you start out with R, the easiest mistake to make is to test for equality with = instead of ==. When this happens you’ll get an informative error:\n\n\n\n\n\n\n\n\nIf you give filter() more than one logical test, filter() will combine the tests with an implied “and.” In other words, filter() will return only the rows that return TRUE for every test. You can combine tests in other ways with Boolean operators…\n\n\n&, |, and !\nR uses Boolean operators to combine multiple logical comparisons into a single logical test. These include & (and), | (or), ! (not or negation), and xor() (exclusive or).\nBoth | and xor() will return TRUE if one or the other logical comparison returns TRUE. xor() differs from | in that it will return FALSE if both logical comparisons return TRUE. The name xor stands for exclusive or.\nStudy the diagram below to get a feel for how these operators work.\n\n\n\nIn the figure above, x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each command selects.\n\n\n\n\nCommon mistakes\nIn R, the order of operations doesn’t work like English. You can’t write filter(flights, month == 11 | 12), even though you might say “finds all flights that departed in November or December”. Be sure to write out a complete test on each side of a Boolean operator.\nHere are four more tips to help you use logical tests and Boolean operators in R:\n\nA useful short-hand for this problem is x %in% y. This will select every row where x is one of the values in y. We could use it to rewrite the code in the question above:\n\n\n\n\n\n\n\n\n\n\nSometimes you can simplify complicated subsetting by remembering De Morgan’s laws: !(x & y) is the same as !x | !y, and !(x | y) is the same as !x & !y. For example, if you wanted to find flights that weren’t delayed (on arrival or departure) by more than two hours, you could use either of the following two filters:\n\n\n\n\n\n\n\n\n\n\nAs well as & and |, R also has && and ||. Don’t use them with filter()! You’ll learn when you should use them later.\nWhenever you start using complicated, multipart expressions in filter(), consider making them explicit variables instead. That makes it much easier to check your work.\n\n\n\n\nExercises\n\nFilter Statements\nUsing filter and various Boolean operators, find all flights satisfying the following conditions.\n\nHad an arrival delay of two or more hours\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(arr_delay &gt; 120)\nflights |&gt; filter(arr_delay &gt; 120)\n\n\n\n\n\n\n\nFlew to Houston (IAH or HOU)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(dest %in% c(\"IAH\", \"HOU\"))\nflights |&gt; filter(dest %in% c(\"IAH\", \"HOU\"))\n\n\n\n\n\n\n\nWere operated by United (UA), American (AA), or Delta (DL)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\nflights |&gt; filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n\n\n\n\n\n\nDeparted in summer (June, July, or August)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(month &gt;= 6, month &lt;= 8)\nflights |&gt; filter(month &gt;= 6, month &lt;= 8)\n\n\n\n\n\n\n\nArrived more than two hours late, but didn’t leave late\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(arr_delay &gt; 120, dep_delay &lt;= 0)\nflights |&gt; filter(arr_delay &gt; 120, dep_delay &lt;= 0)\n\n\n\n\n\n\n\nWere delayed more than an hour, but made up more than 30 minutes in flight\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(dep_delay &gt; 60, (dep_delay - arr_delay) &gt; 30)\nflights |&gt; filter(dep_delay &gt; 60, (dep_delay - arr_delay) &gt; 30)\n\n\n\n\n\n\n\nDeparted between midnight and 6am (inclusive)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter((dep_time &lt;= 600) | (dep_time == 2400))\nflights |&gt; filter((dep_time &lt;= 600) | (dep_time == 2400))"
  },
  {
    "objectID": "labs/lab04.html#grouped-operations",
    "href": "labs/lab04.html#grouped-operations",
    "title": "STA 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "Grouped Operations",
    "text": "Grouped Operations\nIn this week’s preassignment, you also already saw the basics of the group_by operator for performing analyses on subgroups. The most common use of group_by is to modify summarize to perform group-wise summarization. We’ll next explore how it can be used to do group level filtering, similar to an SQL HAVING clause.\nAs before, let’s start by asking what is the average arrival delay (after removing NA values)?\n\n\n\n\n\n\n\n\nOk. But now suppose we want to know which carrier had flights that were later than average? We_could_ simply copy the value over into a new line of code:\n\n\n\n\n\n\n\n\nTo get carrier-wise statistics, we might try:\n\n\n\n\n\n\n\n\nThis works, but it requires us to keep the number 6.9 at hand, which is a bit inconvenient.\nWe next might be tempted to use a variable here to avoid hard-coding a specific value:\n\n\n\n\n\n\n\n\nThis is definitely better! If our data changes, we don’t have to worry about the number 6.9 being ‘out of date’. But it’s still maybe a bit clunky: we filter our data twice for NA values and have to repeat ourselves.\nLet’s try something else:\n\n\n\n\n\n\n\n\nThis creates a new column called mean delay. On its own, it’s not very interesting:\n\n\n\n\n\n\n\n\nNote the trick of using everything() inside a select statement to reorder columns.\nThe mean_delay column simply repeats the number 6.9 over and over. (Recall R’s recycling rules- we needed a long vector here, so the output of mean was repeated enough to fill the whole table.) But now we can work with this:\n\n\n\n\n\n\n\n\nand, if we want, we can get the carrier specific statistics:\n\n\n\n\n\n\n\n\nPretty nice! And when it matters - for very large data stored on a database - a little faster to boot!\nBefore going deeper down this path, what happens if we move the group_by earlier in our pipeline?\n\n\n\n\n\n\n\n\nDefinitely different! But why?\nTo see the difference, let’s compare the mean_delay column:\n\n\n\n\n\n\n\n\nWe now see here that the mean_delay is computed “group-wise”, so we’re not getting flights that are delayed compared to an average flight; we are instead counting flights that are delayed compared to an average flight on that airline. Put another way, we’re holding American Airlines (AA) and Delta (DL) flights to a higher standard than Jet Blue (B6) or ExpressJet (EV).\nAs always - the question you should ask yourself is not “is this the right thing” but “when is this the right thing?”. It’s simply a different question!\nRecall that group_by followed by a summarize removes one “layer” of grouping. If we use this group_by + mutate + filter construction, the result is still grouped, which can lead to weird bugs. To address this, it is sometimes easier to use the .by argument to mutate and filter which will modify the grouping for that command only.\n\n\n\n\n\n\n\n\nIt’s a matter of taste.\n\nHAVING clause\nRecall that a SQL HAVING clause applies group-level filtering based on some summary statistics: this is easy enough in dplyr.\nFor example, suppose we want the average flight delays of large airlines, which we can define as those with more than 10,000 departures in our data set.\nWe can compute this in two ways: directly, computing the number of flights and average delay for each airline.\n\n\n\n\n\n\n\n\nThis totally works, but now we’ve lost all the other flight-level information. An alternate approach is to compute counts group-wise and filter before averaging:\n\n\n\n\n\n\n\n\nThis has the advantage of being readily adaptable to other non-summarizing questions: for instance, of the delayed flights of the major carrier, how many were going to Houston?\n\n\n\n\n\n\n\n\nHere, we re-used the n column and so the old value of n was quietly replaced. This is probably ok with a simple variable name like n (which wasn’t all that interesting) but for “raw” data columns, you probably should avoid this.\nIn class, we’ll do more exercises based on group-specific filtering, both filtering on groups and filtering within groups. See if you can answer:\n\nWhat carrier has the lowest rate of delayed flights?\nWhat carrier has the highest chance of early arrivals?\nWhat carrier is most likely to “make up time in flight” after a delayed departure?\nWhich origin airport has the highest rate of delays?\nWhich month has the most flights?\nWhat is the furthest flight in this data?\nWhat is the shortest flight in this data?\nAre longer flights more likely to be delayed than short ones?\n\n\nThe readings in this tutorial follow R for Data Science, section 5.2. The exercises for filter were adapted from the official documentation of the learnr package."
  },
  {
    "objectID": "labs/lab04.html#footnotes",
    "href": "labs/lab04.html#footnotes",
    "title": "STA 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 2013 NYC version of this data has become a semi-standard teaching example, but the US Bureau of Transportation Statistics releases new versions of this data constantly. If you are interested in performing this type of analysis for a different set of airports or a different time period, check out the anyflights package. It’s very easy - but a bit slow - to get flight data from almost any US airport this way. If you want to develop your data cleaning skills, it’s a great exercise to parse the BTS website directly and compare your output with the anyflights package.↩︎"
  },
  {
    "objectID": "labs/lab03.html",
    "href": "labs/lab03.html",
    "title": "STA 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "labs/lab03.html#review-of-r",
    "href": "labs/lab03.html#review-of-r",
    "title": "STA 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Review of R",
    "text": "Review of R\nIn this section, we will review some of the basic ‘built-in’ features of R. In the next section (Packages) we will discuss how to add to the “base-bones” functionality. When working with R, there are two interacting ‘subsystems’ in play:\n\nThe R language and interpreter: this is the part of R that is similar to python or C/C++. You will write R code in the R language and the R interpreter will run that code. The fact that all of these elements are called R is a bit confusing, but once you get the hang of things, the distinctions will melt away.\nR packages: When working in R, you do not have to start from scratch every time. Other programmers make sets of code available to you in the form of packages. For our purposes, a package can contain two things:\n\nPre-written functions to help you achieve some goal\nData sets Most of the time, the primary purpose of a package is sharing functions and code: there are easier ways to share data with the world.\n\n\nWhen you first downloaded R, you downloaded the interpreter and a set of base packages written by the “R Core Development Team”.\nRun the following code to see what your R environment looks like:\n\n\n\n\n\n\n\n\nCompare the output of running this here-in the browser-with what you get by running sessionInfo() on your machine.\nThere is lots of useful information here, including\n\nthe version of R being used\nthe operating system\nthe numerical linear algebra libraries (BLAS, LAPACK) used\nsystem language and time zone information\nloaded packages\n\nWhen asking for help, always include the output of the sessionInfo() command so that your helper can quickly know how your system is configured.\n\nPackages\nR code is distributed as packages, many of which come included with R by default. These are the base packages, and they are noted in your sessionInfo(). But we can do many more things with R using contributed (non-base) packages!\nThe most common platform for distributing R packages is CRAN, the Comprehensive R Archive Network, available at https://cran.r-project.org/. You have likely already visited this site to download R. The available.packages() function in R lists all packages currently on CRAN. We can see that there are many:\n\n\n[1] 22009\n\n\nIf you want to use a contributed package, you need to do two things:\n\nDownload it from CRAN and install it onto your computer (one time)\nLoad it from your hard drive into R (every time you restart R)\n\nThe first step - download and install - can be completed using the install.packages() function. For example, to install the palmerpenguins package, I would run:\n\ninstall.packages(\"palmerpenguins\")\n\nThis will automatically download and install this package for me. R is helpful and also tries to automatically install all packages that a given package relies upon. Because of this, it is often sufficient to install the “last step” and trust R to handle the dependencies automatically. In this course, most of the packages we use can be automatically installed by installing the tidyverse package.\n\ninstall.packages(\"tidyverse\")\n\nNote that there really isn’t much in the tidyverse package we want, but it’s a useful proxy for a much larger set of packages.\nOnce a package is installed, we need to load it into R with the library function:\n\nlibrary(palmerpenguins)\n\nAfter doing this, we have access to the contents of the palmerpenguins package until we restart R.\n\nNote that the install.packages function wants you to quote its argument, but library does not. This is a weird historical quirk of R that you will trip up on many times before this course ends.\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\nFor example, palmerpenguins package provides a data set of penguin measurements. If we try to get the data set without loading palmerpenguins, we get an error message:\n\n\n\n\n\n\n\n\nAfter we install and load palermerpenguins, we are good to go:\n\n\n\n\n\n\n\n\nSo much tuxedo goodness!\nIn general, if you get a error message of the form Error: object 'X' not found, you should:\n\nMake sure you spelled X properly\nIf X comes from a package, make sure you library() that package.\n\nThere’s no harm to library()-ing a package multiple times; if you install.packages() a package that you have already loaded, you may need to restart R.\n\nAs mentioned last week, I strongly recommend never saving your workspace in R or RStudio. One of the things “saved” in a workspace is the list of loaded packages, so it becomes essentially impossible to reinstall a package properly.\n\n\n\nVariables and Assignment\nWhenever you type a “word” of R code, it must be one of three things:\n\nA reserved word: this is a small set of keywords that R keeps for its own use. These have special rules for their use that we’ll learn as we go along. The main ones are: if, else, for, in, while, function, repeat, break, and next.\nIf you use one of these words and get a weird error message, it’s likely because you aren’t respecting the special rules for these words.\nFor the nitty gritty, see the Reserved help page but feel free to skip this for now. The Control help page gives additional details.\n(When you run a help page in this tutorial, it looks a bit funny. Try running ?Reserved directly in RStudio for better formatting.)\nA “literal”. This is a word that represents “just the thing” without any additional indirection. The most common types of literals are:\n\nNumeric: e.g., 3, 42.0, or 1e-3\nString: e.g., 'a', \"beach\", or 'cream soda' There are a few rules for literals, but the most important is that strings begin and end with the same character, either a single quote or a double quote. When R sees a single quote, it will read everything until the next single quote as one string, even if there’s a double quote inside.\n\nTry some literals:\n\n\n\n\n\n\n\n\n\nWhat does the literal 0xF represent? (You don’t need to worry about why. This is a fancier literal than we will use in this class.) - A “variable name”. This is the most common sort of “word” in code. It is used to something without actually having to know what it is.\nWe can create variables using the “assignment” operator: &lt;-\n\n\n\n\n\n\n\n\nWhen you read this outloud, read &lt;- as “gets” so x &lt;- 3 becomes “x gets 3.”\nWhen we use the assignment operator on a variable, it overwrites the value of a variable silently and without warning\n\n\n\n\n\n\n\n\nWe also put expressions on the right hand side of an assignment:\n\n\n\n\n\n\n\n\nAlso note the trick we’ve used here a few times: a “plain” line of code without an assignment generally prints its value.\n\n\nComments\nWhen you include a # symbol, R will ignore everything after it. This is called a comment and you can use it to leave notes to yourself about what you are doing and why.\n\n\nVector Data Types\nA vector is a ordered array of the same sort of thing (e.g., all numbers or all strings). We can create vectors using the c command (short for concatenate).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the above example to c(1, \"a\", 3) and examine the output. What happened? Why?\n\n\n\nTo see the type (sort) of a vector, you can use the str command.\n\n\n\n\n\n\n\n\nstr(x) tells us about the structure of x. Here, we see that x is a numeric vector of length 3.\nR will try to do the right thing when doing arithmetic on vectors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen you give R vectors of different lenghts, it will “recycle” the shorter one to the length of the longer one.\n\n\n\n\n\n\n\n\nThis can be a double-edge sword when the two vectors don’t fit together so nicely:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow was the last element of x+y computed?\n\n\n\nHere we see also that R printed a warning message. A warning message is R’s way of saying “something is funny, but I can still do this” while it (successfully) implements your command. It’s here to help you, but sometimes can be safely ignored if you’re sure about what you’re doing.\nAn error is a “I can’t do this” message. When R encounters an error it stops and does not fully execute the command\n\n\n\n\n\n\n\n\nHere we get an error because there is no meaningful way to multiply a string by a number, unlike earlier where the recycling rule told R what to do, even if it was probably a bad idea."
  },
  {
    "objectID": "labs/lab03.html#functions",
    "href": "labs/lab03.html#functions",
    "title": "STA 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Functions",
    "text": "Functions\n\nFunctions\nIn many of these exercises, we have used commands that have the form NAME() with zero or more comma-separated elements in the parentheses.\nThis represents a function call. Specifically, the command func(x, y) calls the function named func with two arguments x and y.\nFunctions are the verbs of the programming world. They are how anything gets done. So far, we’ve only used some basic functions:\n\nc: the concatenate function\nprint: the print function\nstr: the structure function\nlist: the list making function\n\nBut there are tons of other useful ones!\nTry these out: - length on a vector - colnames on a data frame (like PlantGrowth) - toupper on a string (vector) - as.character on a numeric value\n\n\n\n\n\n\n\n\n\nArguments: Positional and Keyword\nThe inputs to a function are called the arguments. They come in two forms: - Positional - Keyword\nSo far we have only seen positional arguments. The function interprets them in an order that depends on they were given:\n\n\n\n\n\n\n\n\nHere paste combines two values into a string. We get different output strings depending on the order of the input.\nOther arguments can be passed as keyword arguments. Keyword arguments come with names that tell functions how to interpret them. For example, the paste function has an optional keyword argument sep that controls how the strings are combined.\n\n\n\n\n\n\n\n\nKeyword arguments typically have defaults so you don’t need to always provide them. For the paste function, the sep defaults to \" \".\n\n\n\nCreating Your Own Functions\nWhen you want to create your own function, you use a variant of the assignment structure\n\nmy_addition &lt;- function(x, y) {\n    x + y\n}\n\nLet’s break this into pieces:\n\nOn the left hand side of the assignment operator &lt;-, we see the function name. This works exactly the same as vector assignment.\nImmediately to the right of the assignment operator, we see the keyword function. This tells R that we are defining a function.\nAfter the word function, we see the “argument list”, i.e., the list of inputs to the function (comma separated). Here, we are not providing default values for any function.\nFinally, between the curly braces, we get the body of the function. This is actually the code defining a function’s behavior. You can do basically anything here! Define variables, do arithmetic, load packages, call other functions - it’s all valid. (In fact, you can even define a function within a function, but that’s sort of advanced.)\nThe last line of the body (here the only line) defines the return value of the function, i.e., its output.\n\nThis function will add two numbers together. Now that we’ve defined it, we can use it just like a built-in function:\n\nmy_addition(2, 4)\n\n[1] 6\n\n\nTip: You can see the code used to define any function by simply printing it: think of the code as being the “value” and the function name as the variable name. (This isn’t actually just a metaphor - it’s literally true!)\n\nDefault Arguments\nSometimes, we want functions to have default but changeable behvaior. This is default arguments come in. If the user provides a value, the function uses it, but otherwise the default is used.\nFor example,\n\nmake_bigger &lt;- function(x, by=2){\n    x + by\n}\n\nmake_bigger(3)\n\n[1] 5\n\nmake_bigger(3, by = 3)\n\n[1] 6\n\n\nHere by defaults to 2, but the user is required to supply x because it has no default.\n\nmake_bigger(by=3)\n\nError in make_bigger(by = 3): argument \"x\" is missing, with no default\n\n\nThere are lots of details in the mechanics - they even can be ‘dynamic’ using some tricks - but in general, they should “just work.”"
  },
  {
    "objectID": "labs/lab03.html#control-flow",
    "href": "labs/lab03.html#control-flow",
    "title": "STA 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Control Flow",
    "text": "Control Flow\nSo far, all of the code we have written executes linearly, one line at a time. To write complex programs, however, we sometimes need code to execute in other ways: e.g., going line by line through a complex data set running the same code (a “loop”) or doing different things depending on the value of a variable (a “conditional”). This brings us to the topic of control flow, or how a program gets executed.\n\nConditionals\nPerhaps the most common control flow operator is the “conditional” - the if operator. In R, the if operator looks like this:\n\nif(TEST){\n    # Some code goes here\n    # This gets run if `TEST` is true\n} else {\n    # Some code goes here\n    # This gets run if `TEST` if false\n}\n\nFor example\n\nx &lt;- 3\nif(x &gt; 0){\n    print(\"x is positive!\")\n} else {\n    print(\"x is negative\")\n}\n\n[1] \"x is positive!\"\n\n\nThe element inside the if (the test statement) should ideally be Boolean (TRUE/FALSE-ish) but R will make a reasonable guess if it isn’t.\nNote that you can omit the else part and the second set of braces that go with it, but the first set of braces, immediately after if(), should always be there.\n\n\n\n\n\n\n\n\nChange the value of x and see what happens. Next, modify this by adding an else statement to handle the case of odd numbers.\nNote that we’re using the %% operator here. If you haven’t seen it before, recall you can get help by typing\n\n?%%\n\nin R. In this case, %% is a modulo operator; that is, it is the “remainder” from division. (Do you see how it works here?)\nWe’ll practice using conditional operators below.\n\n\nLooping\nIn other ## Programming Exercises\nWrite functions to perform each of the following tasks.\n\nWrite a function that takes in a vector of numbers, calculates the length and maximum value of the vector, and prints that information to the screen in a formatted way.\n\n&gt; func_1(c(1, 2, 3, 5, 7))\nThe largest value in that list of 5 numbers is 7.\n\n&gt; func_1(c(1, 2, 5, 5))\nThe largest value in that list of 4 numbers is 5.\nTo make your output as attractive as possible, you might want to use the cat command instead of the print command.\n\nWrite a program that tests whether its (integer) outputs are leap years. Recall the leap year rules:\n\nA year is a leap year if it is divisible by 4\nBut it is not a leap year if it is divisible by 100\nUnless it is also divisible by 400\n\n\n&gt; leap_year(2023)\nFALSE\n&gt; leap_year(2024)\nTRUE\n&gt; leap_year(2100)\nFALSE\n&gt; leap_year(2000)\nTRUE\nRemember our discussion of the %\\% operator from class.\n\nWrite a function to greet your classmates with varying levels of enthusiasm. It should have three optional arguments:\n\nname. The name of the person to greet. Default \"friend\"\ntimes. The number of times to repeat the greeting.\nemphasis. A Boolean (TRUE/FALSE) value indicating whether the greeting should end with an exclamaition point. (Default FALSE)\n\n\n&gt; greetings()\nHello, friend\n&gt; greetings(name=\"Michael\")\nHello, Michael\n&gt; greetings(times=2)\nHello, friend\nHello, friend\n&gt; greetings(emphasis=TRUE)\nHello, friend!\n&gt; greetings(\"Michael\", 5, TRUE)\nHello, Michael!\nHello, Michael!\nHello, Michael!\nHello, Michael!\nHello, Michael!\n\nThe Riemann Zeta Function is a famous function in analytic number theory1 defined as \\[\\zeta(k) = 1 + \\left(\\frac{1}{2}\\right)^k + \\left(\\frac{1}{3}\\right)^k + \\dots = \\sum_{i=1}^{\\infty} i^{-k} \\] We cannot implement an infinite series in R, but we can get very close by taking a large number of terms in the series (e.g, the first 500,000). Implement the zeta function and show that \\(\\zeta(2) = \\frac{\\pi^2}{6}\\)\n\n&gt; zeta(2)\n[1] 1.644932\n&gt; zeta(3)\n[1] 1.202057\n&gt; zeta(4)\n[1] 1.082323\n&gt; all.equal(zeta(2), pi^2/6, tol=1e-4)\n[1] TRUE\n\nHero of Alexandria developed a method for computing square roots numerically. He showed that by performing the following update repeatedly, \\(x\\) will converge to \\(\\sqrt{n}\\): \\[x \\leftarrow \\frac{1}{2}\\left(x + \\frac{n}{x}\\right)\\] You can start with any positive \\(x\\), but \\(n/2\\) is a good choice.\nImplement this method to compute square roots. Use an optional keyword argument (default value 100) to control how many iterations are performed:\n\n&gt; hero_sqrt(100)\n[1] 1.644932\n&gt; hero_sqrt(3)^2\n[1] 3\n&gt; hero_sqrt(3, iter=2)\n[1] 1.732143\n&gt; hero_sqrt(3000)\n[1] 54.77226"
  },
  {
    "objectID": "labs/lab03.html#footnotes",
    "href": "labs/lab03.html#footnotes",
    "title": "STA 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nANL is basically the application of calculus techniques to prove properties of prime numbers: it’s a surprisingly powerful approach.↩︎"
  },
  {
    "objectID": "labs/lab02.html",
    "href": "labs/lab02.html",
    "title": "STA 9750 Week 2 In-Class Activity: Getting Down with Markdown",
    "section": "",
    "text": "Welcome!\nSlides"
  },
  {
    "objectID": "labs/lab01.html",
    "href": "labs/lab01.html",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "Topics:\n\nInstalling R and RStudio\n\nInstalling git\n\nGetting Started on GitHub\n\nBasic Principles of “Clean Code”\n\n\nThe primary programming language used in this course is R, one of the two most popular languages used in data science. R, like its predecessor the S language, is optimized for interactive, data-analytic work, in contrast with python, which is optimized for general purpose computing.\nR is a programming language and runtime; we will supplement it with RStudio, an Integrated Development Environment or, less formally, an editor. RStudio is the software where you will write the code and then the R runtime will execute it.\n\nStudents should first install R from https://cloud.r-project.org/.\n\n\nDon’t fear the 90’s web design! Click image for detailed installation instructions.\n\nAs of 2024-08-26, the most recent version of R is 4.4.1. Using the most current version of R will reduce the likelihood of issues later in the course.\n\nNext, download and install the RStudio IDE (desktop edition).\n\n\nClick image for detailed installation instructions.\n\nRStudio is highly configurable and I recommend taking advantage of all its built-in features. If you go to the Global Options menu (accessible under Tools), I recommend the following settings:\n\nGeneral: Uncheck “Restore .RData into workspace at startup”.\nGeneral: Set “Save workspace to .RData on exit” to “Never”\nCode / Editing: Set “Tab width” to 2\nCode / Editing: Check\n\n“Insert spaces for Tab”\nAuto-detect code indentation\nInsert matching parens / quotes\nUse native pipe operator\nAuto-indent code after paste\nVertically align arguments in auto-indent\nContinue comment when inserting new line\n\n\nCode / Display: Check\n\nShow line numbers\nShow margin (margin column should be 80)\n\n\nCode / Diagnostics: check all “R” diagnostics.\nAppearance: Pick a color theme you enjoy. (I’m partial to light text on a dark background)\n\nYou may wish to enable GitHub Copilot. I have little experience with GH Copilot, but it seems quite popular and is allowed in this course. It is not guaranteed to be accurate at all times - and “the AI told me to” is not a valid excuse if your code is wrong - but on balance, it should be useful.\n\nWe won’t use it this week, but you will need to install Quarto before starting on Mini-Project #00.\n\n\ngit is a source-code management tool, used by developers to manage the code they write. If you’ve ever been part of a large project and struggled to coordinate all team members using the same version of a document, git exists to solve that problem.\nIf you don’t have git pre-installed, install either Git for Windows or the XCode Command Line Tools for MacOS. If not automatically prompted when you try to use git, the Mac install can be manually triggered by running xcode-select --install at a command line.\nIn this course, we will use three main functions of git:\n\n\nstaging: telling git, I want you to prepare to save a certain file\n\ncommitting: saving a set of related changes\n\npushing: copying your committed changes to a separate server for sharing and backup\n\nWhenever you write code you are happy with, you should use git to save it. Saving changes with git is cheap and easy - so do it regularly. You always want git to have a backup of good code in case you loose power, accidentally delete a file, break something in a way you’re not sure how to undo, etc..\nRStudio comes with powerful git integration. Once you have created a project, you should see a tab labelled “Git” in the top right corner of your IDE window that looks something like this:\n\nTo stage a file - prepare to save it - click the empty check box next to the file name. A new file shows a status of “?” - this is git saying “I’ve never seen this file before. Do you want me to track it for you?”. Later, when you make further changes to file you have already asked git to track, a status of “M” (for Modified) will be shown.\nOn its own staging a file does nothing. You also need to commit it for git to truly track it.1 The Commit button will commit all staged changes. When you make a commit, git requires a brief message summarizing the changes. There’s no particular formatting requirement to this message, but it should be something that future-you is able to easily understand. For instance, the commit message from the initial draft of this document reads as:\nInitial draft of Lab 01 (STA9750)\n\n- Installing R and RStudio\n- Git and GitHub\n- Leaflet Example for Styler\n\nTODO: Fuller shell explainers\nTODO: Link more git help\nWhen I read this, I know the purpose of the change I made (first line), the contents of that change (list), and parts that still need more work.\nFinally, after you save a change, it is only saved on your computer. The true power of git comes from its ability to copy changes and backups across machines. This gives you an easy way to store backups in case your computer dies and makes collaboration efficient and fun. git allows you to push and pull changes between machines in endlessly powerful (but sometimes complex) ways. For this course, we’ll keep things simple and only use GitHub to share code. We discuss GitHub in the next section.\nReference: We will not use all of the functionality of git in this course, but you should familiarize yourself with Chapters 1, 2, and 6 of the Git Book over the next two weeks.\n\nGitHub is an industry-standard code hosting and collaboration platform. In addition to hosting copies of code, GitHub provides web hosting, bug reporting, code review, continuous integration, documentation wikis, and discussion fora. You will explore GitHub in more detail starting in Mini-Project #00.\n\n\nA major theme of this course will be sharing and co-developing code with your classmates, both for peer feedback and for the course project. Code sharing is hard! Everyone writes code a little differently and what is clear to you may not be clear at all to your reader.\nTo make code sharing just a bit easier, we use tools to ensure all code shared in this course is consistently formatted. By using consistent formatting, you reduce the cognitive load on your reader, making it easier for them to focus on the ideas of your code, not how you chose to write it.\nA major strength of R is its huge number of user-contributed packages. These are “add-ins” which provide additional functionality not available in the basic version of R. As of 2024-08-26, there are over 21 thousand packages available on CRAN, the largest official repository of R packages. Beyond all those, there are thousands more packages available on other code hosting websites like GitHub.2\nWe will use the contributed styler package to format code in this course. Run the following command to automatically download and install the styler package:\n\ninstall.packages(\"styler\")\n\n(Use the clipboard icon on the right of code snippets to automatically copy code suitable for pasting into RStudio.)\nYou should see something like this:\n\nThe styler package has been downloaded and installed on your computer, but it is not yet “active” or “open” in R. In general, you will only need to download packages once, but you will need to load them each time you want to use them.3\nOpen a R file in RStudio and copy the following (ugly) code:\n\nif(!require(\"leaflet\")) install.packages(\"leaflet\")\nif(!require(\"tidyverse\")){\n    install.packages(\"tidyverse\")\n   }\n library(tidyverse)\n     library(rvest)\nlibrary(leaflet)\n\npAGE = read_html('https://en.wikipedia.org/wiki/Baruch_College')\n  pAGE |&gt; html_element(\".latitude\") |&gt; html_text2() -&gt; BaruchLatitude\n  baruch_longitude &lt;- pAGE |&gt; html_element(\".longitude\") |&gt; html_text2()\n  \n    BaruchLatitude &lt;- sum(as.numeric(strsplit(BaruchLatitude, \n                                     \"[^0123456789]\")[[1]]) * (1/60)^(0:2), na.rm=TRUE)\n baruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) * \n                             (1/60)^(0:2), na.rm=TRUE)\n  \nleaflet() %&gt;% addTiles() %&gt;% setView(-baruch_longitude, BaruchLatitude, zoom=17) %&gt;%\n    addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nYou don’t need to understand what this does just yet, but it’s hopefully clear that this is ugly code. Nothing is lined up properly, capitalization is erratic, and different coding styles are intermixed rather recklessly.\nNear the top of your RStudio pane, you will see a drop-down menu titled Addins. If you successfully installed styler above, one of the Addins choices will be “style active file.” Click this and the code will be cleaned up (a bit) resulting in something like this:\n\nif (!require(\"leaflet\")) install.packages(\"leaflet\")\nif (!require(\"tidyverse\")) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(leaflet)\n\npAGE &lt;- read_html(\"https://en.wikipedia.org/wiki/Baruch_College\")\npAGE |&gt;\n  html_element(\".latitude\") |&gt;\n  html_text2() -&gt; BaruchLatitude\nbaruch_longitude &lt;- pAGE |&gt;\n  html_element(\".longitude\") |&gt;\n  html_text2()\n\nBaruchLatitude &lt;- sum(as.numeric(strsplit(\n  BaruchLatitude,\n  \"[^0123456789]\"\n)[[1]]) * (1 / 60)^(0:2), na.rm = TRUE)\nbaruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) *\n  (1 / 60)^(0:2), na.rm = TRUE)\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(-baruch_longitude, BaruchLatitude, zoom = 17) %&gt;%\n  addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nIt’s far from perfect - and we will discuss the many issues in this example throughout the course - but it’s better! At a minimum, you should make sure to run styler like this on all code you submit during this course.\nAnd now that your code is cleaned up, you should run it! The Source button in the top right corner will run all code in the open file. Running the code produces something like this:\n\n\n\n\n\n\nNot too shabby! That’s an interactive, dynamic map showing the location of Baruch College obtained by parsing the Baruch Wikipedia page, getting the GPS coordinates of Baruch, downloading a map file, and locating Baruch on that map.\nChallenge: Adjust this code to show Hunter college instead of Baruch.\n\nIf you want even more feedback on writing good code, install the lintr package and use the associated RStudio add-in. Unlike styler, lintr won’t make changes automatically for you, but it will highlight much more subtle possible problems.4\n\nTo become a true “power user” of tools like R and python, you will need to become more familiar with the command line interface (CLI) and associated tools.5\nThe Software Carpentry Unix Shell Tutorial is a great introduction to shell usage. Check it out!\nNB: MacOS and Linux systems work quite similarly under the hood, as both descend from the Unix tradition. By contrast, Windows works somewhat differently. Learners whose personal machine runs Windows are encouraged to take advantage of the provided Linux-running virtual machines6 as they work through this section.\n\n\nNext week, we will use these tools to begin coding in earnest. If you’re feeling ambitious, go ahead and get started on Mini-Project #00."
  },
  {
    "objectID": "labs/lab01.html#r-and-rstudio",
    "href": "labs/lab01.html#r-and-rstudio",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "The primary programming language used in this course is R, one of the two most popular languages used in data science. R, like its predecessor the S language, is optimized for interactive, data-analytic work, in contrast with python, which is optimized for general purpose computing.\nR is a programming language and runtime; we will supplement it with RStudio, an Integrated Development Environment or, less formally, an editor. RStudio is the software where you will write the code and then the R runtime will execute it.\n\nStudents should first install R from https://cloud.r-project.org/.\n\n\nDon’t fear the 90’s web design! Click image for detailed installation instructions.\n\nAs of 2024-08-26, the most recent version of R is 4.4.1. Using the most current version of R will reduce the likelihood of issues later in the course.\n\nNext, download and install the RStudio IDE (desktop edition).\n\n\nClick image for detailed installation instructions.\n\nRStudio is highly configurable and I recommend taking advantage of all its built-in features. If you go to the Global Options menu (accessible under Tools), I recommend the following settings:\n\nGeneral: Uncheck “Restore .RData into workspace at startup”.\nGeneral: Set “Save workspace to .RData on exit” to “Never”\nCode / Editing: Set “Tab width” to 2\nCode / Editing: Check\n\n“Insert spaces for Tab”\nAuto-detect code indentation\nInsert matching parens / quotes\nUse native pipe operator\nAuto-indent code after paste\nVertically align arguments in auto-indent\nContinue comment when inserting new line\n\n\nCode / Display: Check\n\nShow line numbers\nShow margin (margin column should be 80)\n\n\nCode / Diagnostics: check all “R” diagnostics.\nAppearance: Pick a color theme you enjoy. (I’m partial to light text on a dark background)\n\nYou may wish to enable GitHub Copilot. I have little experience with GH Copilot, but it seems quite popular and is allowed in this course. It is not guaranteed to be accurate at all times - and “the AI told me to” is not a valid excuse if your code is wrong - but on balance, it should be useful.\n\nWe won’t use it this week, but you will need to install Quarto before starting on Mini-Project #00."
  },
  {
    "objectID": "labs/lab01.html#source-code-management",
    "href": "labs/lab01.html#source-code-management",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "git is a source-code management tool, used by developers to manage the code they write. If you’ve ever been part of a large project and struggled to coordinate all team members using the same version of a document, git exists to solve that problem.\nIf you don’t have git pre-installed, install either Git for Windows or the XCode Command Line Tools for MacOS. If not automatically prompted when you try to use git, the Mac install can be manually triggered by running xcode-select --install at a command line.\nIn this course, we will use three main functions of git:\n\n\nstaging: telling git, I want you to prepare to save a certain file\n\ncommitting: saving a set of related changes\n\npushing: copying your committed changes to a separate server for sharing and backup\n\nWhenever you write code you are happy with, you should use git to save it. Saving changes with git is cheap and easy - so do it regularly. You always want git to have a backup of good code in case you loose power, accidentally delete a file, break something in a way you’re not sure how to undo, etc..\nRStudio comes with powerful git integration. Once you have created a project, you should see a tab labelled “Git” in the top right corner of your IDE window that looks something like this:\n\nTo stage a file - prepare to save it - click the empty check box next to the file name. A new file shows a status of “?” - this is git saying “I’ve never seen this file before. Do you want me to track it for you?”. Later, when you make further changes to file you have already asked git to track, a status of “M” (for Modified) will be shown.\nOn its own staging a file does nothing. You also need to commit it for git to truly track it.1 The Commit button will commit all staged changes. When you make a commit, git requires a brief message summarizing the changes. There’s no particular formatting requirement to this message, but it should be something that future-you is able to easily understand. For instance, the commit message from the initial draft of this document reads as:\nInitial draft of Lab 01 (STA9750)\n\n- Installing R and RStudio\n- Git and GitHub\n- Leaflet Example for Styler\n\nTODO: Fuller shell explainers\nTODO: Link more git help\nWhen I read this, I know the purpose of the change I made (first line), the contents of that change (list), and parts that still need more work.\nFinally, after you save a change, it is only saved on your computer. The true power of git comes from its ability to copy changes and backups across machines. This gives you an easy way to store backups in case your computer dies and makes collaboration efficient and fun. git allows you to push and pull changes between machines in endlessly powerful (but sometimes complex) ways. For this course, we’ll keep things simple and only use GitHub to share code. We discuss GitHub in the next section.\nReference: We will not use all of the functionality of git in this course, but you should familiarize yourself with Chapters 1, 2, and 6 of the Git Book over the next two weeks.\n\nGitHub is an industry-standard code hosting and collaboration platform. In addition to hosting copies of code, GitHub provides web hosting, bug reporting, code review, continuous integration, documentation wikis, and discussion fora. You will explore GitHub in more detail starting in Mini-Project #00."
  },
  {
    "objectID": "labs/lab01.html#code-styling",
    "href": "labs/lab01.html#code-styling",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "A major theme of this course will be sharing and co-developing code with your classmates, both for peer feedback and for the course project. Code sharing is hard! Everyone writes code a little differently and what is clear to you may not be clear at all to your reader.\nTo make code sharing just a bit easier, we use tools to ensure all code shared in this course is consistently formatted. By using consistent formatting, you reduce the cognitive load on your reader, making it easier for them to focus on the ideas of your code, not how you chose to write it.\nA major strength of R is its huge number of user-contributed packages. These are “add-ins” which provide additional functionality not available in the basic version of R. As of 2024-08-26, there are over 21 thousand packages available on CRAN, the largest official repository of R packages. Beyond all those, there are thousands more packages available on other code hosting websites like GitHub.2\nWe will use the contributed styler package to format code in this course. Run the following command to automatically download and install the styler package:\n\ninstall.packages(\"styler\")\n\n(Use the clipboard icon on the right of code snippets to automatically copy code suitable for pasting into RStudio.)\nYou should see something like this:\n\nThe styler package has been downloaded and installed on your computer, but it is not yet “active” or “open” in R. In general, you will only need to download packages once, but you will need to load them each time you want to use them.3\nOpen a R file in RStudio and copy the following (ugly) code:\n\nif(!require(\"leaflet\")) install.packages(\"leaflet\")\nif(!require(\"tidyverse\")){\n    install.packages(\"tidyverse\")\n   }\n library(tidyverse)\n     library(rvest)\nlibrary(leaflet)\n\npAGE = read_html('https://en.wikipedia.org/wiki/Baruch_College')\n  pAGE |&gt; html_element(\".latitude\") |&gt; html_text2() -&gt; BaruchLatitude\n  baruch_longitude &lt;- pAGE |&gt; html_element(\".longitude\") |&gt; html_text2()\n  \n    BaruchLatitude &lt;- sum(as.numeric(strsplit(BaruchLatitude, \n                                     \"[^0123456789]\")[[1]]) * (1/60)^(0:2), na.rm=TRUE)\n baruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) * \n                             (1/60)^(0:2), na.rm=TRUE)\n  \nleaflet() %&gt;% addTiles() %&gt;% setView(-baruch_longitude, BaruchLatitude, zoom=17) %&gt;%\n    addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nYou don’t need to understand what this does just yet, but it’s hopefully clear that this is ugly code. Nothing is lined up properly, capitalization is erratic, and different coding styles are intermixed rather recklessly.\nNear the top of your RStudio pane, you will see a drop-down menu titled Addins. If you successfully installed styler above, one of the Addins choices will be “style active file.” Click this and the code will be cleaned up (a bit) resulting in something like this:\n\nif (!require(\"leaflet\")) install.packages(\"leaflet\")\nif (!require(\"tidyverse\")) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(leaflet)\n\npAGE &lt;- read_html(\"https://en.wikipedia.org/wiki/Baruch_College\")\npAGE |&gt;\n  html_element(\".latitude\") |&gt;\n  html_text2() -&gt; BaruchLatitude\nbaruch_longitude &lt;- pAGE |&gt;\n  html_element(\".longitude\") |&gt;\n  html_text2()\n\nBaruchLatitude &lt;- sum(as.numeric(strsplit(\n  BaruchLatitude,\n  \"[^0123456789]\"\n)[[1]]) * (1 / 60)^(0:2), na.rm = TRUE)\nbaruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) *\n  (1 / 60)^(0:2), na.rm = TRUE)\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(-baruch_longitude, BaruchLatitude, zoom = 17) %&gt;%\n  addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nIt’s far from perfect - and we will discuss the many issues in this example throughout the course - but it’s better! At a minimum, you should make sure to run styler like this on all code you submit during this course.\nAnd now that your code is cleaned up, you should run it! The Source button in the top right corner will run all code in the open file. Running the code produces something like this:\n\n\n\n\n\n\nNot too shabby! That’s an interactive, dynamic map showing the location of Baruch College obtained by parsing the Baruch Wikipedia page, getting the GPS coordinates of Baruch, downloading a map file, and locating Baruch on that map.\nChallenge: Adjust this code to show Hunter college instead of Baruch.\n\nIf you want even more feedback on writing good code, install the lintr package and use the associated RStudio add-in. Unlike styler, lintr won’t make changes automatically for you, but it will highlight much more subtle possible problems.4"
  },
  {
    "objectID": "labs/lab01.html#extra-welcome-to-shell",
    "href": "labs/lab01.html#extra-welcome-to-shell",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "To become a true “power user” of tools like R and python, you will need to become more familiar with the command line interface (CLI) and associated tools.5\nThe Software Carpentry Unix Shell Tutorial is a great introduction to shell usage. Check it out!\nNB: MacOS and Linux systems work quite similarly under the hood, as both descend from the Unix tradition. By contrast, Windows works somewhat differently. Learners whose personal machine runs Windows are encouraged to take advantage of the provided Linux-running virtual machines6 as they work through this section."
  },
  {
    "objectID": "labs/lab01.html#looking-ahead",
    "href": "labs/lab01.html#looking-ahead",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "Next week, we will use these tools to begin coding in earnest. If you’re feeling ambitious, go ahead and get started on Mini-Project #00."
  },
  {
    "objectID": "labs/lab01.html#footnotes",
    "href": "labs/lab01.html#footnotes",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis two stage process is a bit cumbersome for the first stage of a small project, but it quickly becomes incredibly valuable. Instead of saving everything every time, there is great power in only saving “good” or “finished” changes to a large project, while leaving work-in-progress elsewhere unsaved. You probably won’t need this level of control until you get to the course project, but it’s better to have it than not.↩︎\nIf you are interested in bioinformatics, the Bioconductor project develops incredible open-source R packages.↩︎\nWhile this may feel cumbersome, it’s really not dissimilar to any other software you use (or R itself). You need to download it once, but you need to open it each time you intend to use it. There’s no harm in re-downloading–free software!–but it wastes time and bandwidth. Since we benefit so much from the free-software community, the very least we can do is not run up their internet bills unnecessarily.↩︎\nSome of the issues identified by lintr may be false positives, but the false positive rate is quite low, especially for the sort of procedural code that is the focus of this course. You should default to trying to appease lintr, but feel free to use the course discussion board for any questions.↩︎\nAs an added benefit, use of the CLI also makes you look like a 90s movie hacker to all your friends.↩︎\nSee the Course Resources page.↩︎"
  },
  {
    "objectID": "labs/lab11.html",
    "href": "labs/lab11.html",
    "title": "STA 9750 Week 11 In-Class Activity: HTML Import",
    "section": "",
    "text": "Week 11 Slides\n\n\n\n\n\n\nCUNY Mapping Code\n\n\n\n\n\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(leaflet)\n\nCUNYs &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_City_University_of_New_York_institutions\") |&gt; \n    html_element(\"tbody\") |&gt;\n    html_elements(\"tr td:nth-child(2)\") |&gt;\n    html_elements(\"a\")\n\nCUNYs &lt;- data.frame(name = CUNYs |&gt; html_text(),\n                    link = CUNYs |&gt; html_attr(\"href\")\n)\n\nget_cuny_gps &lt;- function(url){\n    COORDS &lt;- read_html(url) |&gt; html_element(\".geo\") |&gt; html_text() |&gt; str_split_1(\";\")\n    LAT &lt;- as.numeric(COORDS[1])\n    LON &lt;- as.numeric(COORDS[2])\n    list(LAT=LAT, LON=LON)\n}\n\nCUNYs &lt;- CUNYs |&gt; \n    mutate(link = paste0(\"https://en.wikipedia.org/\", link)) |&gt;\n    rowwise() |&gt;\n    mutate(gps = list(get_cuny_gps(link))) |&gt;\n    unnest_wider(gps)\n\nMAP &lt;- leaflet() |&gt; \n    addTiles() |&gt;\n    addMarkers(CUNYs$LON, \n               CUNYs$LAT, \n               popup=CUNYs$name, \n              options = popupOptions(closeOnClick=FALSE))\n\nMAP"
  },
  {
    "objectID": "labs/lab13.html",
    "href": "labs/lab13.html",
    "title": "STA 9750 Week 13 In-Class Activity: Predictive Modeling",
    "section": "",
    "text": "Week 13 Slides"
  },
  {
    "objectID": "labs/lab12.html",
    "href": "labs/lab12.html",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "",
    "text": "Week 12 Slides"
  },
  {
    "objectID": "labs/lab12.html#regular-expression-practice",
    "href": "labs/lab12.html#regular-expression-practice",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "Regular Expression Practice",
    "text": "Regular Expression Practice\nComplete the following exercises using functionality from the stringr package.\n\nIn the following sentence, extract all plural nouns1:\n\n\ntodo &lt;- \"Yesterday, I needed to buy four cups of flour, a piece of Parmesan cheese, two gallons of ice cream, and a six-pack of bottled (non-alcoholic) beers.\"\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(stringr)\ntodo &lt;- \"Yesterday, I needed to buy four cups of flour, a piece of Parmesan cheese, two gallons of ice cream, and a six-pack of bottled (non-alcoholic) beers.\"\nstr_extract_all(todo, \" [A-Za-z]+s[. ]\", simplify=TRUE)\nlibrary(stringr)\ntodo &lt;- \"Yesterday, I needed to buy four cups of flour, a piece of Parmesan cheese, two gallons of ice cream, and a six-pack of bottled (non-alcoholic) beers.\"\nstr_extract_all(todo, \" [A-Za-z]+s[. ]\", simplify=TRUE)\n\n\n\n\n\n\n\nIn the following sentence, compute the total number of fruits on my shopping list:\n\n\nshopping &lt;- \"Today, I need to purchase 3 apples, 5 limes, and 2 lemons.\"\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(stringr)\nshopping &lt;- \"Today, I need to purchase 3 apples, 5 limes, and 2 lemons.\"\nsum(as.numeric(str_extract_all(shopping, \"\\\\d+\", simplify=TRUE)))\nlibrary(stringr)\nshopping &lt;- \"Today, I need to purchase 3 apples, 5 limes, and 2 lemons.\"\nsum(as.numeric(str_extract_all(shopping, \"\\\\d+\", simplify=TRUE)))\n\n\n\n\n\n\n\nThe following text is adapted from the Taylor Swift wikipedia page, with some changes made to the punctuation to make things easier.\n\n\nTaylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles ‘Teardrops on My Guitar’, ‘Love Story’, and ‘You Belong with Me’ were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, ‘We Are Never Ever Getting Back Together’. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs ‘Shake It Off’, ‘Blank Space’, and ‘Bad Blood’. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single ‘Look What You Made Me Do’. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs ‘Cruel Summer’, ‘Cardigan’, ‘Willow’, ‘Anti-Hero’, ‘All Too Well’, and ‘Is It Over Now?’. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award, 40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\n\nHow many times does Taylor Swift’s last name appear?\n\n\n\n\n\n\n\n\n\n\n\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_count(swift, \"Swift\")\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_count(swift, \"Swift\")\n\n\n\n\n\n\n\nIn the above quote, how many different years (strings of exactly 4 digits) appear?\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_count(swift, \"\\\\d{4}\")\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_count(swift, \"\\\\d{4}\")\n\n\n\n\n\n\n\nExtract the names of all songs mentioned in the biography above. (Note that song names are surrounded by single quotes.)\nYou will need to use a lazy regular expression.\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_extract_all(swift, \"'.*?'\", simplify=TRUE)\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_extract_all(swift, \"'.*?'\", simplify=TRUE)"
  },
  {
    "objectID": "labs/lab12.html#scraping-practice-i-cocktail-recipies-part-2",
    "href": "labs/lab12.html#scraping-practice-i-cocktail-recipies-part-2",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "Scraping Practice I: Cocktail Recipies (Part 2)",
    "text": "Scraping Practice I: Cocktail Recipies (Part 2)\nLast week, we began to scrape Hadley’s Cocktails with an (eventual) goal of creating a “spreadsheet” of recipes by ingredients.\nWe found the following:\n\nlibrary(rvest)\nBASE_URL &lt;- \"https://cocktails.hadley.nz/\"\n\nPAGES &lt;- read_html(BASE_URL) |&gt; \n    html_elements(\"nav a\") |&gt; \n    html_attr(\"href\")\n\nread_article &lt;- function(article){\n    title &lt;- article |&gt; html_element(\"h2\") |&gt; html_text()\n    ingredients &lt;- article |&gt; html_elements(\"li\") |&gt; html_text()\n    \n    data.frame(title=title, ingredient=ingredients)\n}\n\nread_page &lt;- function(stub){\n    URL &lt;- paste0(BASE_URL, stub)\n    COCKTAILS &lt;- read_html(URL) |&gt; html_elements(\"article\")\n    \n    map_df(COCKTAILS, read_article)\n}\n\nRECIPES_LONG &lt;- map_df(PAGES, read_page)\n\nTake this output and use stringr and tidyr to complete the transition to a well-formated “wide” set of recipies.\n\nClean up the title column.\nSplit the ingredient column into three new columns:\n\nAmount\nUnit\nIngredient Name\n\nThe following functions may be useful to you:\n\n\nget_number_part &lt;- function(x){\n    library(dplyr)\n    library(stringr)\n    x |&gt;\n        str_replace_all(\"[A-Za-z,']\", \"\") |&gt;\n        str_trim() |&gt;\n        case_match(\n            \"1\" ~ 1, \n            \"2\" ~ 2,\n            \"2½\" ~ 2.5,\n            \"1½\" ~ 1.5,\n            \"½\" ~ 0.5, \n            \"¾\" ~ 0.75,\n            \"1¾\" ~ 1.75,\n            \"¼\" ~ 0.25,\n            \"1¼\" ~ 1.25,\n            \"3\" ~ 3,\n            \"4\" ~ 4, \n            \"5\" ~ 5, \n            \"6\" ~ 6, \n            \"7\" ~ 7, \n            \"8\" ~ 8\n        )\n}\n\nget_cocktail_unit &lt;- function(x){\n    library(dplyr)\n    library(stringr)\n    case_when(\n        str_detect(x, \"oz\") ~ \"oz\",\n        str_detect(x, \"dash\") ~ \"dash\",\n        str_detect(x, \"drop\") ~ \"drop\",\n        str_detect(x, \" t \") ~ \"t\", \n        str_detect(x, \"chunk\") ~ \"chunk\",\n        str_detect(x, \"leaves\") ~ \"leaves\",\n        str_detect(x, \" cm \") ~ \"cm\"\n    )\n}\n\nYou will need to create a third helper function to pull out the actual ingredient name. You can implement this with a (somewhat complex) str_replace_all.\nYou should wind up with a table that looks something like\n\n\n\nCocktail\nIngredient\nUnit\nAmount\n\n\n\n\nBachelor\nrum, dark\noz\n1\n\n\nBachelor\nMeletti\noz\n1\n\n\n\n\nCombine the ingredient and unit columns so that Meletti and oz in two separate columns becomes Meletti (oz) in a single column.\nUse a pivot_* function to create a new wide table with each ingredient as a column.\nWhich pivot operation do you want to use here? How should the empty cells be treated?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nget_ingredient_name &lt;- function(x){\n    str_remove_all(x, \"[0-9¼½¾]|( oz )|( dashes )|( dash )|( drops )|( drop )|( t )|( chunks )|( chunk )|( leaves )|( leaf )|( cm )\")\n}\n\nRECIPES_LONG |&gt;\n    # Remove duplicates from import process\n    # When we import each ingredient page, we get duplicates\n    # as drinks are listed on multiple ingredient pages.\n    distinct() |&gt;\n    mutate(title  = str_trim(title), \n           amount = get_number_part(ingredient), \n           unit  = get_cocktail_unit(ingredient), \n           ingredient = get_ingredient_name(ingredient)) |&gt;\n    # For some ingredients, e.g. a lemon twist, the implied\n    # but unstated quantity is 1\n    mutate(amount = case_when(\n        is.na(amount) ~ 1, \n        TRUE ~ amount), \n        ingredient = str_to_title(ingredient)) |&gt;\n    rename(Cocktail = title, \n           Amount = amount, \n           Unit = unit, \n           Ingredient = ingredient) |&gt;\n    mutate(Ingredient = case_when(\n        is.na(Unit) ~ Ingredient, # Handle unit-less ingredients\n        TRUE ~ paste0(Ingredient, \" (\", Unit, \")\"))) |&gt;\n    # Spanish Coffee lists orange liqueur twice, so let's \n    # add up repeated ingredients before pivoting.\n    # (I think this is the only one)\n    group_by(Cocktail, Ingredient) |&gt;\n    summarize(Amount = sum(Amount)) |&gt;\n    ungroup() |&gt;\n    pivot_wider(id_cols = Cocktail, \n                names_from = Ingredient, \n                values_from = Amount, \n                values_fill = 0) |&gt;\n    select(\"Cocktail\", sort(tidyselect::peek_vars()))"
  },
  {
    "objectID": "labs/lab12.html#scraping-practice-ii-quotes",
    "href": "labs/lab12.html#scraping-practice-ii-quotes",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "Scraping Practice II: Quotes",
    "text": "Scraping Practice II: Quotes\nNext, let’s analyze the website https://quotes.toscrape.com, a website designed to practice web-scraping.\nScrape the contents of that website–note that quotes continue for multiple pages–and answer the following questions. You can check most of these “by hand” but you need to compute your answers in code!\n\nHow many quotes are on this website (all pages)?\nHow many quotes are tagged “Death”?\nWhat is the longest quote (by number of characters)? The nchar function will be helpful.\nHow many quotes are by (or at least are attributed to) Albert Einstein?\nOf all authors quoted, who has the earliest (estimated) birthday?"
  },
  {
    "objectID": "labs/lab12.html#basic-statistical-modeling",
    "href": "labs/lab12.html#basic-statistical-modeling",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "Basic Statistical Modeling",
    "text": "Basic Statistical Modeling\nR has a number of powerful built-in statistical methods, many of which you will explore in other courses, e.g., lm and glm in your regression course. Time allowing, we’re going to explore an alternative approach to statistics using computational inference. While these techniques can be implemented by hand, we’re going to use the infer package.\nTime Allowing:\n\nDiscussion of computational inference\nDemonstration with infer package."
  },
  {
    "objectID": "labs/lab12.html#footnotes",
    "href": "labs/lab12.html#footnotes",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile English pluralization rules are tricky, you can just find the words ending with an s.↩︎"
  },
  {
    "objectID": "labs/lab09.html",
    "href": "labs/lab09.html",
    "title": "STA 9750 Week 9 In-Class Activity: Data Import",
    "section": "",
    "text": "Week 9 Slides"
  },
  {
    "objectID": "miniprojects/mini04.html",
    "href": "miniprojects/mini04.html",
    "title": "STA 9750 Mini-Project #04: Exploring Recent US Political Shifts",
    "section": "",
    "text": "\\[\\newcommand{\\P}{\\mathbb{P}} \\newcommand{\\E}{\\mathbb{E}}\\]"
  },
  {
    "objectID": "miniprojects/mini04.html#introduction",
    "href": "miniprojects/mini04.html#introduction",
    "title": "STA 9750 Mini-Project #04: Exploring Recent US Political Shifts",
    "section": "Introduction",
    "text": "Introduction\nWelcome to Mini-Project #04! Following the 2024 US Presidential election, the New York Times published a highly influential map depicting a national political shift to the right. This map, and associated commentary, portended a national “vibe shift”, which has already (arguably) been reflected throughout corporate, educational, and government sectors. In this mini-project, you will explore this national political shift, determining the extent (if any) it may have been over- or under-exaggerated and developing appropriate visualizations.\nFor this mini-project, you will pay the roll of a partisan “Talking Head” aligned with one of the two major US political parties. You will find facts supporting your party’s narrative and prepare related visualizations. For this project, you are encouraged (but not required) to adopt the opposite side of your own political beliefs.1 For example, if you adopt a Republican perspective, you may choose to highlight the fact that 89% of counties nationwide voted more Republican than in 2020; if you adopt a Democratic perspective, you may retort that “land doesn’t vote” and that a population weighted measure of national shift shows a far smaller effect.\nNote that, compared to previous mini-projects, the scope of this project is relatively smaller: in light of this, and the more advanced skills you have spent the past 3 months developing, this mini-project should be the least difficult of the course. At this point in the course, you should be spending the majority of your out-of-class hours on your Course Project.\nThis mini-project completes our whirlwind tour of several different forms of data-driven writing:\n\nWhite-Papers and Policy Briefs (MP#01)\nQuantifying Successful Public Policy Endeavors (MP#02)\nUsing Data to Supplement the Creative Process (“assistive intelligence”, MP#03)\nUsing Data to Support a Pre-Ordained Conclusion (this mini-project)\n\nThere are, of course, many other ways that data can be used to generate and communicate insights, but hopefully this “hit parade” has exposed you to many of the ways that you can use data to evaluate complex qualitative and quantitative claims outside of a binary classroom “correct/incorrect” structure. The tools of quantitative analysis and communication you have developed in this course can be used in essentially infinite contexts– we have only scratched the surface–and I’m excited to see what you do in the remainder of this course, in your remaining time at Baruch, and in your future careers.\nStudent Responsbilities\nRecall our basic analytic workflow and table of student responsibilities:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn this mini-project, you are in charge of the whole pipeline, from TBD to TBD. The rubric below evaluates your work on all aspects of this project.\nRubric\nSTA 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.\nCode completes all instructor-provided tasks satisfactorially.\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three or ore instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nReport exhibits particularly creative insights drawn from thorough student-initiated analyses.\n\n\nFormatting & Display\n\nTables and figures are full ‘publication-quality’.\nReport includes at least one animated visualization designed to effectively communicate findings.\n\n\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation.\nFigures are ‘publication-quality’, with suitable axis labels, well-chosen structure, attractive color schemes, titles, subtitles, and captions, etc.\n\n\nTables are well-formatted, but still have room for improvement.\nFigures are above ‘exploratory-quality’, but do not reach full ‘publication-quality’.\n\n\nTables lack significant ‘polish’ and need improvement in substance (filtering and down-selecting of presented data) or style.\nFigures are suitable to support claims made, but are ‘exploratory-quality’, reflecting minimal effort to customize and ‘polish’ beyond ggplot2 defaults.\n\n\nUnfiltered ‘data dump’ instead of curated table.\nBaseline figures that do not fully support claims made.\n\nReport includes interactive (not just animated) visual elements.\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nData import is fully-automated and efficient, taking care to only download from web-sources if not available locally.\nData is imported and prepared effectively, in an automated fashion with minimal hard-coding of URLs and file paths.\nData is imported and prepared effectively, though source and destination file names are hard-coded.\nData is imported in a manner likely to have errors.\nData is hard-coded and not imported from an external source.\nReport uses additional data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp04.qmd so the rendered document can be found at docs/mp04.html in the student’s repository and served at the URL:2\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp04.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/michaelweylandt/STA9750-2025-SPRING/issues/new .\n\nTitle the issue STA 9750 &lt;GITHUB_ID&gt; MiniProject #04 and fill in the following text for the issue:\n\nHi @michaelweylandt!\n\nI've uploaded my work for MiniProject #**04** - check it out!\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp04.html\n\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation."
  },
  {
    "objectID": "miniprojects/mini04.html#mini-project",
    "href": "miniprojects/mini04.html#mini-project",
    "title": "STA 9750 Mini-Project #04: Exploring Recent US Political Shifts",
    "section": "Mini-Project #04: Exploring Recent US Political Shifts",
    "text": "Mini-Project #04: Exploring Recent US Political Shifts\nFor this project, we are going to need to collect election result data. As of the time I’m writing this project, it’s surprisingly hard to find free, clean county-level election data, so we are going to use our new web-scraping skills to extract this data from Wikipedia.\n\nHigh-quality R packages exist to help work with some of the data providers used in this mini-project. Despite this, you may not use these packages: the learning goal of this mini-project is to practice scraping and importing data from various web based services. Using R packages instead of scraping the data “by hand” will not provide this practice. By the same token, do not use alternative data sources. (That said, if you find better county level records of the 2024 election, please let me know. I found this surprisingly difficult data to locate.)\n\nCounty Shapes: US Census Bureau\nBegin by downloading a shapefile containing US county (and equivalent) boundaries from the US Census Bureau. Note that this file is available in three resolutions. I recommend the finest resolution you can to produce the most accurate graphics, but if your compute is struggling with high levels of detail, you should consider falling back to the coarser resolutions.\n\n\n\n\n\n\nTask 1: US County Shapefiles\n\n\n\nWrite code to download an appropriate US County shapefile from the Census Bureau website. To be responsible, your download code should:\n\nOnly download the file if it is not already present\nCreate a directory titled data/mp04 if one is not already present\nSave the file in data/mp04 and, if necessary, decompress it.\nUse built-in R functions like download.file\n\n\nSee previous mini-projects for guidance.\n\n\n2024 County-Level Election Results\nWe will next obtain county-level election results for each of the 50 US states. While most states make this data available via the Secretary of State or an Election Board, these sites are non-uniform and tricky to use in an automated fashion. Instead, we will take our results from Wikipedia. For each state, there is a Wikipedia page describing the 2024 US Presidential Election results in that state; e.g., New York.\nEach of these pages has a table of county-level results which we need to extract.\n\n\n\n\n\n\nTask 2: Acquire 2024 US Presidential Election Results\n\n\n\nUsing httr2 and rvest download county-level election results from Wikipedia for each of the 50 US States.\nThis is a moderately advanced web-scraping exercise, so I recommend you approach it by writing a function which does the following.\n\nTake a US state name as input\nConstruct an appropriate request using httr2::request and req_url_path()3\n\nPerforms the request, saving the result locally to avoid unnecessary repeated downloads\nExtracts all tables into a list object\nSelects only the table with a column titled County\n\nAppropriately cleans and parses the table\nAdds a column with the state name\n\nNote that you will have to deal with various additional minor irregularities not described above. For instance, at least one US State uses a term other than “county” to describe the relevant unit of government.\n\n\nThe code used to extract EIA State Energy Profiles in an early mini-project may be somewhat helpful here.\n2020 County-Level Election Results\nNext, modify your code to extract results for the 2020 election.\n\n\n\n\n\n\nTask 3: Acquire 2020 US Presidential Election Results\n\n\n\nModify your code from Task 2 to acquire 2020 US Presidential Election Results.\n\n\nCombine Data and Perform Initial Analyses\nAt this point, we have all of the data needed to complete this mini-project (though you are, as always, welcome to download additional data you find helpful.) Combine the three data files (county shapes, 2020 results, 2024 results) and use them to answer the following questions.\n\n\n\n\n\n\nTask 4: Initial Analysis Questions\n\n\n\nAnswer the following questions using the combined data sources. As always, as you do this initial analysis, use it as an opportunity to verify that your data import and cleaning was accurate.\n\nWhich county or counties cast the most votes for Trump (in absolute terms) in 2024?\nWhich county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\nWhich county or counties had the largest shift towards Trump (in absolute terms) in 2024?\nWhich state had the largest shift towards Harris (or smallest shift towards Trump) in 2024? (Note that the total votes for a state can be obtained by summing all counties in that state.)\nWhat is the largest county, by area, in this data set?\nWhich county has the highest voter density (voters per unit of area) in 2020?\nWhich county had the largest increase in voter turnout in 2024?\n\n\n\nReproduce NYT Figure\nHaving confirmed our data is mainly reliable, we are now ready to reproduce the NYT Figure that initially motivated this project.\nTo do so, you need to:\n\nCompute the shift (as a percentage of votes cast) rightwards for each county.\nModify the geometry file to put Hawaii and Alaska in a more reasonable position for visualization. This StackOverflow Answer has useful suggestions.\nDraw a map of the US with the modified geometry. 4\n\nCompute the center point (“centroid”) of each county.\nAdd an arrow for each county, located at its centroid. The length of the arrow should be proportional to the shift and the direction should indicate whether that shift was rightward or leftward.\n\nYou may need to play around with the parameters defining each arrow to create an attractive plot.\n\n\n\n\n\n\nTask 5: Reproduce NYT Figure\n\n\n\nReproduce the NYT County Shift figure using the Census shapefiles and Wikipedia-extracted election results.\n\n\nAdditional Data Analysis\n\n\n\n\n\n\nTask 6: Additional Analysis and Figure Creation\n\n\n\nFurther analyze this data and come up with three partisan “talking points” (and associated figures) to support “your side”.\nYour analysis should include at least two computationally-intensive statistical tests, e.g., “did the median county shift by more than 5%?”. You may implement these directly or with help of the infer package."
  },
  {
    "objectID": "miniprojects/mini04.html#deliverable-partisan-talking-points",
    "href": "miniprojects/mini04.html#deliverable-partisan-talking-points",
    "title": "STA 9750 Mini-Project #04: Exploring Recent US Political Shifts",
    "section": "Deliverable: Partisan Talking Points",
    "text": "Deliverable: Partisan Talking Points\nTaking the position of a “partisan hack”, analyze the 2024 election and argue that Trump’s victory was either a nationwide seismic shift portending a new era in American politics (if you adopt a Republican persona) or a narrow win made to look more meaningful than it actually was by accidents of geometry and political organization (if you adopt a Democratic persona).\nYou should write in the style of an “op-ed” or a television commercial script (your choice) attempting to influence national post-election discourse in support of your chosen side.\nHave some fun with this! You can be as shameless and over-the-top as you want. After all, it was a politician who (anecdotally) gave us the phrase:\n\n“Lies, Damned Lies, and Statistics”\n\nand a (rather controversial) journalist who gave us\n\nHow to Lie with Statistics"
  },
  {
    "objectID": "miniprojects/mini04.html#extra-credit-opportunities",
    "href": "miniprojects/mini04.html#extra-credit-opportunities",
    "title": "STA 9750 Mini-Project #04: Exploring Recent US Political Shifts",
    "section": "Extra Credit Opportunities",
    "text": "Extra Credit Opportunities\nThere are no structured opportunities for extra credit on this mini-project beyond those stated in the rubric. As always, evaluators should assign appropriate extra credit (no more than 5 points total) for particularly creative insights, use of additional data sources, attractive figures, etc.\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "miniprojects/mini04.html#footnotes",
    "href": "miniprojects/mini04.html#footnotes",
    "title": "STA 9750 Mini-Project #04: Exploring Recent US Political Shifts",
    "section": "Footnotes",
    "text": "Footnotes\n\nIt is well-documented that political partisans subject their opponents to far more rigorous critique than their co-partisans. By adopting a point-of-view opposite your own, you will not only (hopefully) become more sympathetic to and informed about alternative political views, but you will also perform a more rigorous analysis. Still, since I do not know and do not want to know your true political views, this is only a suggestion and cannot be enforced.↩︎\nThroughout this section, replace &lt;GITHUB_ID&gt; with your GitHub ID from Mini-Project #00, making sure to remove the angle brackets. Note that the automated course infrastructure will be looking for precise formatting, so follow these instructions closely.↩︎\nNote that Wikipedia allows spaces in URLs here, so you can use https://en.wikipedia.org/wiki/2024 United States presidential election in New York and it will be automatically corrected to https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_New_York.↩︎\nA map with lines for each county may be a bit too dense to read. You might find it more visually appealing to take the “union” of counties to get states for creating the base map layer.↩︎"
  },
  {
    "objectID": "miniprojects/mini03.html",
    "href": "miniprojects/mini03.html",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "",
    "text": "Released to Students: 2025-03-20\nInitial Submission: 2025-04-23 11:45pm ET on GitHub and Brightspace\n\nPeer Feedback:\n\nPeer Feedback Assigned: 2025-04-24 on GitHub\nPeer Feedback Due: 2025-04-30 11:45pm ET on GitHub\n\n\n\nEstimated Time to Complete: 5 Hours\nEstimated Time for Peer Feedback: 1 Hour"
  },
  {
    "objectID": "miniprojects/mini03.html#introduction",
    "href": "miniprojects/mini03.html#introduction",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Introduction",
    "text": "Introduction\nWelcome to Mini-Project #03! In this project, you will dive into the world of music analytics in an attempt to create The Ultimate Playlist. Specifically, we will explore two data exports made available by Spotify to identify i) the most popular songs on the platform and ii) the characteristics of those songs. From this data, you will create the ultimate playlist. Note that, while this project is inspired by the work of the Great Sage, Mr Barney Stinson, pioneer of the “All Rise” playlist, you can create whatever type of playlist you want, as long as it is Ultimate.\nAlso note that this mini-project is intended to be a bit less demanding than Mini-Project #02. At this point in the course, you should be diving into your Course Project, which should consume the majority of your out-of-class time dedicated to this course for the remainder of the semester.\nStudent Responsbilities\nRecall our basic analytic workflow and table of student responsibilities:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn this project, I am no longer providing code to download and read the necessary data files. The data files I have selected for this mini-project are relatively easy to work with and should not provide a significant challenge, particularly after our in-class discussion of Data Import. See the modified rubric below which now includes a grade for data import.\nRubric\nSTA 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.\nCode completes all instructor-provided tasks satisfactorially.\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three or ore instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nReport exhibits particularly creative insights drawn from thorough student-initiated analyses.\n\n\nFormatting & Display\n\nTables and figures are full ‘publication-quality’.\nReport includes at least one animated visualization designed to effectively communicate findings.\n\n\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation.\nFigures are ‘publication-quality’, with suitable axis labels, well-chosen structure, attractive color schemes, titles, subtitles, and captions, etc.\n\n\nTables are well-formatted, but still have room for improvement.\nFigures are above ‘exploratory-quality’, but do not reach full ‘publication-quality’.\n\n\nTables lack significant ‘polish’ and need improvement in substance (filtering and down-selecting of presented data) or style.\nFigures are suitable to support claims made, but are ‘exploratory-quality’, reflecting minimal effort to customize and ‘polish’ beyond ggplot2 defaults.\n\n\nUnfiltered ‘data dump’ instead of curated table.\nBaseline figures that do not fully support claims made.\n\nReport includes interactive (not just animated) visual elements.\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nData import is fully-automated and efficient, taking care to only download from web-sources if not available locally.\nData is imported and prepared effectively, in an automated fashion with minimal hard-coding of URLs and file paths.\nData is imported and prepared effectively, though source and destination file names are hard-coded.\nData is imported in a manner likely to have errors.\nData is hard-coded and not imported from an external source.\nReport uses additional data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp03.qmd so the rendered document can be found at docs/mp03.html in the student’s repository and served at the URL:1\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp03.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/michaelweylandt/STA9750-2025-SPRING/issues/new .\n\nTitle the issue STA 9750 &lt;GITHUB_ID&gt; MiniProject #03 and fill in the following text for the issue:\n\nHi @michaelweylandt!\n\nI've uploaded my work for MiniProject #**03** - check it out!\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp03.html\n\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation."
  },
  {
    "objectID": "miniprojects/mini03.html#mini-project",
    "href": "miniprojects/mini03.html#mini-project",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Mini-Project #03: Creating the Ultimate Playlist",
    "text": "Mini-Project #03: Creating the Ultimate Playlist\nData Acquisition\nWe will use two Spotify data exports:\n\nA data set of songs and their characteristics\nAn export of user-created playlists\n\nInterestingly, Spotify no longer makes these datasets available directly, but nothing ever leaves the internet and we can use mirrors of the original data posted by other data scientists.\nSong Characteristics\nFirst, we can download the song properties data from the mirror posted by GitHub user gabminamendez. Download the data and import it into R.\n\n\n\n\n\n\nTask 1: Song Characteristics Dataset\n\n\n\nWrite a function called load_songs to\n\ndownload the Spotify song analytics dataset (if needed) from https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\n\nread it into R.\n\nYour function should return a well-formatted data frame.\nTo be responsible, your download code should:\n\nOnly download the file if it is not already present\nCreate a directory titled data/mp03 if one is not already present\nSave the file in data/mp03 and, if necessary, decompress it.\nUse built-in R functions like download.file\n\n\nSee prior mini-projects for examples of responsible downloading code.\n\n\nThe artists column of this data set is a bit oddly formatted: it contains multiple artists in a “list-type” format: e.g., the song “Blinding Lights” has artists as ['The Weeknd'] and the song “Uptown Funk (feat. Bruno Mars)” has artists as ['Mark Ronson', 'Bruno Mars']. The following code will split the artists across multiple rows, yielding e.g., two rows for Uptown Funk, one each for Mark Ronson and Bruno Mars.\n\nlibrary(tidyr)\nlibrary(stringr)\nclean_artist_string &lt;- function(x){\n    str_replace_all(x, \"\\\\['\", \"\") |&gt; \n        str_replace_all(\"'\\\\]\", \"\") |&gt;\n        str_replace_all(\" '\", \"\")\n}\nSONGS |&gt; \n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)\n\nAdapt it as needed for your analysis.\nPlaylists\nNext, we’ll download the Spotify Million Playlist dataset from GitHub user DevinOgrady. Because this dataset is large, DevinOgrady has uploaded it as a series of JSON files in his spotify_million_playlist_dataset repository. Write a function to download all files from this repository (data1 directory), store them locally, and read them into R. As above, your download function should be responsible (only downloading the data once as needed).\n\n\n\n\n\n\nTask 2: Playlist Dataset\n\n\n\nWrite a function called load_playlists to\n\ndownload the Spotify million playlist dataset (if needed);\nread all files into R; and\nconcatenate them into a list object.\n\nAs before, your download code should be “responsible” and avoid unnecessary duplicate downloads. (If your code is not ‘responsible’, GitHub may put a short-term block up to stop you from making too many requests. Be careful!)\nYou may not hard-code invidual file names. You need to use a loop-type construct and create individual file names programmatically.\n\n\nUnlike the song characteristics data, this is hierarchical data, not trivially represented as a tidy data frame. To proceed we will need to process the playlists data into a more standard format. In particular, we want a table of the following columns:\n\nPlaylist Name (playlist_name)\nPlaylist ID (playlist_id)\nPlaylist Position (playlist_position)\nPlaylist Followers (playlist_followers)\nArtist Name (artist_name)\nArtist ID (artist_id)\nTrack Name (track_name)\nTrack ID (track_id)\nAlbum Name (album_name)\nAlbum ID (album_id)\nDuration (duration)\n\nwhere each row is one “track” from a playlist. (Certain songs will be repeated because they appear on multiple playlists.)\n\n\n\n\n\n\nTask 3: ‘Rectangle’ the Playlist Data\n\n\n\nUsing functions from the tidyr, purrr, and dplyr packages, convert the playlist data from Task 2 into the rectangular format described above.\nTo clean up the ID columns, you can use the following function to strip the spotify:type: prefix.\n\nstrip_spotify_prefix &lt;- function(x){\n    library(stringr)\n    str_extract(x, \".*:.*:(.*)\", group=1)\n}\n\n\n\nInitial Exploration\nNow that your data is imported and cleaned, it is time to begin exploring it and seeing how comprehensive it is. (Note that these exports were created at different times, so they will not have fully overlapping coverage.)\n\n\n\n\n\n\nTask 4: Initial Exploration\n\n\n\n\nHow many distinct tracks and artists are represented in the playlist data?\nWhat are the 5 most popular tracks in the playlist data?\nWhat is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\nAccording to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\nWhich playlist has the longest average track length?\nWhat is the most popular playlist on Spotify?\n\n\n\nIdentifying Characteristics of Popular Songs\n\n\n\n\n\n\nInner Join to Combine Data Sets\n\n\n\nFor the remainder of this assignment, I recommend using an inner_join to combine the playlist and song characteristic data sets. This will throw out a (potentially large) fraction the playlist data, but given the timing and construction of these data exports, it is the best we can do.\n\n\nNext, we will visually explore this data, with an eye towards finding characteristics of the most popular songs on Spotify.\n\n\n\n\n\n\nTask 5: Visually Identifying Characteristics of Popular Songs\n\n\n\nAnswer the following questions using one or more visualizations. Make sure your plots are “publication-quality”, with well-formatted axes, color schemes, etc.\n\n\nIs the popularity column correlated with the number of playlist appearances? If so, to what degree?\nFor the following questions, select a threshold that defines a “popular” song. There’s no ‘right’ answer here, but I would recommend finding a song that is right “at the line” of being popular per your threshold and seeing if you consider it “almost popular.” If it’s a very popular song, adjust your threshold lower; if it’s obscure, adjust your threshold higher.\n\nIn what year were the most popular songs released?\nIn what year did danceability peak?\nWhich decade is most represented on user playlists? (The integer division (%/%) operator may be useful for computing decades from years.)\nCreate a plot of key frequency among songs. Because musical keys exist in a ‘cycle’, your plot should use polar (circular) coordinates.\nWhat are the most popular track lengths? (Are short tracks, long tracks, or something in between most commonly included in user playlists?)\nPose and visually answer at least two more other exploratory questions.\n\n\n\nNote that you can answer many of these using dplyr functions only, but a learning objective of this mini-project is to practice creating informative visualizations, so you are required to include at least one plot for each of these questions. If helpful, you can validate your visual findings with a descriptive statistic: e.g., if your plot shows a positive correlation between track length and popularity, you can also compute this correlation numerically and include it in your text.\nBuilding a Playlist from Anchor Songs\nTo begin building your playlist, pick one or two “anchor” songs that you like. (You can adjust these if you don’t like how your playlist comes out.)\nYou can find songs that work well in a playlist with these songs using various heuristics:\n\nWhat other songs commonly appear on playlists along side this song?\nWhat other songs are in the same key2 and have a similar tempo? (This makes it easy for a skilled DJ to transition from one song to the next.)\nWhat other songs were released by the same artist?\nWhat other songs were released in the same year and have similar levels of acousticness, danceability, etc.?\n\n\n\n\n\n\n\nTask 6: Finding Related Songs\n\n\n\nImplement the four heuristics above plus one of your own to find potential songs that belong on a playlist with your anchor songs. You should identify at least 20 candidates, at least 8 of which are not “popular” based on the threshold you set above.\n\n\nCreate Your Playlist\nGiven your anchor songs and playlist candidates, it is now time to filter down to a playlist of around 12 songs, suitably ordered. Using all the data available to you, determine an optimal playlist. Your playlist should include at least 2 songs you were not previously familiar with and at least 3 which are not “popular.”\nOnce you have created your playlist, visualize its evolution on the various quantitative metrics Spotify provides. Does it, per Stinson, ‘rise and fall’ or is it ‘all rise’?\n\n\n\n\n\n\nTask 7: Curate and Analyze Your Ultimate Playlist\n\n\n\nCurate and Analyze the Ultimate Playlist per instructions above. In addition to its musical structure, make sure to consider thematic unity, e.g., songs about cars, and to give your playlist a creative name."
  },
  {
    "objectID": "miniprojects/mini03.html#deliverable-the-ultimate-playlist",
    "href": "miniprojects/mini03.html#deliverable-the-ultimate-playlist",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Deliverable: The Ultimate Playlist",
    "text": "Deliverable: The Ultimate Playlist\nNow that you have created the Ultimate Playlist, it is time to nominate it for the Internet’s Best Playlist award. Write a title, description, and “design principles” for your playlist. Make sure to include at least one visualization that argues why your playlist is Ultimate. Describe how you used statistical and visual analysis to create this playlist, taking pride to showcase lesser known pieces that are integral to your playlist structure."
  },
  {
    "objectID": "miniprojects/mini03.html#extra-credit",
    "href": "miniprojects/mini03.html#extra-credit",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Extra Credit",
    "text": "Extra Credit\nIn addition to the rubric-specified extra credit, extra credit may be given for the following:\n\nUp to 2 points: use of Quarto’s video support to embed recordings of tracks from the playlist.\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "miniprojects/mini03.html#footnotes",
    "href": "miniprojects/mini03.html#footnotes",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Footnotes",
    "text": "Footnotes\n\nThroughout this section, replace &lt;GITHUB_ID&gt; with your GitHub ID from Mini-Project #00, making sure to remove the angle brackets. Note that the automated course infrastructure will be looking for precise formatting, so follow these instructions closely.↩︎\nAs coded by the key column, 0 is C, 1 is C#, 2 is D, etc. For non-musicians, this just means that the two songs share some harmonic structure that makes it easy to tie them together.↩︎"
  },
  {
    "objectID": "miniprojects/mini00.html",
    "href": "miniprojects/mini00.html",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "",
    "text": "In lieu of traditional homework, this course has a set of four mini-projects, which will be assessed in two stages. In the first, you will complete a small data analysis project1; after submission of your analysis, it will be assigned to a classmate, who will evaluate it according to an instructor-provided rubric. This peer feedback stage is an opportunity to see how your classmates answered questions and to compare it to your own response. In doing so, you will learn to evaluate data science work product and will develop a critical eye that can be turned to your own work.\nThis mini-project, however, is a meta-mini-project, designed to help you set up the course infrastructure you will use for the four graded mini-projects.\nNB: Mini-Project #00 is not graded, but it is required. For STA 9750, it serves as the legally mandated Verification of Enrollment activity. If it is not completed on time, you may be involuntarily disenrolled from the course."
  },
  {
    "objectID": "miniprojects/mini00.html#stage-1-github-account-creation",
    "href": "miniprojects/mini00.html#stage-1-github-account-creation",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 1: GitHub Account Creation",
    "text": "Stage 1: GitHub Account Creation\nTo complete this course, you will need a free GitHub personal account, which you can create here. Please note that whatever account name you use will be public, so you need to define a pseudonym here if you choose to use one."
  },
  {
    "objectID": "miniprojects/mini00.html#stage-2-course-repo-creation",
    "href": "miniprojects/mini00.html#stage-2-course-repo-creation",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 2: Course Repo Creation",
    "text": "Stage 2: Course Repo Creation\n\nCreating GitHub Repo\nNow that you have created a GitHub account, log in and proceed to your dashboard at https://github.com. In the top right corner, click the + symbol and select “New Repository.”23\n\nCreate a new repository named STA9750-2025-SPRING with a suitable description.\n\nThis repo needs to be public. You do not need to select a README, .gitignore, or a license at this time.\nAfter you create your repo, you should see a page like this:\n\nNote the URL highlighted in the main box:\n**https://github.com/&lt;GITHUB_ID&gt;/STA9750-2025-SPRING.git**\nYou will need this in the next step.\n\n\nConnecting GitHub Repo to RStudio\nNow that you have set up an empty repo, you need to connect it to your personal machine and to RStudio. RStudio’s concept of projects roughly map to GitHub repos and that is what we will use here.\nOpen RStudio and click the project menu in the top right corner:\n\nFollow through the menu to click:\n\nNew Project\nVersion Control\nGit\n\nThis will take you to the following screen:\n\nCopy the .git URL from the previous step into Repository URL.4\n\n\nSecuring Connections to GitHub\nAn important aspect of source code management is access control to your code repository. While it’s typically no risk to make your code world-readable, you don’t want just anyone being able to add code to your repository. Traditionally, this type of access would be controlled with a username+password scheme, but GitHub has moved to a Access Token structure.\nThis process is a “one-time” task, documented in the Connect Section of the Happy Git with R book, but we’ll cover the highlights here.\n\nGit Credential Manager\nBefore starting, you will want to make sure you have installed the Git Credential Manager (GCM). If you used the Git for Windows bundle, you already have GCM installed. If you are on a Mac, you can download the GCM installer directly or install GCM via the GitHub Desktop Client.\n\n\nGitHub Personal Access Token\nNext, you need to create a Personal Access Token (PAT) for GitHub. You can think of this as a “special-purpose” password. Unlike your general account password, a PAT can be restricted to only perform certain activities or only for a limited time period.\nThe easiest way to do so is to run the following code:\n\nif(!require(\"usethis\")) install.packages(\"usethis\")\nusethis::create_github_token()\n\nOnce you have created a token, copy and save it on your computer. (Note that you only be able to copy it one time, so make sure you save it somewhere memorable.) Then run the following code to register the GitHub PAT on your computer:\n\nif(!require(\"gitcreds\")) install.packages(\"gitcreds\")\ngitcreds::gitcreds_set()\n\nIf that doesn’t work, you can also perform the same steps “by hand”:\nAfter logging in to GitHub via a web browser, visit https://github.com/settings/personal-access-tokens/new to begin the token creation process. Give the token a meaningful name and description and set an expiration date after (at least) the end of the semester.\nSet the “Repository Access” to “All repositories” and, under “Permissions &gt; Repository Permissions”, set “Contents” to “Read and Write”. This will now let anyone using your token read and write to all your repositories. After you create your token, you will be given only one opportunity to copy it. (Note that you can change permissions later, but you can only copy the token once.) Copy this and save it for later use. If you loose this token, you may need to generate a new one.\nWhen you make your first push to GitHub (as described below), use this token as your password. If everything is set up correctly, GCM will save this token and use it to authenticate you every time you push to GitHub. You should not need to paste this token every time.\n\n\n\n.gitignore\nThis course will involve many files, not all of which you will want to push to GitHub. Create a file named .gitignore and paste in the following to tell git to ignore certain files. (Note the leading . in the file name.)\n\n**/.quarto/*\n.Rproj.user\ndata/**\n**/*_cache/*\n*_files/*\n**/*_freeze/*\n**/*tsv*\n**/*csv*\n**/*xlsx*\n**/*zip\n**/*pdf\n.DS_Store\n.Rhistory\n/.quarto/*\n\nYou may already have a .gitignore file created automatically for you by RStudio. If so, simply open it (by clicking the file name in the Files pane of RStudio) and add the above lines at the bottom.\n\n\nAdditional git configuration\nYou will, from time to time, need to send somewhat large files to GitHub, primarily high-resolution figures. Since git is primarily designed for small-ish text files (code), this can be a bit tricky, but you can increase the maximum transferrable size by running the following command at the Terminal.\ngit config --global http.postbuffer 524288000\nThis only increases a limit - you might still run into problems with exceptionally large files, esp. data files if you do not set your .gitignore up properly - but it should avoid most problems.\nNext, run the following at the Terminal:\ngit config --global init.defaultBranch main\nThis will tell git to use the term “main” for your first branch. We won’t use branches in this course, but this can eliminate some issues.\n\n\ngit Check\nAt this point, run the following code to check your git setup:\n\nif(!require(\"usethis\")) install.packages(\"usethis\")\nusethis::git_sitrep()\n\nThis will print out a large amount of helpful information, but the important things to check are:\n\nAre your name and email set correctly?\nIs your GitHub user set correctly?\nDoes the token have the requisite “repo” scope?\nIs the “origin” set correctly for the GitHub project?\n\n\n\n\n\n\n\nDefault branch mismatch between local repo and remote.\n\n\n\n\n\nCertain older versions of git will use the default name master for the initial branch (though the command above should address this). If you get the message\n\nDefault branch mismatch between local repo and remote.\n\nfrom git_sitrep(), proceed to make your first commit (as described below) and then run the command\ngit branch -m main\nThis should eliminate the above issue. Note that this command needs to be run between making your first commit and pushing that first commit.\n\n\n\n\n\nInitial Push\nNow, to make sure everything is working, let’s save a basic README file and push it to GitHub. This is a plain text file with no particular structure.\nTo create it, click the new file button in RStudio (top left; piece of paper with a green plus) and select Text File. RStudio will open this file in the editor: type some basic content, e.g.,\nSubmission materials for STA 9750 at Baruch College. \n\nOwner: &lt;YOURNAME&gt;\n(It doesn’t matter what you push: whatever you type will be the default text appearing when someone visits your repo.)\nSave the file and open the Git pane in RStudio.\n\nCheck the box next to the README file to stage it for git.\n\nThen click the Commit button a type a brief message (Initial commit is fine).\n\nFinally, push the Push button. If everything works, you should see a screen like the below:\n\nTo confirm everything worked, return to the GitHub repo in your browser. You should see the text of your README file displayed at the bottom of the page."
  },
  {
    "objectID": "miniprojects/mini00.html#stage-3-personal-website-creation",
    "href": "miniprojects/mini00.html#stage-3-personal-website-creation",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 3: Personal Website Creation",
    "text": "Stage 3: Personal Website Creation\nNow that you created a place where you can push files to GitHub and have successfully pushed a basic README, it’s time to build a webpage using quarto.\nWe will need three pages to build a website:\n\nA configuration file, _quarto.yml, used to specify the look and layout of your website.\nAn index.qmd file used to create the homepage.\nA build script to create the website.\n\n\nConfiguration File\nOpen a new text file and save it as _quarto.yml. This is a configuration file used by quarto to control the layout of your site. For a barebones site, copy the following into _quarto.yml:\nproject:\n  type: website\n  output-dir: docs\n\nwebsite:\n  title: \"STA 9750 Submission Material\"\n  description:\n    Course Projects for STA 9750 at Baruch College\n  site-url: \"https://&lt;GITHUB_NAME&gt;.github.io/STA9750-2025-SPRING/\"\n  navbar:\n    background: primary\n    search: false\n    \nformat:\n  html:\n    theme: &lt;THEME&gt;\n    toc: false\n\neditor:\n  mode: source\nNote that the indentation pattern is important so copy this exactly.\nReplace &lt;GITHUB_NAME&gt; with your GitHub user name.\nFor &lt;THEME&gt;, visit the Bootswatch theme gallery and pick your preferred theme. Replace &lt;THEME&gt; with a lower case version of the theme name; if you want to use the Sandstone theme used for this course website, &lt;THEME&gt; will be sandstone (NB: lower case) without angle brackets.5 See, e.g., the _quarto.yml used for this course.\nOnce you have created this _quarto.yml, stage it (click the check mark) in RStudio’s git pane.\n\n\nindex.qmd\nNext, we’ll build your home page, conventionally called index.html. We will not write the HTML code by hand - it’s quite cumbersome - and will instead let quarto create it for us. Create another plain text file and save it as index.qmd.\nThis file will be divided into two parts, a header giving the metadata for the site, and a body, giving the content of the site.\nFirst write the header, separated by three horizontal bars (minus signs) above and below. For now, all you need to specify is a title:\n\n---\ntitle: \"YOUR TITLE GOES HERE\"\n---\n\nBelow the header, write the basic content of your website: a brief introduction of who you are.6 You can use markdown here for formatting. Basic text will suffice, but this is also a great opportunity to include things like a personal headshot, a link to a full resume, or similar.\nAs you work on this, click the “Render” button at the top of the editor pane to see what your site will look like.\n\n\nAdditional Set-Up - Rendered Code\nIn order to make sure that your R and quarto environment has been properly configured, it is necessary to include to include a bit of R code in your submission. At this point in the course, you are not required to write any R code of your own, so I’ve written some options here. Pick one to include in your submission - or modify them to do something else fun. If there’s something else you’d like to include, reach out to the instructor and I might be able to help you put something together.\nOnce you are happy with this landing page and have included on of these dynamic components (or similar), stage it and we’ll move on to building the website properly.\n\nLast Updated\nThe following code will add a “Last Updated:” footer at the bottom of your page.\n\n```{r}\n#| include: false\n1+1\n```\n--------------\nLast Updated: `r format(Sys.time(), \"%A %m %d, %Y at %H:%M%p\")`\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that the 1+1 block will not actually be printed because of the #| include: false flag, but it is necessary to force quarto to execute inline code blocks. Alternatively, you can add engine: knitr to the document header (where the title is specified). See Discussion at Github or the Engine Binding documentation.\n\n\n\n\nBaruch Map\nThe following adds a map with the location of Baruch College. You can change the GPS coordinates and text to another place of relevance to you.\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n\nif(!require(\"leaflet\")){\n    options(repos=c(CRAN=\"https://cloud.r-project.org\"))\n    install.packages(\"leaflet\")\n    stopifnot(require(\"leaflet\"))\n}\n\nbaruch_longitude &lt;- -73.98333\nbaruch_latitude  &lt;- +40.75028\n\nleaflet() |&gt;\n  addTiles() |&gt;\n  setView(baruch_longitude, baruch_latitude, zoom=17) |&gt;\n  addPopups(baruch_longitude, baruch_latitude, \n            \"I am a Master's student at &lt;b&gt;Baruch College&lt;/b&gt;!\")\n```\n\n\n\nResume Hosting\nIf you have a copy of your resume available as a PDF, TODO.\n\n\nTODO\n\n\n\nbuild_site.R\nFinally, open a new file - but now it’s an R script, not a text file, in RStudio. Copy the following into build_site.R:\n#!/usr/bin/env Rscript\nif(!require(\"quarto\")){\n    install.packages(\"quarto\")\n}\nlibrary(quarto)\nif(!quarto::quarto_binary_sitrep()){\n    stop(\"Something is wrong with your quarto installation.\")\n}\nquarto::quarto_render(\".\")\nsystem(\"git add docs/*\")\nif(!any(grepl(\"rstudio\", search()))){q(\"no\")}\nClick the Source button in the top-right corner of the editor pane to run this code. If everything works, it will build your website and automatically stage it. Stage build_site.R as well.\nFinally, Commit all these staged files and Push them to GitHub. You have now created a website and just need to turn on a web server so you can access it."
  },
  {
    "objectID": "miniprojects/mini00.html#stage-4-github-pages-deployment",
    "href": "miniprojects/mini00.html#stage-4-github-pages-deployment",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 4: GitHub Pages Deployment",
    "text": "Stage 4: GitHub Pages Deployment\nReturn to the GitHub repo you created; recall that the URL is something like:\nhttps://github.com/&lt;GITHUB_ID&gt;/STA9750-2025-SPRING/\nOpen the “Settings” menu and proceed to the “Pages” submenu. You should see a page that looks like this:\n\nUnder Build and Deployment, set the main branch to deploy and select the docs directory on that branch. Hit save and your website will go live!\nTo check your website is working, proceed to\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING\nIf everything works, you will see your site! (If you used the Render feature in RStudio, it should look familiar.)\nIf you get stuck, use the course discussion board to seek help from your classmates and, if necessary, the instructor."
  },
  {
    "objectID": "miniprojects/mini00.html#stage-5-submission",
    "href": "miniprojects/mini00.html#stage-5-submission",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 5: Submission",
    "text": "Stage 5: Submission\nOnce your site is live, you will submit it to the instructor in two ways:\n\nLog into the course discussion board (Piazza) and send me your GitHub name so I can link it to my gradebook.\nTag @michaelweylandt on GitHub to make sure I can access your repo.\n\nThese both must be completed to complete the assignment and verify enrollment.\n\nDiscussion Board (Piazza)\nFirst, send me a private message through the course discussion board with the following details:\n\nReal Name\nCUNY EmplID (8 digit ID code)\nCUNY email\nGitHub user name\n\nThis is the only place where you are required to connect your GitHub ID with your real name and CUNY credentials. I need this information to connect your public activity with my (private) gradebook and the CUNY system.\nIf all your information looks good, I might not reply through the discussion board. When I reply through GitHub, I’m acknowledging both parts of your submission.\n\n\nInstructor Tagging\nFinally, you’re going to contact me through GitHub: go to7\nhttps://github.com/michaelweylandt/STA9750-2025-SPRING/issues/new\nto open a new issue. Title the issue STA 9750 &lt;GITHUB_ID&gt; MiniProject #00 and fill in the following text for the issue:\n\nHi @michaelweylandt!\n\nI've created my STA 9750 website - check it out!\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/\n\n(Replace &lt;GITHUB_ID&gt; with your username throughout.)\n\nThis will send me a notification through GitHub and I will confirm that I can access your repository and website. You should also use the course helper scripts to verify you have opened a GitHub issue properly. For this project, simply run:\n\nsource(\"https://michael-weylandt.com/STA9750/load_helpers.R\")\nmp_submission_verify(0, \"&lt;GITHUB_ID&gt;\")\n\nIf you don’t do this, I may not be able toaccess your graded assignments when you submit them! I will confirm that I have your real ID verified as well."
  },
  {
    "objectID": "miniprojects/mini00.html#wrap-up",
    "href": "miniprojects/mini00.html#wrap-up",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nOnce I acknowledge receipt of your ID and website, you’re done with Mini-Project #00! You’ve built a website and are ready for the course to begin in earnest.\nMini-Projects #01-#04 will be submitted as separate pages in your website (different quarto documents) and hosted via GitHub pages for peer feedback. We will discuss that process in more detail after Mini-Project #00 is complete."
  },
  {
    "objectID": "miniprojects/mini00.html#hints",
    "href": "miniprojects/mini00.html#hints",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Hints",
    "text": "Hints\nIf you need help, the course discussion board should be your first stop.\nIf you want to personalize your website further, you can see how I have created mine on GitHub. Recall that the Markdown syntax used by quarto is summarized at https://www.markdownguide.org/basic-syntax/.\nYou may want to use the About Page functionality to improve the look of your home page. This gives you the ability to link to your social media (personal and professional) or personal blog, to include a professional headshot, etc.\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "miniprojects/mini00.html#footnotes",
    "href": "miniprojects/mini00.html#footnotes",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEarly in the course, I will ‘scaffold’ most of the analysis, leaving only some small steps for you to fill in. As the course progresses, the mini-projects will be more self-directed.↩︎\nAlternatively, simply go to https://github.com/new after logging in.↩︎\nNote that these images are taken from a previous offering of this course. Update to the current semester as appropriate.↩︎\nYou can leave the next two boxes blank or set a custom directory name and location. RStudio’s defaults are reasonable; the default directory name will simply be STA9750-2025-SPRING and it will be located in your home directory.↩︎\nHere and everywhere in this course, whenever you are replacing a placeholder of the form &lt;VALUE&gt; with an actual value, make sure to remove the angle brackets.↩︎\nIf you choose to complete the course using a pseudonym, make up something fun. If you are using your real name, this is a great place to state that you are a Baruch student, your expected graduation date, your field of employment (current or desired), and one or two personal facts. This, along with a LinkedIn page, will quickly become one of the first things that comes up when a potential employer searches your name, so make a good impression!↩︎\nThroughout this course, make sure to replace &lt;GITHUB_ID&gt; with your recently created GitHub ID, making sure to remove the angle brackets. Note that the automated course infrastructure will be looking for precise formatting, so follow these instructions closely.↩︎"
  },
  {
    "objectID": "preassigns/pa08.html",
    "href": "preassigns/pa08.html",
    "title": "STA 9750 Week 8 Pre Assignment: More Plots",
    "section": "",
    "text": "Due Date: 2025-03-26 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week, we will dive deeper into the world of data visualization, with a focus on tools for interactive (and animated) data visualization. Before doing so, let’s pause and consolidate everything we’ve done to date:\nOk - we’re now ready to move forward. This week, we will explore various technologies for interactive data visualization. These can be divided into two broad categories:\nGenerally, server-based approaches are more flexible (and a bit easier to implement) while browser-based approaches are more responsive and scalable. Since the browser work is done locally on the user’s computer (or phone or tablet), they are also cheaper and safer to run as there’s no need to have a server constantly responding to user input.\nThis week, we will explore a bit of each modality, though entire courses (and indeed entire careers) have been spent on both.\nIn the R ecosystem, the tool of choice for building server-based1 web applications is shiny.2 For this pre-assignment, you will work through Lessons 1 and 2 of the “Shiny Basics” web tutorial. (You do not need to do the “Next Steps” in Lesson 3, but you are of course welcome to.)\nAfter finishing these activities, complete the Weekly Pre-Assignment Quiz on Brightspace."
  },
  {
    "objectID": "preassigns/pa02.html",
    "href": "preassigns/pa02.html",
    "title": "STA 9750 Week 2 Pre Assignment: Getting Started with Markdown",
    "section": "",
    "text": "Due Date: 2025-02-05 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week, we are going to learn to use quarto, a data science publishing platform. quarto documents are written using Markdown, a light-weight mark-up language.12\nFor this week’s pre-assignment, complete this interactive Markdown tutorial, which should take you about 10 minutes. Once you’ve familiarized yourself with Markdown, take a look at the source code for this website and see how certain Markdown documents are rendered as web pages.\nDuring this week’s lab session, we will take particular advantage of:\nso make sure to pay attention to those parts of the tutorial.\nAfter you are done with the introduction to Markdown, log in to CUNY Brightspace and complete the Pre-Assignment 02 “Getting to Know You” quiz. As part of this quiz, you will be asked to attest that you successfully completed the Markdown tutorial."
  },
  {
    "objectID": "preassigns/pa02.html#footnotes",
    "href": "preassigns/pa02.html#footnotes",
    "title": "STA 9750 Week 2 Pre Assignment: Getting Started with Markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGet it? If you learn nothing else in this class, you will certainly learn that programmers love terrible puns. R itself is actually a pun as it was originally a free ‘knock-off’ of the S programming language developed by Ross and Rob.↩︎\nA mark-up language is a way of specifying the formatting applied to given text. It exists somewhere between “plain text” and a full document format like a .docx file. Other markup languages include HTML (hyper text markup language), rST (reStructured Text), LaTeX (used for scientific typesetting), and many others. Markdown is the simplest of these and the only one you will be required to write in this course. You will need to learn a bit of how HTML is structured and, if you are including math in your mini-project or final project submissions, a bit of LaTeX will go a long way.↩︎"
  },
  {
    "objectID": "preassigns/pa11.html",
    "href": "preassigns/pa11.html",
    "title": "STA 9750 Week 11 Pre Assignment: Intro to HTML",
    "section": "",
    "text": "Due Date: 2025-04-16 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nOur next topic is scraping data from web pages, that is data stored in HTML. Recall our “hierarchy” of data storage preferences:\nAs the course has progressed, we have seen or worked with data in many of these formats. This week, we move to the most difficult data source of this course: HTML.1\nHTML, short for HyperText Markup Language, is the lingua franca of the web. The vast majority of websites you visit or interact with are written using HTML. As such, HTML is ubiquitous in modern life. HTML is flexible, relatively easy to write by hand or programmatically, compressible, and has near universal built-in support on modern operating systems.\nUnfortunately, this ubiquity comes with a cost: because HTML is so universal, web browsers have been designed to “try their best” to read and render even improper HTML.2 In response, web professionals and amateurs have released ever worse HTML upon the world, continuing a vicious cycle. Modern programming practice has moved away from hand-written HTML in favor of tools like Markdown and Quarto, which allow “correct” HTML to be generated automatically. While this is a welcome trend, you will almost certainly still encounter malformatted HTML in your career, as nothing - however flawed - truly leaves the internet.\nBut, for now, we begin with relatively well-formatted HTML.\nRight-click in your browser and view the source code of this page - what you’re seeing is HTML. HTML consists of a hierarchically nested set of elements that look something like this:\nThere are three key pieces of this structure:\nThe power and flexibility of HTML comes from the fact that the contents of one element can include one or more additional elements. For instance, you might encounter an element like\nwhich might render as:\nHere the a tag is used to create hyperlinks, with the target specified by the href attribute. There are many more “standard” HTML tags in addition to those websites might define for their own use. For comprehensive documentation, see the Mozilla Developer Network (MDN) Documentation.\nWhen extracting data from a website, we will typically want to select all the elements of a certain tag or with a certain attribute: e.g., all cells of a table or all bolded paragraph headers. We can do so efficiently using “CSS Selectors”.\nCSS Selectors are a special language used to select multiple elements at once: the basic elements are as follows:\nWe will use these to tell R what elements to import from a web page. A well constructed selector statement can usually highlight exactly the data we hope to extract.\nFor now, however, you will practice using CSS Selectors from within your browser.\nRight click the the following link and add its to your bookmarks. Whenever you’re on a website, you can click that bookmark to open the CSS SelectorGadget.3\nUpon clicking, you will see a toolbar at the bottom of the page. If you type a CSS selector statement into that toolbar, it will highlight all elements on the page that match that selector. For now try a simple a and hit enter: you should see all links on the page highlighted. You can also try more advanced CSS selectors: li a will select all links (a) within list items (li) of the navigation bar at the top of the page.\nYou can also use SelectorGadget to create CSS Selectors. If you click several items that you want to select, SelectorGadget will attempt to create a suitable selector command. (You might need to Clear the input area before trying this.) For instance, try clicking a link in this text and seeing what SelectorGadget automatically selects for you. In this case, SelectorGadget comes up with a for all links on the page. If we want to exclude the links in the navigation bar, we can click them again, marking them in red and SelectorGadget will attempt to exclude them. Here, it creates a CSS selector that selects only links within the main body of the page. For our purposes, two clicks are enough, but you could extend this further. SelectorGadget isn’t perfect, but it’s often a very good starting guess.\nOpen the rvest Star Wars example page in a new tab and use SelectorGadget to select the 7 movie names in the main section. We want only the movie names and not the text below them. We also don’t want the clickable links in the sidebar. We will use this selector as our first example in class.\nNext, open the Wikipedia page listing all CUNY Colleges and confirm that the tbody selector selects the entirety of the main table. Note that if you use SelectorGadget here, you might get something like .jquery-tablesorter. For reasons we will discuss in class, this won’t work in R.\nFinally, open the Baruch College Wikipage and create a selector for just the GPS coordinates in the top right corner of the page. You should try to select just the coordinates themselves and not the text “Coordinates” preceding them.\nAs you explore this, it’s worth noting that Wikipedia is actually a rather complicated web-page. If you want to practice on simpler websites, I recommend starting here.\nAfter finishing this document, complete the Weekly Pre-Assignment Quiz on Brightspace."
  },
  {
    "objectID": "preassigns/pa11.html#footnotes",
    "href": "preassigns/pa11.html#footnotes",
    "title": "STA 9750 Week 11 Pre Assignment: Intro to HTML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe won’t cover reading data from PDFs in this course.↩︎\nIf you are of a certain age, you will remember an era when websites would work in one browser and not others. Proper HTML should work in all browsers, but each browser had its own way of handling malformatted HTML. Developers were, in essence, requiring users to use a piece of software that would automatically correct their mistakes. These were dark times…↩︎\nAll credit to Andrew Cantino at https://selectorgadget.com/. Use here inspired by the [rvest documentation]↩︎"
  },
  {
    "objectID": "preassigns/pa12.html",
    "href": "preassigns/pa12.html",
    "title": "STA 9750 Week 12 Pre Assignment: Strings and Things",
    "section": "",
    "text": "Due Date: 2025-04-30 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week, we begin to study the world of text data. While numerical data is reasonably straight-forward to deal with, text data is remarkably complex. A full discussion of text data requires understanding the vast world of human written language, but we will discuss enough of the major points to hopefully solve 95% of the challenges you will face in your career."
  },
  {
    "objectID": "preassigns/pa12.html#goals",
    "href": "preassigns/pa12.html#goals",
    "title": "STA 9750 Week 12 Pre Assignment: Strings and Things",
    "section": "Goals",
    "text": "Goals\nIn our quest to understand text data, we have two major goals:\n\nUnderstanding String Encodings and Unicode\nManipulating Strings with Regular Expressions\n\nBefore we get into these, let’s begin with a basic review of the character data type in R."
  },
  {
    "objectID": "preassigns/pa12.html#string-vectors",
    "href": "preassigns/pa12.html#string-vectors",
    "title": "STA 9750 Week 12 Pre Assignment: Strings and Things",
    "section": "String Vectors",
    "text": "String Vectors\nRecall that R works by default on vectors - ordered collections of the “same sort” of thing. R supports the following vector types:\n\nRaw for pure access to bytes without any additional meaning: rarely useful for pure data-analytic work, but commonly used to interact with binary file formats and with non-R software\nInteger: 32-bit signed integers, ranging from \\(-2^{30}\\) to \\(2^{30}-1\\). (If you have done low-level work before, you might ask where the extra bit went: it’s used for encoding NA values.)\nNumeric: 64-bit (double precision) floating point values, ranging from (approximately) \\(\\pm 10^{308}\\). The detailed behavior of numeric (often called double) data is beyond this course, but it is well documented elsewhwere.\nCharacter: the topic of today’s discussion.\n\nR makes no difference between a character - in the sense of a single letter - and a string: in particular, each element of a character vector is an (arbitrary length) string. Specialized functions are required for work at the true “single letter” scale. If you come from other languages, this behavior might be surprising, but it allows R to handle much of the complexity associated with characters automagically, which greatly simplifies data analysis.\nWhen speaking, we refer to R as using strings, even if R itself calls them character elements for historical reasons."
  },
  {
    "objectID": "preassigns/pa12.html#encoding",
    "href": "preassigns/pa12.html#encoding",
    "title": "STA 9750 Week 12 Pre Assignment: Strings and Things",
    "section": "Encoding",
    "text": "Encoding\nHow are strings represented on a computer? The answer has evolved over time, but the current state of the art - used by almost all non-legacy software - is based on the Unicode system and the UTF-8 encoding.\nThe Unicode system is comprised of two essential parts: - A numbered list of “letter-like” elements - Rules for manipulating those elements\nWhile this seems simple, it is anything but. The history of string representations in computers is a long and painful story of programmers repeatedly underestimating the complexity of the seemingly simple task of listing “all the letters.”\nThe Unicode consortium makes a long list of characters that computers should be able to represent: the most recent version of the Unicode standard includes 149,813 characters divided into 161 scripts. These include everything from the basic (Anglo-American) Latin alphabet to the Greek and Cyrillic alphabets to Chinese and Japanese characters to the undeciphered Linear A alphabet and Tengwar, the fictional script used in the Lord of the Rings novels. The Unicode standard also includes a wide set of Emoji (approximately 4000) and many “modifying” characters.\nTo each of these, the Unicode consortium assigns a code point : a numerical identifier. Even superficially similar characters may be assigned different code points to distinguish them: for example, “H” is code point U+0048 with the official description “Latin Capital Letter H” while “Η” is U+0397, “Greek Capital Letter Eta.”\nThe difference between these characters is essential to know how to manipulate them:\n\n\n\n\n\n\n\n\nUse the tolower function to lower-case each of these:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Unicode standard defines the lower case mapping of U+0048 as the Latin lower case h, while the lower case mapping of U+0397 as the Greek lower case eta, which looks something like a streched n. \nIn general, these mappings are incredibly complicated and depend not only on the specific code point, but also the set of translation rules being used. (Certain languages have different lower/upper mappings for what are otherwise the same letter.)\nWhile you don’t need to know all of this complexity, it is essential to know that it’s out there and to rely on battle-tested libraries to perform these mappings.\nUnicode is supplemented by the UTF-8 encodings, which controls how 0/1-bit strings are actually translated to code points. (Fonts then map code points to what you see on the screen.) UTF-8 is more-or-less back-compatible with other major encodings, so it’s a good default. When dealing with modern websites or public data sources, they almost always present their contents in a UTF-8 compatible encoding (if not UTF-8 proper) so you should be ok.\nA well-formatted website will state its encoding near the top of the page:\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nread_html(\"http://www.baruch.cuny.edu\") |&gt;\n    html_elements(\"meta[charset]\") |&gt;\n    html_attr(\"charset\")\n\n[1] \"UTF-8\"\n\n\nAdvice: Whenever possible, make sure you are using UTF-8 strings: if your data is not UTF-8, reencode it to UTF-8 as soon as possible. This will save you much pain."
  },
  {
    "objectID": "preassigns/pa12.html#string-manipulation",
    "href": "preassigns/pa12.html#string-manipulation",
    "title": "STA 9750 Week 12 Pre Assignment: Strings and Things",
    "section": "String Manipulation",
    "text": "String Manipulation\nOnce data is in R and encoded as UTF-8 Unicode points, we have several tools for dealing with strings. Your first port of call should be the stringr package.\nAll the functions of the stringr package start with str_ and take a vector of strings as the first argument, making them well suited for chained analysis.\nLet’s start with str_length which simply computes the length of each element. For the basic Latin alphabet, this more or less matches our intuition:\n\nlibrary(stringr)\nx &lt;- c(\"I\", \"am\", \"a\", \"student\", \"at\", \"Baruch.\")\nstr_length(x)\n\n[1] 1 2 1 7 2 7\n\n\nbut it can be tricky for strings that involve Unicode combining characters.\n\nstr_length(\"X̅\")\n\n[1] 2\n\n\nHere the “overbar” is a combining character which we add on to the X. This is commonly (though not always) used for languages with accents (e.g. French) or for languages where vowels are written above and below the main script (Arabic or Hebrew). This same idea is used for certain Emoji constructs:\n\nstr_length(\"👨🏿\")\n\n[1] 2\n\n\nHere, “Man with Dark Skin Tone” is the combination of “Man” and “Dark Skin Tone.” (Compare how this appears in the rendered document to how RStudio prints it.)\nWhile there is complexity in all of Unicode, str_length will behave as you might expect for “regular” text. I’m going to stop showing the “scary case” of Unicode, but you should be aware of it for the remainder of these exercises.\n\nConcatenation\nYou have already seen the base paste and paste0 functions for combining two string vectors together.\n\nx &lt;- c(\"Michael\", \"Mary\", \"Gus\")\ny &lt;- c(\"Son\", \"Daughter\", \"Dog\")\n\npaste(x, y)\n\n[1] \"Michael Son\"   \"Mary Daughter\" \"Gus Dog\"      \n\n\nBy default, paste combines strings with a space between them, while paste0 omits the space. paste is typically what you want for strings for human reading, while paste0 is a better guess for computer-oriented text (e.g., putting together a URL).\nYou can change the separator by passing a sep argument to paste:\n\npaste(x, y, sep = \" is my \")\n\n[1] \"Michael is my Son\"   \"Mary is my Daughter\" \"Gus is my Dog\"      \n\n\nYou can also combine together multiple elements of a vector using the collapse argument:\n\npaste(x, collapse = \" and \")\n\n[1] \"Michael and Mary and Gus\"\n\n\n\nExercises:\nUsing the paste function, make a vector of strings like “John’s favorite color is blue”:\n\n\n\n\n\n\n\n\nModify your answer to write a (run-on) sentence of favorite colors: “John’s favorite color is blue and Jane’s favorite color is orange and …”\n\n\n\n\n\n\n\n\n\n\n\nSubstring Selection\nWhen cleaning up data for analysis, it is common to need to take substrings from larger text. The str_sub function will do this:\n\nx &lt;- c(\"How\", \"much\", \"is\", \"that\", \"puppy\", \"in\", \"the\", \"window?\")\nstr_sub(x, 1, 2)\n\n[1] \"Ho\" \"mu\" \"is\" \"th\" \"pu\" \"in\" \"th\" \"wi\"\n\n\nIf you want to go all the way to the end, set the end element to -1:\n\nstr_sub(x, 2, -1)\n\n[1] \"ow\"     \"uch\"    \"s\"      \"hat\"    \"uppy\"   \"n\"      \"he\"     \"indow?\"\n\n\n\nExercises\nUsing str_sub, remove the system name (CUNY or UC) and return only the campus name:\n\n\n\n\n\n\n\n\n\n\n\nDetect and Matching\nOften we only need to know whether a particular substring is present in a larger string. We can use str_detect to do this:\n\nlibrary(stringr)\ndogs &lt;- c(\"basset hound\", \"greyhound\", \"labrador retreiver\", \"border collie\", \"Afgahn hound\")\nstr_detect(dogs, \"hound\")\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE\n\n\nThe str_match function will return the text of the match. Here it’s useless, but we’ll see that it becomes more powerful when we allow more flexible pattern specifications.\n\nlibrary(stringr)\ndogs &lt;- c(\"basset hound\", \"greyhound\", \"labrador retreiver\", \"border collie\", \"Afgahn hound\")\nstr_match(dogs, \"hound\")\n\n     [,1]   \n[1,] \"hound\"\n[2,] \"hound\"\n[3,] NA     \n[4,] NA     \n[5,] \"hound\"\n\n\n\nExercises\nUse str_detect to find the CUNY schools:\n\n\n\n\n\n\n\n\n\n\n\nSpecifying Patterns\nWhile working by individual characters is sometimes useful (for very regular data), we generally need more powerful tools: regular expressions (RE) provide a compact language for specifying patterns in strings. We’ll introduce the basics here to help with string functions and then explore some more advanced RE features.\nThe most basic pattern is a set of elements in brackets: this means “any of these”.\nFor example, we want to see which names have an “A” in them:\n\nnames &lt;- c(\"Jane\", \"Rhonda\", \"Reggie\", \"Bernie\", \"Walter\", \"Arthur\")\nstr_detect(names, \"a\") ## Wrong!\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE FALSE\n\nstr_detect(names, \"A\") ## Wrong!\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE\n\nstr_detect(names, \"[Aa]\") ## Right!\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n\n\nAlternatively, we can see which strings contain numbers:\n\nx &lt;- c(\"73 cows\", \"47 chickens\", \"a dozen eggs\")\nstr_detect(x, \"[0123456789]\")\n\n[1]  TRUE  TRUE FALSE\n\n\nIf we use str_match we can pull out the matching element:\n\nx &lt;- c(\"2 burgers\", \"3 soups\", \"5 fish\")\nstr_match(x, \"[0123456789]\")\n\n     [,1]\n[1,] \"2\" \n[2,] \"3\" \n[3,] \"5\" \n\n\nBy default, this only finds one appearance of the pattern:\n\nx &lt;- c(\"23 burgers\", \"34 soups\", \"56 fish\")\n\n# Why is this wrong?\nstr_match(x, \"[0123456789]\")\n\n     [,1]\n[1,] \"2\" \n[2,] \"3\" \n[3,] \"5\" \n\n\nWe can modify the pattern specifier to include count information. The basic behavior is to add explicit count bounds:\n\nx &lt;- c(\"2 burgers\", \"34 soups\", \"567 fish\")\nstr_match(x, \"[0123456789]{2}\")\n\n     [,1]\n[1,] NA  \n[2,] \"34\"\n[3,] \"56\"\n\nstr_match(x, \"[0123456789]{3}\")\n\n     [,1] \n[1,] NA   \n[2,] NA   \n[3,] \"567\"\n\nstr_match(x, \"[0123456789]{2,3}\")\n\n     [,1] \n[1,] NA   \n[2,] \"34\" \n[3,] \"567\"\n\nstr_match(x, \"[0123456789]{2,}\")\n\n     [,1] \n[1,] NA   \n[2,] \"34\" \n[3,] \"567\"\n\n\nHere a single number is an exact count ({2}), while pairs ({2,3}) specify a range. If one end of the range is left empty, it is 0 or infinite (depending on the direction).\nCertain count specifications are sufficiently useful to get their own syntax:\n\nOne or more: + is equivalent to {1,}\nZero or more: * is equivalent to {0,}\nOne or zero: ? is equivalent to {0,1}.\n\nUse these specifications for the next set of exercises.\n\nExercises\nWhich strings contain a three digit number?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstr_detect(x, \"\\\\d{3}\")\nstr_detect(x, \"\\\\d{3}\")\n\n\n\n\n\n\n\n\n\nCombining patterns\nYou can combine REs to make more complex patterns:\n\n(a|b) means a or b. This is like [] notation but a, b can be more complex than single characters\n\n\nx &lt;- c(\"Baruch College, CUNY\", \"UC Berkeley\", \"Harvard University\")\nstr_detect(x, \"(CUNY|UC)\")\n\n[1]  TRUE  TRUE FALSE\n\n\n\n[^abc] means anything other than a, b, c. You can often achieve a similar effect using the negate argument to str_detect, but you need this specifically for str_match\n\n\nx &lt;- c(\"10 blue fish\", \"three wet goats\", \"15 otters in hats\")\nstr_detect(x, \"[^0123456789]\")\n\n[1] TRUE TRUE TRUE\n\n\n\n^ outside of a bracket denotes the start of a line:\n\n\nx &lt;- c(\"rum\", \"white rum\", \"flavored rum\")\nstr_detect(x, \"^rum\")\n\n[1]  TRUE FALSE FALSE\n\n\n\n$ denotes the end of a line:\n\n\nx &lt;- c(\"bourbon whiskey\", \"scotch whisky\", \"whiskey liqueurs\")\nstr_detect(x, \"whisk[e]?y$\")\n\n[1]  TRUE  TRUE FALSE\n\n\nSee the stringr RE docs for more examples of regular expressions.\n\nExercises\n\nUse a regular expression to find which of these are fish species:\n\n\n\n\n\n\n\n\n\n\nUse a regular expression to find words with three or more vowels in a row:\n\n\n\n\n\n\n\n\n\n\nFind the words where “q” is not followed by a “u”\n\n\n\n\n\n\n\n\n\n\n\n\nReplacement\nThe str_replace function allows us to replace a string with something else. This is particularly useful when cleaning up text:\n\nx &lt;- c(\"Manhattan, NY\", \"Bronx, New York\", \"Brooklyn, ny\", \"Queens, nY\")\nstr_replace(x, \"([nN][yY]|New York)\", \"NY\")\n\n[1] \"Manhattan, NY\" \"Bronx, NY\"     \"Brooklyn, NY\"  \"Queens, NY\""
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab02.html",
    "href": "archive/AY-2024-FALL/labs/lab02.html",
    "title": "STA 9750 Week 2 In-Class Activity: Getting Down with Markdown",
    "section": "",
    "text": "Welcome!\nSlides"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab01.html",
    "href": "archive/AY-2024-FALL/labs/lab01.html",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "Topics:\n\nInstalling R and RStudio\n\nInstalling git\n\nGetting Started on GitHub\n\nBasic Principles of “Clean Code”\n\n\nThe primary programming language used in this course is R, one of the two most popular languages used in data science. R, like its predecessor the S language, is optimized for interactive, data-analytic work, in contrast with python, which is optimized for general purpose computing.\nR is a programming language and runtime; we will supplement it with RStudio, an Integrated Development Environment or, less formally, an editor. RStudio is the software where you will write the code and then the R runtime will execute it.\n\nStudents should first install R from https://cloud.r-project.org/.\n\n\nDon’t fear the 90’s web design! Click image for detailed installation instructions.\n\nAs of 2024-08-26, the most recent version of R is 4.4.1. Using the most current version of R will reduce the likelihood of issues later in the course.\n\nNext, download and install the RStudio IDE (desktop edition).\n\n\nClick image for detailed installation instructions.\n\nRStudio is highly configurable and I recommend taking advantage of all its built-in features. If you go to the Global Options menu (accessible under Tools), I recommend the following settings:\n\nGeneral: Uncheck “Restore .RData into workspace at startup”.\nGeneral: Set “Save workspace to .RData on exit” to “Never”\nCode / Editing: Set “Tab width” to 2\nCode / Editing: Check\n\n“Insert spaces for Tab”\nAuto-detect code indentation\nInsert matching parens / quotes\nUse native pipe operator\nAuto-indent code after paste\nVertically align arguments in auto-indent\nContinue comment when inserting new line\n\n\nCode / Display: Check\n\nShow line numbers\nShow margin (margin column should be 80)\n\n\nCode / Diagnostics: check all “R” diagnostics.\nAppearance: Pick a color theme you enjoy. (I’m partial to light text on a dark background)\n\nYou may wish to enable GitHub Copilot. I have little experience with GH Copilot, but it seems quite popular and is allowed in this course. It is not guaranteed to be accurate at all times - and “the AI told me to” is not a valid excuse if your code is wrong - but on balance, it should be useful.\n\nWe won’t use it this week, but you will need to install Quarto before starting on Mini-Project #00.\n\n\ngit is a source-code management tool, used by developers to manage the code they write. If you’ve ever been part of a large project and struggled to coordinate all team members using the same version of a document, git exists to solve that problem.\nIf you don’t have git pre-installed, install either Git for Windows or the XCode Command Line Tools for MacOS. If not automatically prompted when you try to use git, the Mac install can be manually triggered by running xcode-select --install at a command line.\nIn this course, we will use three main functions of git:\n\n\nstaging: telling git, I want you to prepare to save a certain file\n\ncommitting: saving a set of related changes\n\npushing: copying your committed changes to a separate server for sharing and backup\n\nWhenever you write code you are happy with, you should use git to save it. Saving changes with git is cheap and easy - so do it regularly. You always want git to have a backup of good code in case you loose power, accidentally delete a file, break something in a way you’re not sure how to undo, etc..\nRStudio comes with powerful git integration. Once you have created a project, you should see a tab labelled “Git” in the top right corner of your IDE window that looks something like this:\n\nTo stage a file - prepare to save it - click the empty check box next to the file name. A new file shows a status of “?” - this is git saying “I’ve never seen this file before. Do you want me to track it for you?”. Later, when you make further changes to file you have already asked git to track, a status of “M” (for Modified) will be shown.\nOn its own staging a file does nothing. You also need to commit it for git to truly track it.1 The Commit button will commit all staged changes. When you make a commit, git requires a brief message summarizing the changes. There’s no particular formatting requirement to this message, but it should be something that future-you is able to easily understand. For instance, the commit message from the initial draft of this document reads as:\nInitial draft of Lab 01 (STA9750)\n\n- Installing R and RStudio\n- Git and GitHub\n- Leaflet Example for Styler\n\nTODO: Fuller shell explainers\nTODO: Link more git help\nWhen I read this, I know the purpose of the change I made (first line), the contents of that change (list), and parts that still need more work.\nFinally, after you save a change, it is only saved on your computer. The true power of git comes from its ability to copy changes and backups across machines. This gives you an easy way to store backups in case your computer dies and makes collaboration efficient and fun. git allows you to push and pull changes between machines in endlessly powerful (but sometimes complex) ways. For this course, we’ll keep things simple and only use GitHub to share code. We discuss GitHub in the next section.\nReference: We will not use all of the functionality of git in this course, but you should familiarize yourself with Chapters 1, 2, and 6 of the Git Book over the next two weeks.\n\nGitHub is an industry-standard code hosting and collaboration platform. In addition to hosting copies of code, GitHub provides web hosting, bug reporting, code review, continuous integration, documentation wikis, and discussion fora. You will explore GitHub in more detail starting in Mini-Project #00.\n\n\nA major theme of this course will be sharing and co-developing code with your classmates, both for peer feedback and for the course project. Code sharing is hard! Everyone writes code a little differently and what is clear to you may not be clear at all to your reader.\nTo make code sharing just a bit easier, we use tools to ensure all code shared in this course is consistently formatted. By using consistent formatting, you reduce the cognitive load on your reader, making it easier for them to focus on the ideas of your code, not how you chose to write it.\nA major strength of R is its huge number of user-contributed packages. These are “add-ins” which provide additional functionality not available in the basic version of R. As of 2024-08-26, there are over 21 thousand packages available on CRAN, the largest official repository of R packages. Beyond all those, there are thousands more packages available on other code hosting websites like GitHub.2\nWe will use the contributed styler package to format code in this course. Run the following command to automatically download and install the styler package:\n\ninstall.packages(\"styler\")\n\n(Use the clipboard icon on the right of code snippets to automatically copy code suitable for pasting into RStudio.)\nYou should see something like this:\n\nThe styler package has been downloaded and installed on your computer, but it is not yet “active” or “open” in R. In general, you will only need to download packages once, but you will need to load them each time you want to use them.3\nOpen a R file in RStudio and copy the following (ugly) code:\n\nif(!require(\"leaflet\")) install.packages(\"leaflet\")\nif(!require(\"tidyverse\")){\n    install.packages(\"tidyverse\")\n   }\n library(tidyverse)\n     library(rvest)\nlibrary(leaflet)\n\npAGE = read_html('https://en.wikipedia.org/wiki/Baruch_College')\n  pAGE |&gt; html_element(\".latitude\") |&gt; html_text2() -&gt; BaruchLatitude\n  baruch_longitude &lt;- pAGE |&gt; html_element(\".longitude\") |&gt; html_text2()\n  \n    BaruchLatitude &lt;- sum(as.numeric(strsplit(BaruchLatitude, \n                                     \"[^0123456789]\")[[1]]) * (1/60)^(0:2), na.rm=TRUE)\n baruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) * \n                             (1/60)^(0:2), na.rm=TRUE)\n  \nleaflet() %&gt;% addTiles() %&gt;% setView(-baruch_longitude, BaruchLatitude, zoom=17) %&gt;%\n    addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nYou don’t need to understand what this does just yet, but it’s hopefully clear that this is ugly code. Nothing is lined up properly, capitalization is erratic, and different coding styles are intermixed rather recklessly.\nNear the top of your RStudio pane, you will see a drop-down menu titled Addins. If you successfully installed styler above, one of the Addins choices will be “style active file.” Click this and the code will be cleaned up (a bit) resulting in something like this:\n\nif (!require(\"leaflet\")) install.packages(\"leaflet\")\nif (!require(\"tidyverse\")) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(leaflet)\n\npAGE &lt;- read_html(\"https://en.wikipedia.org/wiki/Baruch_College\")\npAGE |&gt;\n  html_element(\".latitude\") |&gt;\n  html_text2() -&gt; BaruchLatitude\nbaruch_longitude &lt;- pAGE |&gt;\n  html_element(\".longitude\") |&gt;\n  html_text2()\n\nBaruchLatitude &lt;- sum(as.numeric(strsplit(\n  BaruchLatitude,\n  \"[^0123456789]\"\n)[[1]]) * (1 / 60)^(0:2), na.rm = TRUE)\nbaruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) *\n  (1 / 60)^(0:2), na.rm = TRUE)\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(-baruch_longitude, BaruchLatitude, zoom = 17) %&gt;%\n  addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nIt’s far from perfect - and we will discuss the many issues in this example throughout the course - but it’s better! At a minimum, you should make sure to run styler like this on all code you submit during this course.\nAnd now that your code is cleaned up, you should run it! The Source button in the top right corner will run all code in the open file. Running the code produces something like this:\n\n\n\n\n\n\nNot too shabby! That’s an interactive, dynamic map showing the location of Baruch College obtained by parsing the Baruch Wikipedia page, getting the GPS coordinates of Baruch, downloading a map file, and locating Baruch on that map.\nChallenge: Adjust this code to show Hunter college instead of Baruch.\n\nIf you want even more feedback on writing good code, install the lintr package and use the associated RStudio add-in. Unlike styler, lintr won’t make changes automatically for you, but it will highlight much more subtle possible problems.4\n\nTo become a true “power user” of tools like R and python, you will need to become more familiar with the command line interface (CLI) and associated tools.5\nThe Software Carpentry Unix Shell Tutorial is a great introduction to shell usage. Check it out!\nNB: MacOS and Linux systems work quite similarly under the hood, as both descend from the Unix tradition. By contrast, Windows works somewhat differently. Learners whose personal machine runs Windows are encouraged to take advantage of the provided Linux-running virtual machines6 as they work through this section.\n\n\nNext week, we will use these tools to begin coding in earnest. If you’re feeling ambitious, go ahead and get started on Mini-Project #00."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab01.html#r-and-rstudio",
    "href": "archive/AY-2024-FALL/labs/lab01.html#r-and-rstudio",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "The primary programming language used in this course is R, one of the two most popular languages used in data science. R, like its predecessor the S language, is optimized for interactive, data-analytic work, in contrast with python, which is optimized for general purpose computing.\nR is a programming language and runtime; we will supplement it with RStudio, an Integrated Development Environment or, less formally, an editor. RStudio is the software where you will write the code and then the R runtime will execute it.\n\nStudents should first install R from https://cloud.r-project.org/.\n\n\nDon’t fear the 90’s web design! Click image for detailed installation instructions.\n\nAs of 2024-08-26, the most recent version of R is 4.4.1. Using the most current version of R will reduce the likelihood of issues later in the course.\n\nNext, download and install the RStudio IDE (desktop edition).\n\n\nClick image for detailed installation instructions.\n\nRStudio is highly configurable and I recommend taking advantage of all its built-in features. If you go to the Global Options menu (accessible under Tools), I recommend the following settings:\n\nGeneral: Uncheck “Restore .RData into workspace at startup”.\nGeneral: Set “Save workspace to .RData on exit” to “Never”\nCode / Editing: Set “Tab width” to 2\nCode / Editing: Check\n\n“Insert spaces for Tab”\nAuto-detect code indentation\nInsert matching parens / quotes\nUse native pipe operator\nAuto-indent code after paste\nVertically align arguments in auto-indent\nContinue comment when inserting new line\n\n\nCode / Display: Check\n\nShow line numbers\nShow margin (margin column should be 80)\n\n\nCode / Diagnostics: check all “R” diagnostics.\nAppearance: Pick a color theme you enjoy. (I’m partial to light text on a dark background)\n\nYou may wish to enable GitHub Copilot. I have little experience with GH Copilot, but it seems quite popular and is allowed in this course. It is not guaranteed to be accurate at all times - and “the AI told me to” is not a valid excuse if your code is wrong - but on balance, it should be useful.\n\nWe won’t use it this week, but you will need to install Quarto before starting on Mini-Project #00."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab01.html#source-code-management",
    "href": "archive/AY-2024-FALL/labs/lab01.html#source-code-management",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "git is a source-code management tool, used by developers to manage the code they write. If you’ve ever been part of a large project and struggled to coordinate all team members using the same version of a document, git exists to solve that problem.\nIf you don’t have git pre-installed, install either Git for Windows or the XCode Command Line Tools for MacOS. If not automatically prompted when you try to use git, the Mac install can be manually triggered by running xcode-select --install at a command line.\nIn this course, we will use three main functions of git:\n\n\nstaging: telling git, I want you to prepare to save a certain file\n\ncommitting: saving a set of related changes\n\npushing: copying your committed changes to a separate server for sharing and backup\n\nWhenever you write code you are happy with, you should use git to save it. Saving changes with git is cheap and easy - so do it regularly. You always want git to have a backup of good code in case you loose power, accidentally delete a file, break something in a way you’re not sure how to undo, etc..\nRStudio comes with powerful git integration. Once you have created a project, you should see a tab labelled “Git” in the top right corner of your IDE window that looks something like this:\n\nTo stage a file - prepare to save it - click the empty check box next to the file name. A new file shows a status of “?” - this is git saying “I’ve never seen this file before. Do you want me to track it for you?”. Later, when you make further changes to file you have already asked git to track, a status of “M” (for Modified) will be shown.\nOn its own staging a file does nothing. You also need to commit it for git to truly track it.1 The Commit button will commit all staged changes. When you make a commit, git requires a brief message summarizing the changes. There’s no particular formatting requirement to this message, but it should be something that future-you is able to easily understand. For instance, the commit message from the initial draft of this document reads as:\nInitial draft of Lab 01 (STA9750)\n\n- Installing R and RStudio\n- Git and GitHub\n- Leaflet Example for Styler\n\nTODO: Fuller shell explainers\nTODO: Link more git help\nWhen I read this, I know the purpose of the change I made (first line), the contents of that change (list), and parts that still need more work.\nFinally, after you save a change, it is only saved on your computer. The true power of git comes from its ability to copy changes and backups across machines. This gives you an easy way to store backups in case your computer dies and makes collaboration efficient and fun. git allows you to push and pull changes between machines in endlessly powerful (but sometimes complex) ways. For this course, we’ll keep things simple and only use GitHub to share code. We discuss GitHub in the next section.\nReference: We will not use all of the functionality of git in this course, but you should familiarize yourself with Chapters 1, 2, and 6 of the Git Book over the next two weeks.\n\nGitHub is an industry-standard code hosting and collaboration platform. In addition to hosting copies of code, GitHub provides web hosting, bug reporting, code review, continuous integration, documentation wikis, and discussion fora. You will explore GitHub in more detail starting in Mini-Project #00."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab01.html#code-styling",
    "href": "archive/AY-2024-FALL/labs/lab01.html#code-styling",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "A major theme of this course will be sharing and co-developing code with your classmates, both for peer feedback and for the course project. Code sharing is hard! Everyone writes code a little differently and what is clear to you may not be clear at all to your reader.\nTo make code sharing just a bit easier, we use tools to ensure all code shared in this course is consistently formatted. By using consistent formatting, you reduce the cognitive load on your reader, making it easier for them to focus on the ideas of your code, not how you chose to write it.\nA major strength of R is its huge number of user-contributed packages. These are “add-ins” which provide additional functionality not available in the basic version of R. As of 2024-08-26, there are over 21 thousand packages available on CRAN, the largest official repository of R packages. Beyond all those, there are thousands more packages available on other code hosting websites like GitHub.2\nWe will use the contributed styler package to format code in this course. Run the following command to automatically download and install the styler package:\n\ninstall.packages(\"styler\")\n\n(Use the clipboard icon on the right of code snippets to automatically copy code suitable for pasting into RStudio.)\nYou should see something like this:\n\nThe styler package has been downloaded and installed on your computer, but it is not yet “active” or “open” in R. In general, you will only need to download packages once, but you will need to load them each time you want to use them.3\nOpen a R file in RStudio and copy the following (ugly) code:\n\nif(!require(\"leaflet\")) install.packages(\"leaflet\")\nif(!require(\"tidyverse\")){\n    install.packages(\"tidyverse\")\n   }\n library(tidyverse)\n     library(rvest)\nlibrary(leaflet)\n\npAGE = read_html('https://en.wikipedia.org/wiki/Baruch_College')\n  pAGE |&gt; html_element(\".latitude\") |&gt; html_text2() -&gt; BaruchLatitude\n  baruch_longitude &lt;- pAGE |&gt; html_element(\".longitude\") |&gt; html_text2()\n  \n    BaruchLatitude &lt;- sum(as.numeric(strsplit(BaruchLatitude, \n                                     \"[^0123456789]\")[[1]]) * (1/60)^(0:2), na.rm=TRUE)\n baruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) * \n                             (1/60)^(0:2), na.rm=TRUE)\n  \nleaflet() %&gt;% addTiles() %&gt;% setView(-baruch_longitude, BaruchLatitude, zoom=17) %&gt;%\n    addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nYou don’t need to understand what this does just yet, but it’s hopefully clear that this is ugly code. Nothing is lined up properly, capitalization is erratic, and different coding styles are intermixed rather recklessly.\nNear the top of your RStudio pane, you will see a drop-down menu titled Addins. If you successfully installed styler above, one of the Addins choices will be “style active file.” Click this and the code will be cleaned up (a bit) resulting in something like this:\n\nif (!require(\"leaflet\")) install.packages(\"leaflet\")\nif (!require(\"tidyverse\")) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(leaflet)\n\npAGE &lt;- read_html(\"https://en.wikipedia.org/wiki/Baruch_College\")\npAGE |&gt;\n  html_element(\".latitude\") |&gt;\n  html_text2() -&gt; BaruchLatitude\nbaruch_longitude &lt;- pAGE |&gt;\n  html_element(\".longitude\") |&gt;\n  html_text2()\n\nBaruchLatitude &lt;- sum(as.numeric(strsplit(\n  BaruchLatitude,\n  \"[^0123456789]\"\n)[[1]]) * (1 / 60)^(0:2), na.rm = TRUE)\nbaruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) *\n  (1 / 60)^(0:2), na.rm = TRUE)\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(-baruch_longitude, BaruchLatitude, zoom = 17) %&gt;%\n  addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nIt’s far from perfect - and we will discuss the many issues in this example throughout the course - but it’s better! At a minimum, you should make sure to run styler like this on all code you submit during this course.\nAnd now that your code is cleaned up, you should run it! The Source button in the top right corner will run all code in the open file. Running the code produces something like this:\n\n\n\n\n\n\nNot too shabby! That’s an interactive, dynamic map showing the location of Baruch College obtained by parsing the Baruch Wikipedia page, getting the GPS coordinates of Baruch, downloading a map file, and locating Baruch on that map.\nChallenge: Adjust this code to show Hunter college instead of Baruch.\n\nIf you want even more feedback on writing good code, install the lintr package and use the associated RStudio add-in. Unlike styler, lintr won’t make changes automatically for you, but it will highlight much more subtle possible problems.4"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab01.html#extra-welcome-to-shell",
    "href": "archive/AY-2024-FALL/labs/lab01.html#extra-welcome-to-shell",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "To become a true “power user” of tools like R and python, you will need to become more familiar with the command line interface (CLI) and associated tools.5\nThe Software Carpentry Unix Shell Tutorial is a great introduction to shell usage. Check it out!\nNB: MacOS and Linux systems work quite similarly under the hood, as both descend from the Unix tradition. By contrast, Windows works somewhat differently. Learners whose personal machine runs Windows are encouraged to take advantage of the provided Linux-running virtual machines6 as they work through this section."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab01.html#looking-ahead",
    "href": "archive/AY-2024-FALL/labs/lab01.html#looking-ahead",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "Next week, we will use these tools to begin coding in earnest. If you’re feeling ambitious, go ahead and get started on Mini-Project #00."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab01.html#footnotes",
    "href": "archive/AY-2024-FALL/labs/lab01.html#footnotes",
    "title": "STA 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis two stage process is a bit cumbersome for the first stage of a small project, but it quickly becomes incredibly valuable. Instead of saving everything every time, there is great power in only saving “good” or “finished” changes to a large project, while leaving work-in-progress elsewhere unsaved. You probably won’t need this level of control until you get to the course project, but it’s better to have it than not.↩︎\nIf you are interested in bioinformatics, the Bioconductor project develops incredible open-source R packages.↩︎\nWhile this may feel cumbersome, it’s really not dissimilar to any other software you use (or R itself). You need to download it once, but you need to open it each time you intend to use it. There’s no harm in re-downloading–free software!–but it wastes time and bandwidth. Since we benefit so much from the free-software community, the very least we can do is not run up their internet bills unnecessarily.↩︎\nSome of the issues identified by lintr may be false positives, but the false positive rate is quite low, especially for the sort of procedural code that is the focus of this course. You should default to trying to appease lintr, but feel free to use the course discussion board for any questions.↩︎\nAs an added benefit, use of the CLI also makes you look like a 90s movie hacker to all your friends.↩︎\nSee the Course Resources page.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab11.html",
    "href": "archive/AY-2024-FALL/labs/lab11.html",
    "title": "STA 9750 Week 11 In-Class Activity: HTML Import",
    "section": "",
    "text": "Week 11 Slides\n\n\n\n\n\n\nCUNY Mapping Code\n\n\n\n\n\n\nlibrary(rvest)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()         masks stats::filter()\n✖ readr::guess_encoding() masks rvest::guess_encoding()\n✖ dplyr::lag()            masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(leaflet)\n\nCUNYs &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_City_University_of_New_York_institutions\") |&gt; \n    html_element(\"tbody\") |&gt;\n    html_elements(\"tr td:nth-child(2)\") |&gt;\n    html_elements(\"a\")\n\nCUNYs &lt;- data.frame(name = CUNYs |&gt; html_text(),\n                    link = CUNYs |&gt; html_attr(\"href\")\n)\n\nget_cuny_gps &lt;- function(url){\n    COORDS &lt;- read_html(url) |&gt; html_element(\".geo\") |&gt; html_text() |&gt; str_split_1(\";\")\n    LAT &lt;- as.numeric(COORDS[1])\n    LON &lt;- as.numeric(COORDS[2])\n    list(LAT=LAT, LON=LON)\n}\n\nCUNYs &lt;- CUNYs |&gt; \n    mutate(link = paste0(\"https://en.wikipedia.org/\", link)) |&gt;\n    rowwise() |&gt;\n    mutate(gps = list(get_cuny_gps(link))) |&gt;\n    unnest_wider(gps)\n\nMAP &lt;- leaflet() |&gt; \n    addTiles() |&gt;\n    addMarkers(CUNYs$LON, \n               CUNYs$LAT, \n               popup=CUNYs$name, \n              options = popupOptions(closeOnClick=FALSE))\n\nMAP"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab13.html",
    "href": "archive/AY-2024-FALL/labs/lab13.html",
    "title": "STA 9750 Week 13 In-Class Activity: Predictive Modeling",
    "section": "",
    "text": "Week 13 Slides"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab12.html",
    "href": "archive/AY-2024-FALL/labs/lab12.html",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "",
    "text": "Week 12 Slides"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab12.html#regular-expression-practice",
    "href": "archive/AY-2024-FALL/labs/lab12.html#regular-expression-practice",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "Regular Expression Practice",
    "text": "Regular Expression Practice\nComplete the following exercises using functionality from the stringr package.\n\nIn the following sentence, extract all plural nouns1:\n\n\ntodo &lt;- \"Yesterday, I needed to buy four cups of flour, a piece of Parmesan cheese, two gallons of ice cream, and a six-pack of bottled (non-alcoholic) beers.\"\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(stringr)\ntodo &lt;- \"Yesterday, I needed to buy four cups of flour, a piece of Parmesan cheese, two gallons of ice cream, and a six-pack of bottled (non-alcoholic) beers.\"\nstr_extract_all(todo, \" [A-Za-z]+s[. ]\", simplify=TRUE)\nlibrary(stringr)\ntodo &lt;- \"Yesterday, I needed to buy four cups of flour, a piece of Parmesan cheese, two gallons of ice cream, and a six-pack of bottled (non-alcoholic) beers.\"\nstr_extract_all(todo, \" [A-Za-z]+s[. ]\", simplify=TRUE)\n\n\n\n\n\n\n\nIn the following sentence, compute the total number of fruits on my shopping list:\n\n\nshopping &lt;- \"Today, I need to purchase 3 apples, 5 limes, and 2 lemons.\"\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(stringr)\nshopping &lt;- \"Today, I need to purchase 3 apples, 5 limes, and 2 lemons.\"\nsum(as.numeric(str_extract_all(shopping, \"\\\\d+\", simplify=TRUE)))\nlibrary(stringr)\nshopping &lt;- \"Today, I need to purchase 3 apples, 5 limes, and 2 lemons.\"\nsum(as.numeric(str_extract_all(shopping, \"\\\\d+\", simplify=TRUE)))\n\n\n\n\n\n\n\nThe following text is adapted from the Taylor Swift wikipedia page, with some changes made to the punctuation to make things easier.\n\n\nTaylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles ‘Teardrops on My Guitar’, ‘Love Story’, and ‘You Belong with Me’ were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, ‘We Are Never Ever Getting Back Together’. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs ‘Shake It Off’, ‘Blank Space’, and ‘Bad Blood’. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single ‘Look What You Made Me Do’. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs ‘Cruel Summer’, ‘Cardigan’, ‘Willow’, ‘Anti-Hero’, ‘All Too Well’, and ‘Is It Over Now?’. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award, 40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\n\nHow many times does Taylor Swift’s last name appear?\n\n\n\n\n\n\n\n\n\n\n\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_count(swift, \"Swift\")\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_count(swift, \"Swift\")\n\n\n\n\n\n\n\nIn the above quote, how many different years (strings of exactly 4 digits) appear?\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_count(swift, \"\\\\d{4}\")\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_count(swift, \"\\\\d{4}\")\n\n\n\n\n\n\n\nExtract the names of all songs mentioned in the biography above. (Note that song names are surrounded by single quotes.)\nYou will need to use a lazy regular expression.\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_extract_all(swift, \"'.*?'\", simplify=TRUE)\nlibrary(stringr)\nswift &lt;- \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. A subject of widespread public interest, she has influenced the music industry and popular culture through her artistry, especially in songwriting, and entrepreneurship. She is an advocate of artists rights and womens empowerment. Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles 'Teardrops on My Guitar', 'Love Story', and 'You Belong with Me' were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, 'We Are Never Ever Getting Back Together'. Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs 'Shake It Off', 'Blank Space', and 'Bad Blood'. Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single 'Look What You Made Me Do'. After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylors Version after a dispute with Big Machine. These albums spawned the number-one songs 'Cruel Summer', 'Cardigan', 'Willow', 'Anti-Hero', 'All Too Well', and 'Is It Over Now?'. Her Eras Tour (2023-2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling artists, with 200 million records sold worldwide as of 2019. She is the most-streamed artist on Spotify, the highest-grossing female touring act, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award,  40 American Music Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.\"\nstr_extract_all(swift, \"'.*?'\", simplify=TRUE)"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab12.html#scraping-practice-i-cocktail-recipies-part-2",
    "href": "archive/AY-2024-FALL/labs/lab12.html#scraping-practice-i-cocktail-recipies-part-2",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "Scraping Practice I: Cocktail Recipies (Part 2)",
    "text": "Scraping Practice I: Cocktail Recipies (Part 2)\nLast week, we began to scrape Hadley’s Cocktails with an (eventual) goal of creating a “spreadsheet” of recipes by ingredients.\nWe found the following:\n\nlibrary(rvest)\nBASE_URL &lt;- \"https://cocktails.hadley.nz/\"\n\nPAGES &lt;- read_html(BASE_URL) |&gt; \n    html_elements(\"nav a\") |&gt; \n    html_attr(\"href\")\n\nread_article &lt;- function(article){\n    title &lt;- article |&gt; html_element(\"h2\") |&gt; html_text()\n    ingredients &lt;- article |&gt; html_elements(\"li\") |&gt; html_text()\n    \n    data.frame(title=title, ingredient=ingredients)\n}\n\nread_page &lt;- function(stub){\n    URL &lt;- paste0(BASE_URL, stub)\n    COCKTAILS &lt;- read_html(URL) |&gt; html_elements(\"article\")\n    \n    map_df(COCKTAILS, read_article)\n}\n\nRECIPES_LONG &lt;- map_df(PAGES, read_page)\n\nTake this output and use stringr and tidyr to complete the transition to a well-formated “wide” set of recipies.\n\nClean up the title column.\nSplit the ingredient column into three new columns:\n\nAmount\nUnit\nIngredient Name\n\nThe following functions may be useful to you:\n\n\nget_number_part &lt;- function(x){\n    library(dplyr)\n    library(stringr)\n    x |&gt;\n        str_replace_all(\"[A-Za-z,']\", \"\") |&gt;\n        str_trim() |&gt;\n        case_match(\n            \"1\" ~ 1, \n            \"2\" ~ 2,\n            \"2½\" ~ 2.5,\n            \"1½\" ~ 1.5,\n            \"½\" ~ 0.5, \n            \"¾\" ~ 0.75,\n            \"1¾\" ~ 1.75,\n            \"¼\" ~ 0.25,\n            \"1¼\" ~ 1.25,\n            \"3\" ~ 3,\n            \"4\" ~ 4, \n            \"5\" ~ 5, \n            \"6\" ~ 6, \n            \"7\" ~ 7, \n            \"8\" ~ 8\n        )\n}\n\nget_cocktail_unit &lt;- function(x){\n    library(dplyr)\n    library(stringr)\n    case_when(\n        str_detect(x, \"oz\") ~ \"oz\",\n        str_detect(x, \"dash\") ~ \"dash\",\n        str_detect(x, \"drop\") ~ \"drop\",\n        str_detect(x, \" t \") ~ \"t\", \n        str_detect(x, \"chunk\") ~ \"chunk\",\n        str_detect(x, \"leaves\") ~ \"leaves\",\n        str_detect(x, \" cm \") ~ \"cm\"\n    )\n}\n\nYou will need to create a third helper function to pull out the actual ingredient name. You can implement this with a (somewhat complex) str_replace_all.\nYou should wind up with a table that looks something like\n\n\n\nCocktail\nIngredient\nUnit\nAmount\n\n\n\n\nBachelor\nrum, dark\noz\n1\n\n\nBachelor\nMeletti\noz\n1\n\n\n\n\nCombine the ingredient and unit columns so that Meletti and oz in two separate columns becomes Meletti (oz) in a single column.\nUse a pivot_* function to create a new wide table with each ingredient as a column.\nWhich pivot operation do you want to use here? How should the empty cells be treated?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nget_ingredient_name &lt;- function(x){\n    str_remove_all(x, \"[0-9¼½¾]|( oz )|( dashes )|( dash )|( drops )|( drop )|( t )|( chunks )|( chunk )|( leaves )|( leaf )|( cm )\")\n}\n\nRECIPES_LONG |&gt;\n    # Remove duplicates from import process\n    # When we import each ingredient page, we get duplicates\n    # as drinks are listed on multiple ingredient pages.\n    distinct() |&gt;\n    mutate(title  = str_trim(title), \n           amount = get_number_part(ingredient), \n           unit  = get_cocktail_unit(ingredient), \n           ingredient = get_ingredient_name(ingredient)) |&gt;\n    # For some ingredients, e.g. a lemon twist, the implied\n    # but unstated quantity is 1\n    mutate(amount = case_when(\n        is.na(amount) ~ 1, \n        TRUE ~ amount), \n        ingredient = str_to_title(ingredient)) |&gt;\n    rename(Cocktail = title, \n           Amount = amount, \n           Unit = unit, \n           Ingredient = ingredient) |&gt;\n    mutate(Ingredient = case_when(\n        is.na(Unit) ~ Ingredient, # Handle unit-less ingredients\n        TRUE ~ paste0(Ingredient, \" (\", Unit, \")\"))) |&gt;\n    # Spanish Coffee lists orange liqueur twice, so let's \n    # add up repeated ingredients before pivoting.\n    # (I think this is the only one)\n    group_by(Cocktail, Ingredient) |&gt;\n    summarize(Amount = sum(Amount)) |&gt;\n    ungroup() |&gt;\n    pivot_wider(id_cols = Cocktail, \n                names_from = Ingredient, \n                values_from = Amount, \n                values_fill = 0) |&gt;\n    select(\"Cocktail\", sort(tidyselect::peek_vars()))"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab12.html#scraping-practice-ii-quotes",
    "href": "archive/AY-2024-FALL/labs/lab12.html#scraping-practice-ii-quotes",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "Scraping Practice II: Quotes",
    "text": "Scraping Practice II: Quotes\nNext, let’s analyze the website https://quotes.toscrape.com, a website designed to practice web-scraping.\nScrape the contents of that website–note that quotes continue for multiple pages–and answer the following questions. You can check most of these “by hand” but you need to compute your answers in code!\n\nHow many quotes are on this website (all pages)?\nHow many quotes are tagged “Death”?\nWhat is the longest quote (by number of characters)? The nchar function will be helpful.\nHow many quotes are by (or at least are attributed to) Albert Einstein?\nOf all authors quoted, who has the earliest (estimated) birthday?"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab12.html#basic-statistical-modeling",
    "href": "archive/AY-2024-FALL/labs/lab12.html#basic-statistical-modeling",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "Basic Statistical Modeling",
    "text": "Basic Statistical Modeling\nR has a number of powerful built-in statistical methods, many of which you will explore in other courses, e.g., lm and glm in your regression course. Time allowing, we’re going to explore an alternative approach to statistics using computational inference. While these techniques can be implemented by hand, we’re going to use the infer package.\nTime Allowing:\n\nDiscussion of computational inference\nDemonstration with infer package."
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab12.html#footnotes",
    "href": "archive/AY-2024-FALL/labs/lab12.html#footnotes",
    "title": "STA 9750 Week 12 In-Class Activity: Strings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile English pluralization rules are tricky, you can just find the words ending with an s.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/labs/lab09.html",
    "href": "archive/AY-2024-FALL/labs/lab09.html",
    "title": "STA 9750 Week 9 In-Class Activity: Data Import",
    "section": "",
    "text": "Week 9 Slides"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini04.html",
    "href": "archive/AY-2024-FALL/miniprojects/mini04.html",
    "title": "STA 9750 Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "\\[\\newcommand{\\P}{\\mathbb{P}} \\newcommand{\\E}{\\mathbb{E}}\\]"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini04.html#introduction",
    "href": "archive/AY-2024-FALL/miniprojects/mini04.html#introduction",
    "title": "STA 9750 Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Introduction",
    "text": "Introduction\nWelcome to Mini-Project #04! In this project, you will use R to make an important personal financial decision. New faculty hired at CUNY have 30 days to choose one of two retirement plans.1 This is an important and early choice faculty have to make, as it is essentially permanent and cannot be changed. Financial forecasting is quite difficult and it is far from clear which plan is the better long-term choice. In this mini-project, you will use historical financial data and a bootstrap inference strategy to estimate the probability that one plan is better than the other.\nIn this project, you will:\n\nUse a password-protected API to acquire financial data\nUse resampling inference to estimate complex probability distributions\nSee how the optimal financial decision varies as a function of market returns\nInvestigate how demographic and actuarial assumptions, as well as individual risk-tolerances, change the optimal decision.\n\nAs always, there isn’t a single “right” answer to the questions posed herein. The optimal retirement plan will depend on your starting salary, your projections of future salary growth, your current age and expected age of retirement, stock market returns, inflation, etc.\nAlso note that this mini-project is intended to be the least difficult of the course; Note that, compared to previous projects, the scope of this project is relatively smaller: in light of this, and the more advanced skills you have spent the past 3 months developing, this mini-project should be the least difficult of the course. At this point in the course, you should be spending the majority of your out-of-class hours on your Course Project.\nThis mini-project completes our whirlwind tour of several different forms of data-driven writing:\n\nData Analysis Report (MP#01)\nData-Based Support for “Sales Pitch” (MP#02)\nData-Based Evaluation of Third-Party Assertion (“Fact Check”) (MP#03)\nData-Driven Decision Support (this project)\n\nThere are, of course, other ways that data can be used to generate and communicate insights, but hopefully this “hit parade” has exposed you to many of the ways that you can use data to evaluate complex qualitative and quantitative claims outside of a binary classroom “correct/incorrect” structure. The tools of quantitative analysis and communication you have developed in this course can be used in essentially infinite contexts– we have only scratched the surface–and I’m excited to see what you do in the remainder of this course, in your remaining time at Baruch, and in your future careers.\nPlease note that you have the option of creating an interactive shiny retirement forecasting tool as extra credit, but this is not required. See details below."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini04.html#background",
    "href": "archive/AY-2024-FALL/miniprojects/mini04.html#background",
    "title": "STA 9750 Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Background",
    "text": "Background\nPlease Note: Nothing in this document constitutes an official NYS, NYC, or CUNY statement about the retirement plans. This document omits several subtleties in the interest of pedagogical simplicity. If you are a potential or new CUNY employee who has stumbled across this document, please speak to your personal financial advisor, your Human Resources department, or your union representative for up-to-date and accurate retirement benefit information.\nCUNY Retirement Plans\nCUNY offers two retirement plans, the traditional defined-benefit Teachers Retirement System (TRS) plan and the newer defined-contribution Optional Retirement Plan (ORP).2\nFor this project, you may ignore the effect of taxes as both plans offer pre-tax retirement savings, so whichever plan has the greater (nominal, pre-tax) income will also yield the greater (post-tax) take-home amount.\nTeachers Retirement System\nThe TRS plan is a traditional pension plan: after retirement, the employer (CUNY) continues to pay employees a fraction of their salary until death. This type of plan is called a “defined-benefit” because the retirement pay (the benefit) is fixed a priori and the employer takes the market risk. If the market underperforms expectations, CUNY has to “pony up” and make up the gap; if the market overperforms expectations, CUNY pockets the excess balance.\nAt CUNY, the TRS is administered as follows:\n\n\nEmployees pay a fixed percentage of their paycheck into the pension fund. For CUNY employees joining after March 31, 2012–which you may assume for this project–the so-called “Tier VI” contribution rates are based on the employee’s annual salary and increase as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\n\n\nThe retirement benefit is calculated based on the Final Average Salary of the employee: following 2024 law changes, the FAS is computed based on the final three years salary. (Previously, FAS was computed based on 5 years: since salaries tend to increase monotonically over time, this is a major win for TRS participants.)\nIf \\(N\\) is the number of years served, the annual retirement benefit is:\n\n\n\\(1.67\\% * \\text{FAS} * N\\) if \\(N \\leq 20\\)\n\n\n\\(1.75\\% * \\text{FAS} * N\\) if \\(N = 20\\)\n\n\n\\((35\\% + 2\\% * (N - 20)) * \\text{FAS}\\) if \\(N \\geq 20\\)34\n\n\nIn each case, the benefit is paid out equally over 12 months.\n\n\nThe benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent: e.g., a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.\nThe inflation adjustement is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months; so the September 2024 adjustment depends on the CPI from September 2023 to August 2024.\n\nOptional Retirement Plan\nThe ORP plan is more similar to a 401(k) plan offered by a private employer. The employee and the employer both make contributions to a retirement account which is then invested in the employee’s choice of mutual funds. Those investments grow “tax-free” until the employee begins to withdraw them upon retirement. If the employee does not use up the funds, they are passed down to that employee’s spouse, children, or other heirs; if the employee uses the funds too quickly and zeros out the account balance, no additional retirement funds are available. Though the employee hopefully still has Social Security retirement benefits and other savings to cover living expenses. This type of plan is called a defined-contribution plan as only the contributions to the retirement account are fixed by contract: the final balance depends on market factors outside of the employee’s control.\nAt retirement, the employee has access to those funds and can choose to withdraw them at any rate desired. A general rule of thumb is withdrawing 4% of the value per year, e.g., this Schwab discussion; you can assume a constant withdrawal rate in your analysis. Note that unwithdrawn funds continue to experience market returns.\nThe funds available in a ORP account depend strongly on the investments chosen. For this analysis, you can assume that the ORP participants invest in a Fidelity Freedom Fund with the following asset allocation:5\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\n\nUnder the ORP, both the employee and the employer make monthly contributions to the employee’s ORP account. These contributions are calculated as a percentage of the employee’s annual salary. Specifically, the employee contributes at the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at:\n\n8% for the first seven years of employment at CUNY.\n10% for all years thereafter.\n\nFor example, if an employee made $52,267 in their first year at CUNY6, the total annual contributions to their ORP account would be:\n\n\nEmployer: 8% of salary = 0.08 * $52,267 = $4,181.36\n\nEmployee: 3.5% of salary = 0.035 * $52,267 = $1,829.35\n\nTotal: 11.5% of $52,267 = 4181.36 + 1829.35 = $6010.71\n\nbecause their salary puts them in the 3.5% Contribution Range. On a monthly basis, this works out to a contribution of about $500.89 per month.\nLater in their career, if they have been at CUNY for 8 years and just been promoted to (Associate) Professor with a base salary of $67,784, the total annual contributions to their ORP account are now:\n\n\nEmployer: 10% of salary = 0.10 * $67,784 = $6,778.40\n\nEmployee: 4.5% of salary = 0.045 * $67,784 = $3050.28\n\nTotal: 14.5% of salary = 0.145 * $67,784 = 6778.40 + 3050.28 = $9,828.68\n\nHere, the employee now has a higher employer contribution rate because they have been at CUNY more than 7 years and has a higher employee contribution rate because the salary has increased with a promotion. On a monthly basis, this works out to a total contribution of about $819.06 per month.\nYou may assume that the contributions are immediately invested according to the asset allocations above.\nThe Power of Compound Interest\nRetirement in the US would scarcely be possible without the magic of compounded growth. When you invest early, the money you put in your retirement savings grows, as do the earning you make along the way. This creates a virtuous cycle, resembling exponential growth, making it possible to retire with (hopefully far) more money than you originally put in the account.\nCompound interest works as follows: let \\(X_t\\) be the amount of money in the account at the start of month \\(t\\), let \\(C_t\\) be the contributions during month \\(t\\), and let \\(r_t\\) be the fund return during month \\(t\\). We then have the recurrence relation:\n\\[X_{t+1} = X_t(1+r_t) + C_t\\]\nRepeating this, we get:\n\\[\\begin{align*}\nX_{t+2} &= X_{t+1}(1+r_{t+1}) + C_{t+1} \\\\\n        &= X_{t}(1+r_t)(1+r_{t+1}) + C_t(1+r_{t+1}) + C_{t+1} \\\\\nX_{t+3} &= X_{t}(1+r_t)(1+r_{t+2})(1+r_{t+3}) + C_t(1+r_{t+1})(1+r_{t+2}) + C_{t+1}(1+r_{t+2}) + C_{t+2}\n\\end{align*}\\]\nand so on. In R, you might calculate two years of compounded growth as follows:\n\nRETIREMENT &lt;- data.frame(\n    r = rnorm(24, mean=0.5) / 100, # Monthly returns\n    C = rep(100, 24),            # Monthly savings: 100 per month\n    period = 1:24                # Period ID (# of months)\n)\n\nRETIREMENT |&gt; \n    mutate(net_total_return = order_by(desc(period), \n                                       cumprod(1 + lead(r, default=0)))) |&gt;\n    summarize(future_value = sum(C * net_total_return))\n\n  future_value\n1     2520.644\n\n\n(It is a good exercise to try to understand the calculation above.)\nHere, we combine a series of monthly returns, column r, with periodic contributions, column C, to find the final future value of the account. Because the returns have a small positive mean (0.5% per month, equivalent to about 6.2% annual), the final value of the account is likely more than the total contributions.\nData Sources - AlphaVantage & FRED\nFor this project, we will use data from two economic and financial data sources:\n\n\nAlphaVantage: a commercial stock market data provider\n\nFRED: the Federal Reserve Economic Data repository maintained by the Federal Reserve Bank of St. Louis\n\nFRED is free to access, but AlphaVantage is a commercial service requiring subscription. For this mini-project, the free tier of AlphaVantage will suffice.\n\n\n\n\n\n\nTask 1 - Register for AlphaVantage API Key\n\n\n\nCreate your AlphaVantage free API key at https://www.alphavantage.co/support/#api-key.\nDo Not Include This Key in Your Submission.7 It is your personal account and linked to whatever email you use to create your account.\nI recommend creating a new file on your computer and storing your key there. You can read it into R using the readLines function and use it as needed. Just make sure not to print it.\nOnce you store your AlphaVantage key in a plain text file, make sure to add that file to your .gitignore to make sure you don’t accidentally include it in your git history.\nDocumentation for the AlphaVantage API can be found at https://www.alphavantage.co/documentation/.\n\n\n\n\n\n\n\n\nTask 2 - Register for FRED API Key\n\n\n\nCreate your FRED free API key at https://fredaccount.stlouisfed.org/login/secure/.\nDo Not Include This Key in Your Submission. It is your personal account and linked to whatever email you use to create your account.\nI recommend creating a new file on your computer and storing your key there. You can read it into R using the readLines function and use it as needed. Just make sure not to print it.\nOnce you store your FRED key in a plain text file, make sure to add that file to your .gitignore to make sure you don’t accidentally include it in your git history.\nDocumentation for the FRED API is available at https://fred.stlouisfed.org/docs/api/fred/.\n\n\n\n\n\n\n\n\nDon’t Use Specialized R Packages To Access Data Sources\n\n\n\nFor this project, you may not use any R packages that wrap AlphaVantage or FRED. A learning objective of this mini-project is accessing data via an API, and you won’t practice that skill if you simply use a package like quantmod or alphavantager.\nYou should interact with the APIs directly using httr2 or equivalent packages.\n\n\nYou will need to download data from FRED and/or AlphaVantage to complete this assignment.\n\n\n\n\n\n\nSetting the function parameter in the AlphaVantage API\n\n\n\nFor AlphaVantage, you will need to set a function value in the query string. To do this in httr2, you will need code something like\n\nlibrary(httr2)\n\nreq &lt;- request(ALPHAVANTAGE_URL) |&gt;\n    req_url_query(`function` = VALUE)\n\nHere, because function is a keyword (reserved word) in R, we have to surround it with backquotes if we want to use it as the literal word instead of using it to create a function.\n\n\nBootstrap Resampling\nTo complete this project, you will use a boostrap resampling strategy. The details of how bootstrap sampling can be applied to this type of financial analysis are given in more detail below. Here, we simply review the basic principles of the bootstrap.\nClassically, statistics proceeds by assuming some sort of parametric model for the population from which we have samples. Parametric models describe a family of not-dissimilar distributions, indexed by a small number of parameters; e.g., the two parameters of the normal distribution, mean and variance. While this leads to quite elegant mathematical theory, it is unclear how useful this theory is in practice: we rarely know with certainty that a particular parametric family is an accurate model for a phenomenon of interest. Asymptotic results like the Central Limit Theorem give us some sense that we don’t need to know the true distribution family exactly to get accurate answers, but the era of computers gives us an alternate approach.\nIf we have a large sample size, the empirical CDF of our data is guaranteed to converge to the true CDF of the population. Since essentially all quantities of interest can be computed using the CDF, this guarantees that, with enough data, we can compute any quantity of interest using the empirical CDF only and get an accurate answer. The bootstrap principle, and the associated broader field of resampling inference, uses this concept to compute variances of complex statistical procedures without using parametric models.\nFormally, let our data come from some distribution \\(\\P\\) and let \\(\\P_n\\) be the empirical (sample) distribution. The core statistical theorems tell us that \\(\\P_n \\to \\P\\) in various ways:\n\nLaw of Large Numbers: \\(\\E_{\\P_n}[f(X)] \\to \\E_{\\P}[f(X)]\\)\n\nGlivenko-Cantelli: \\(\\sup |\\P_n(X \\leq x) - \\P(X \\leq x)| \\to 0\\)\n\n\nBootstrap sampling relies on a similar principle:\n\\[ \\text{Var}_{\\P_n}[f(X_1, \\dots, X_n)] \\to \\text{Var}_{\\P}[f(X_1, \\dots, X_n)]\\]\nHere, \\(\\text{Var}_{\\P}[f(X_1, \\dots, X_n)]\\) is the sampling variance of the estimator \\(f\\) under the true distribution, and \\(\\text{Var}_{\\P_n}[f(X_1, \\dots, X_n)]\\) is the variance of that same estimator under the sample distribution. What exactly does this last claim mean? Essentially, if we repeat \\(f\\) on our sample data many times, the variance will approximate the “true” sampling variance.\nBut if we simply repeat \\(f\\) on the same data, we get the same answer each time - so where is the variance? This is where the magic of the bootstrap comes in. Instead of using the sample data as is, we resample it in the following manner.\n\nGiven data \\(\\mathcal{D} = \\{x_1, \\dots, x_n\\}\\), create a new sample \\(\\mathcal{D}^\\#_1 = \\{x^\\#_1, x^\\#_2, \\dots, x^\\#_n\\}\\) by drawing from the original data with replacement. (If you draw without replacement, you just get back the original data in full every time, possibly permuted.) This new sample \\(\\mathcal{D}^\\#_1\\) is the same size as \\(\\mathcal{D}\\), so we can apply \\(f\\) to it unchanged. \\(\\mathcal{D}^\\#_1\\) is called a boostrap sample.\nCompute \\(f\\) on the bootstrap sample to get \\(f^\\#_1\\).\nRepeat this process many times to get bootstrap samples of \\(f\\): \\(f^\\#_1, \\dots, f^\\#_B\\) for some large \\(B\\) (typically around 200-500).\nThe variance of \\(f^\\#_1, \\dots, f^\\#_B\\) is an approximation of the sampling variance of \\(f\\) under the data-generating process.\n\nThis is all a bit heady, but the key point to keep in mind is this: as we get more data, our sample is a good estimate of the true distribution, so variance from resampling our data from our sample approximates variance from resampling our sample from the population.\nLet’s put this into practice: suppose we have \\(250\\) samples from an unknown distribution and we want to estimate the median of the distribution using the sample median. Computing the variance of the sample median is rather tricky, but asymptotically it approaches \\(1/(4n f_X(m_X)^2)\\) where \\(m_X\\) is the true median. This formula is essentially useless however if we don’t assume a parametric model for \\(f_X\\), so we instead resort to bootstrapping.\nTo demonstrate this, let’s suppose \\(X\\) comes from a non-central \\(\\chi^2\\) distribution with \\(\\pi\\) degrees of freedom and non-centrality parameter \\(e^2\\). (This is a very weird distribution!)\n\nset.seed(100)\nDATA &lt;- rchisq(250, df=pi, ncp = exp(2))\nSAMPLE_MEDIAN &lt;- median(DATA)\n\nThe median of this distribution is too complex for Wikipedia, but we can compute it empirically using a very large sample:\n\nTRUE_MEDIAN &lt;- median(rchisq(5e7, df=pi, ncp = exp(2)))\n\nSo our sample median (10.74) is a bit off from the true median (9.58) but not catastrophically so. How can we estimate the variance? By bootstrapping!\nWe can implement a bootstrap in dplyr as follows:\n\nB &lt;- 500 # Number of boostrap samples to create\nn &lt;- length(DATA) # Original data size\n\nexpand_grid(B = 1:B, \n            n = 1:n) |&gt;\n    # Notice here we sample _with replacement_ from DATA\n    mutate(x = sample(DATA, n(), replace = TRUE)) |&gt;\n    group_by(B) |&gt;\n    summarize(f_boot = median(x)) |&gt;\n    summarize(var_f = var(f_boot)) |&gt;\n    pull(var_f)\n\n[1] 0.1911888\n\n\nWe can compare this to the CLT-asymptotic variance:\n\n1/(4 * n * dchisq(TRUE_MEDIAN, df=pi, ncp = exp(2))^2)\n\n[1] 0.2121155\n\n\nAnd, since we’re in simulation land, we also have the true variance:\n\nvar(replicate(10000, {\n    median(rchisq(250, df=pi, ncp = exp(2)))\n}))\n\n[1] 0.2082013\n\n\nThe bootstrap variance is a little bit too high, but it’s surprisingly good! And we didn’t have to know anything about the true distribution of the data! I’d call that a win.\nIn general, we can estimate the variability of any function of data this way: i) create bootstrap samples by resampling the original data with replacement, ii) apply the function of interest to each of the bootstrap samples, and iii) take the bootstrap variance as an approximation to the “true” variance.\nThis strategy is particularly useful for this project, as we don’t want to make any modeling assumptions on the returns of the stock market (a famously hard problem!) other than assuming the future looks something like the past.\nBootstrapping is one form of a general computational paradigm known as Monte Carlo, wherein computers are given randomized inputs and told to evaluate complex functions on those inputs. The various randomized outputs are then analyzed to give insight into the behavior of the complex function. Monte Carlo methods are popular because:\n\nComputers are cheap, but thinking is hard.\nThey can treat the function of interest in a “black box” manner, making them suitable for use with complex computer simulation models.\nThey do not require the use of statistical asymptotics or, when performed with resampling, any distributional assumptions at all.\n\nThe name “Monte Carlo” is a reference to the famous casino of the same name in Monaco. While it is hard to evaluate odds of success in complex games of chance, by playing the game many times over and collecting results, it is straightforward to accurately compute the house edge. In the (computational) Monte Carlo approach, we replace the roulette wheel with a computer’s random number generator, but the same principles apply.\nBootstrapping Complex Data\nIn the example above, we had IID scalar data so a simple call to sample(replace=TRUE) was sufficient to implement a proper bootstrap. When dealing with multivariate data, e.g. \\((x, y)\\) pairs in a regression model, we need to “jointly” resample pairs or else we will break the underlying correlation.8 Thankfully, dplyr provides us with a slice_sample function which can be used to do esentially the same thing.\nFor example, suppose we are interested in assessing the uncertainty associated with the Kendall correlation of two variables. This is a rather non-linear function of the data and classical tools like Fisher’s transform for standard (Pearson) correlation can’t be easily applied. Bootstrapping works perfectly well without much additional effort, as we show below.\nWe generate \\((x, y)\\) pairs with a visible, but not precisely linear, relationship.\n\nx &lt;- rchisq(100, df=3, ncp=2)\ny &lt;- x * sin(2 * x) + 15 * log(x)\nplot(x, y)\n\n\n\n\n\n\n\nThe Kendall correlation is easily computed:\n\ncor(x, y, method=\"kendall\")\n\n[1] 0.7927273\n\n\nTo put a confidence interval on this, we can use a bootstrap with \\(B=400\\) replicates:\n\nstopifnot(length(x) == length(y))\nn_samp &lt;- length(x)\nn_boot &lt;- 400\n\ndata.frame(x = x, y = y) |&gt;\n    slice_sample(n = n_samp * n_boot, \n                 replace=TRUE) |&gt;\n    mutate(resample_id = rep(1:n_boot, times=n_samp)) |&gt;\n    group_by(resample_id) |&gt;\n    summarize(kendall_cor = cor(x, y, method=\"kendall\")) |&gt;\n    summarize(var(kendall_cor))\n\n# A tibble: 1 × 1\n  `var(kendall_cor)`\n               &lt;dbl&gt;\n1            0.00181\n\n\nWe can again compare this to the “true” sampling variance since we have access to the data-generating model.\n\nvar(replicate(5000, {\n    x &lt;- rchisq(100, df=3, ncp=2)\n    y &lt;- x * sin(2 * x) + 15 * log(x)\n    cor(x, y, method=\"kendall\")\n}))\n\n[1] 0.001408479\n\n\nAgain - really close! And we don’t need to know anything about the joint distribution of \\((x, y)\\) to apply the bootstrap.\nYou will need to use this “row-resampling” bootstrap to maintain the underlying economic relationships between the various investment returns and macroeconomic quantities you consider in this exercise.9"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini04.html#mini-project-objectives",
    "href": "archive/AY-2024-FALL/miniprojects/mini04.html#mini-project-objectives",
    "title": "STA 9750 Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Mini-Project Objectives",
    "text": "Mini-Project Objectives\nIn this project, you will use historical economic data and a bootstrap resampling strategy to see the distribution of retirement outcomes under different (resampled) economic histories. You will collect relevant economic data including:\n\nRate of Inflation\nRate of Wage Growth\nUS Equity Market Returns\nInternational Equity Market Returns\nBond Returns\nShort Term Debt Returns\n\nYou will resample these to create artificial histories and apply the TRS and ORP rules to each strategy to determine i) how much wealth you have going into retirement; ii) your probability of running out of funds before death; and iii) the amount of funds (if any) you leave to your heirs.\nThe optimal strategy may depend on various parameters including: i) your age when you start saving; ii) your age when you retire; iii) the age at which you shuffle off this mortal coil; and iv) your starting salary.\nYou may select values that seem appropriate to you for each of these or, if you are feeling ambitious, you can perform a sensitivity analysis to see how robust your final decision is to changes in each of these inputs.\nAfter you simulate various “Monte Carlo” histories to assess the relative performance of the TRS and ORP retirement plans, you will make a data-driven decision recommendation to a potential CUNY employee. (In essence, you will play the role of a financial advisor.) Your final report should include a plan recommendation, as well as some notion of confidence or certainty associated with your recommendation."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini04.html#student-responsbilities",
    "href": "archive/AY-2024-FALL/miniprojects/mini04.html#student-responsbilities",
    "title": "STA 9750 Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Student Responsbilities",
    "text": "Student Responsbilities\nRecall our basic analytic workflow and table of student responsibilities:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn this mini-project, you are in charge of the whole pipeline, from deciding which data to use, to finding the data on AlphaVantage and/or FRED, to downloading it into R, to performing the resampled history analyses, to implementing the TRS and ORP savings rules, and to finally interpreting the results. The rubric below evaluates your work on all aspects of this project.\nRubric\nSTA 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.\nCode completes all instructor-provided tasks satisfactorially.\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three or ore instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nReport exhibits particularly creative insights drawn from thorough student-initiated analyses.\n\n\nFormatting & Display\n\nTables and figures are full ‘publication-quality’.\nReport includes at least one animated visualization designed to effectively communicate findings.\n\n\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation.\nFigures are ‘publication-quality’, with suitable axis labels, well-chosen structure, attractive color schemes, titles, subtitles, and captions, etc.\n\n\nTables are well-formatted, but still have room for improvement.\nFigures are above ‘exploratory-quality’, but do not reach full ‘publication-quality’.\n\n\nTables lack significant ‘polish’ and need improvement in substance (filtering and down-selecting of presented data) or style.\nFigures are suitable to support claims made, but are ‘exploratory-quality’, reflecting minimal effort to customize and ‘polish’ beyond ggplot2 defaults.\n\n\nUnfiltered ‘data dump’ instead of curated table.\nBaseline figures that do not fully support claims made.\n\nReport includes interactive (not just animated) visual elements.\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nData import is fully-automated and efficient, taking care to only download from web-sources if not available locally.\nData is imported and prepared effectively, in an automated fashion with minimal hard-coding of URLs and file paths.\nData is imported and prepared effectively, though source and destination file names are hard-coded.\nData is imported in a manner likely to have errors.\nData is hard-coded and not imported from an external source.\nReport uses additional data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp04.qmd so the rendered document can be found at docs/mp04.html in the student’s repository and served at the URL:\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp04.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2025-SPRING/issues/new .\n\nTitle the issue STA 9750 &lt;GITHUB_USERNAME&gt; MiniProject #03 and fill in the following text for the issue:\nHi @michaelweylandt!\n\n\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2025-SPRING/mp04.html\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini04.html#set-up-and-exploration",
    "href": "archive/AY-2024-FALL/miniprojects/mini04.html#set-up-and-exploration",
    "title": "STA 9750 Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Set-Up and Exploration",
    "text": "Set-Up and Exploration\nData Acquisition\nTo begin your Monte Carlo analysis, you will need historical data covering (at a minimum) the following:\n\nWage growth\nInflation\nUS Equity Market total returns\nInternational Equity Market total returns\nBond market total returns\nShort-term debt returns10\n\n\n\n\n\n\n\n\nTask 3: Data Acquisition\n\n\n\nIdentify and download historical data series for each of the above inputs to your Monte Carlo analysis. If necessary, “downsample” each series to a monthly frequency and join them together in a data.frame.\nYou must use at least one data series from AlphaVantage and one from FRED. You must use the APIs of each service to access this data and, as noted above, you need to use the “raw” API, relying only on the httr2 package (or similar) and not wrapper packages like quantmod or alphavantager.\n\n\nNote that, for each of these quantities, there are many possibly-relevant data series: e.g., for inflation, you might compare CPI, core CPI, PCE, both nationally and, if available, in the NY metro area. You may select any series you feel best captures these for a potential CUNY employee. For the market returns, it may be easiest to identify a suitable index ETF and compute its (dividend-adjusted) returns as a proxy for market returns.\nIn any historically-based financial projection, there is a trade-off between having enough history to capture sufficient market cycles and having only relevant data in your training set. I’d recommend using around 15-20 years of data for this project.\nInvestigation and Visualization of Input Data\n\n\n\n\n\n\nTask 4: Initial Analysis\n\n\n\nAfter you have acquired your input data, perform some basic exploratory data analysis to identify key properties of your data. You may choose to measure the correlation among factors, long-term averages, variances, etc. Your analysis should include at least one table and one figure.\nAs part of your analysis, be sure to compute the long-run monthly average value of each series. You will use these in a later task.\n\n\nHistorical Comparison of TRS and ORP\n\n\n\n\n\n\nTask 5: Historical Comparison\n\n\n\nNow that you have acquired data, implement the TRS and ORP formulas above and compare the value of each of them for the first month of retirement. To do this, you may assume that your hypothetical employee:\n\nJoined CUNY in the first month of the historical data\nRetired from CUNY at the end of the final month of data\n\nYou will need to select a starting salary for your employee. Use historical data for wage growth and inflation and assume that the TRS and ORP parameters did not change over time. (That is, the employee contribution “brackets” are not inflation adjusted; the employee will have to make larger contributions as income rises over the span of a 20+ year career.)\n\n\nLong-Term Average Analysis\nThe “first month of retirement” dollar value is interesting, but it arguably undersells a key strength of the TRS. The TRS guarantees income for life, while the ORP can be exhausted if the employee lives a very long time in retirement.\n\n\n\n\n\n\nTask 6: Fixed-Rate Analysis\n\n\n\nModify your simulation from the previous section to project an employee’s pension benefit (TRS) or withdrawal amount (ORP) from retirement until death. (You will need to select an estimated death age.) In order to implement cost-of-living-adjustments (TRS) and future market returns (ORP), you can use the long-run averages you computed previously. This “fixed rate” assumption is rather limiting, but we will address it below.\nAs you compare the plans, be sure to consider:\n\nWhether the employee runs out of funds before death and/or has funds to leave to heirs (ORP only)\nAverage monthly income (TRS vs ORP)\nMaximum and minimum gap in monthly income between TRS and ORP\n\nAs noted above, you can ignore the effect of taxes throughout this analysis.\n\n\nBootstrap (Monte Carlo) Comparison\nNow that you have implemented both the “while working” contributions and returns (ORP) only as well as the “while retired” benefits of both plans, we are finally ready to implement our Monte Carlo assessment.\n\n\n\n\n\n\nTask 7: Monte Carlo Analysis\n\n\n\nUsing your historical data, generate several (at least 200) “bootstrap histories” suitable for a Monte Carlo analysis. Use bootstrap sampling, i.e. sampling with replacement, to generate values for both the “while working” and “while retired” periods of the model; you do not need to assume constant long-term average values for the retirement predictions any more.\nApply your calculations from the previous two tasks to each of your simulated bootstrap histories. Compare the distribution of TRS and ORP benefits that these histories generate. You may want to ask questions like the following:\n\nWhat is the probability that an ORP employee exhausts their savings before death?\nWhat is the probability that an ORP employee has a higher monthly income in retirement than a TRS employee?\nIs the 4% withdrawal rate actually a good idea or would you recommend a different withdrawal rate?\n\nReport your findings to these or other questions of interest in tables or figures, as appropriate."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini04.html#deliverable-data-driven-decision-recommendation",
    "href": "archive/AY-2024-FALL/miniprojects/mini04.html#deliverable-data-driven-decision-recommendation",
    "title": "STA 9750 Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Deliverable: Data-Driven Decision Recommendation",
    "text": "Deliverable: Data-Driven Decision Recommendation\nFinally, write up your findings from Task 7 in the form of a “data-driven recommendation” to a potential CUNY employee. Here, you are playing the role of a financial advisor, so be sure to consider the employee’s current age and starting salary, expected lifetime, and risk tolerance. Be sure to suitably convey the uncertainty of your predictions and the limitations of the bootstrap-history approach used here.11 As you write this, think of what issues would matter most to you if you were making this decision and address them accordingly."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini04.html#extra-credit-opportunities",
    "href": "archive/AY-2024-FALL/miniprojects/mini04.html#extra-credit-opportunities",
    "title": "STA 9750 Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Extra Credit Opportunities",
    "text": "Extra Credit Opportunities\nFor extra credit, you may make an interactive version of your report, allowing your client to alter the parameters of your simulation and see how the predictions change.\nChallenge Level: Basic (Up to 5 points Extra Credit)\nPerform a “sensitivity analysis” by re-running your previous analysis under various different input parameters (starting salary, retirement age, death age, etc.) Then use some sort of interactive functionality to allow the reader to see how the results change.\nThe manipulateWidgets package may be useful here, but any sort of in-browser interactive display will suffice.\nNote that, in this model, all the simulations are run by Quarto at Render time and the interactivity only controls which simulations are displayed.\nChallenge Level: Moderate (Up to 10 points Extra Credit)\nUse the shiny package to implement a reactive dashboard. shiny requires use of a server to perform calculations. The website shinyapps.io provides a free platform to host the “backend” of your shiny dashboard. This example may prove useful, but note that the analysis required for this project (historical resampling) is a bit more advanced than the parametric model used there.\nUnder the shiny model, a back-end server is running (and re-running) simulations in real-time in response to user input.\nChallenge Level: Advanced (Up to 20 points Extra Credit)\nUse the r-shinylive framework to create a fully dynamic in-browser simulation dashboard. This in-development technology allows users to modify and re-run all simulations in their browser, providing the highest level of flexibility. You can allow users to vary their starting salary, retirement age, choice of data series, number of Monte Carlo histories, dates of historical data used for resampling, etc.\nNote that r-shinylive is a new technology and one that remains under active development. The instructor will not be able to provide support and assistance debugging it.\n\nThis work ©2024 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini04.html#footnotes",
    "href": "archive/AY-2024-FALL/miniprojects/mini04.html#footnotes",
    "title": "STA 9750 Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Footnotes",
    "text": "Footnotes\n\nWhile I don’t imagine many of you will have this exact decision to make, these forecasting and analysis strategies are readily applied to a variety of personal financial planning decisions you might face in future careers.↩︎\nCUNY employees have additional voluntary retirement savings plans, but these are non-exclusive and participation is voluntary, so we omit them from this analysis.↩︎\nIf a TRS participant retires before the age of 63, their benefit is reduced, but you can ignore this wrinkle.↩︎\nThe original version of this assignment incorrectly had \\(N\\) instead of \\(N-20\\) for the 2% term in the 30+ year service calculation. Students using the incorrect formula won’t be penalized.↩︎\nThis is simplified from the actual rates used by Fidelity, but it’s close to accurate.↩︎\nCurrent starting salary for an Assisant Professor↩︎\nIf you accidentally include this file in your Git history, see these instructions for discussion of how to remove it. These are dangerous and advanced git “power tools”, so it will be far easier to use .gitignore defensively rather↩︎\n“Breaking” the correlation is not necessarily a bad thing. This essentially underlies the concept of permutation testing, but it’s not what we’re aiming to do here.↩︎\nClearly, the bootstrap as we have described here looses any temporal structure in the data. The so-called block bootstrap can be used to bootstrap in a way that (approximately) respects temporal dependence, but you don’t need to implement this advanced variant in this project.↩︎\nFor short-term debt, it may be easiest to pick a key short-term benchmark, e.g., the 2-year US Treasury yield. The world of “short-term debt” is rather wide and varied.↩︎\nAs the SEC requires all advisors to disclaim: Past Performance is No Guarantee of Future Results.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini03.html",
    "href": "archive/AY-2024-FALL/miniprojects/mini03.html",
    "title": "STA 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "Released to Students: 2024-10-24\nInitial Submission: 2024-11-13 11:45pm ET on GitHub and Brightspace\n\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-11-14 on GitHub\nPeer Feedback Due: 2024-11-20 11:45pm ET on GitHub"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini03.html#introduction",
    "href": "archive/AY-2024-FALL/miniprojects/mini03.html#introduction",
    "title": "STA 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Introduction",
    "text": "Introduction\nWelcome to Mini-Project #03! In this project, you will write a political fact-check, that most iconic form of our current journalistic era. Specifically, you will investigate the claim that the US Electoral College systematically biases election results away from the vox populi. As you dive in to the world of political data, we’ll also learn a bit more about the mechanics of US federal elections.\nIn this Mini-Project, you will:\n\nIntegrate data from disparate governmental and academic sources\nLearn to work with spatial data formats\nCreate many plots\nUse spatial and animated visualizations to make your argument\n\nNote that - as with all these mini-projects - there isn’t a single “right” answer to the questions posed herein. You may have different views about the relative importance of federalism, direct democratic structures, adherence to the formal structures of the US Constitution, etc. than your classmates. Please make sure to make your argument respectfully and, when we reach the peer-evaluation stage, read and comment respectfully. All grading will be done solely on the quality of the code, the writing, the visualizations, and the argument - not on the political implications of what you may or may not find.\nAlso note that this mini-project is intended to be markedly less demanding than Mini-Project #02. At this point in the course, you should be diving into your Course Project, which should consume the majority of your out-of-class time dedicated to this course for the remainder of the semester."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini03.html#background",
    "href": "archive/AY-2024-FALL/miniprojects/mini03.html#background",
    "title": "STA 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Background",
    "text": "Background\nThe US Constitution sets the basic rules of electing the President in Section 1 of Article II, which we quote here in part:\n\nEach State shall appoint, in such Manner as the Legislature thereof may direct, a Number of Electors, equal to the whole Number of Senators and Representatives to which the State may be entitled in the Congress: but no Senator or Representative, or Person holding an Office of Trust or Profit under the United States, shall be appointed an Elector.\nThe Electors shall meet in their respective States, and vote by Ballot for two Persons, of whom one at least shall not be an Inhabitant of the same State with themselves. And they shall make a List of all the Persons voted for, and of the Number of Votes for each; which List they shall sign and certify, and transmit sealed to the Seat of the Government of the United States, directed to the President of the Senate. The President of the Senate shall, in the Presence of the Senate and House of Representatives, open all the Certificates, and the Votes shall then be counted. The Person having the greatest Number of Votes shall be the President, if such Number be a Majority of the whole Number of Electors appointed; and if there be more than one who have such Majority, and have an equal Number of Votes, then the House of Representatives shall immediately chuse by Ballot one of them for President; and if no Person have a Majority, then from the five highest on the List the said House shall in like Manner chuse the President. But in chusing the President, the Votes shall be taken by States, the Representation from each State having one Vote; A quorum for this Purpose shall consist of a Member or Members from two thirds of the States, and a Majority of all the States shall be necessary to a Choice. In every Case, after the Choice of the President, the Person having the greatest Number of Votes of the Electors shall be the Vice President. But if there should remain two or more who have equal Votes, the Senate shall chuse from them by Ballot the Vice President.\n\nThough the details have varied over time due to amendment, statue, and technology, this basic outline of this allocation scheme remains unchanged:\n\nEach state gets \\(R + 2\\) electoral college votes, where \\(R\\) is the number of Representatives that state has in the US House of Representatives. In this mini-project, you can use the number of districts in a state to determine the number of congressional representatives (one per district).\nStates can allocate those votes however they wish\nThe president is the candidate who receives a majority of electoral college votes\n\nNotably, the Constitution sets essentially no rules on how the \\(R + 2\\) electoral college votes (ECVs) for a particular state are allocated. At different points in history, different states have elected to use each of the following:\n\nDirect allocation of ECVs by state legislature (no vote)\nAllocation of all ECVs to winner of state-wide popular vote\nAllocation of all ECVs to winner of nation-wide popular vote\n\nAllocation of \\(R\\) ECVs to popular vote winner by congressional district + allocation of remaining \\(2\\) ECVs to the state-wide popular vote winner\n\nCurrently, only Maine and Nebraska use the final option; the other 48 states and the District of Columbia award all \\(R+2\\) ECVs to the winner of their state-wide popular vote. We emphasize here that “statewide winner-take-all” is a choice made by the individual states, not dictated by the US constitution, and that states have the power to change it should they wish.1\nTo my knowledge, no US state uses true proportionate state-wide representation, though I believe such a ECV-allocation scheme would be consistent with the US Constitution. For example, if a state with 5 ECVs had 60,000 votes for Candidate A and 40,000 cast for Candidate B, it could award 3 ECVs to A and 2 to B, regardless of the spatial distribution of those votes within the state."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini03.html#mini-project-objectives",
    "href": "archive/AY-2024-FALL/miniprojects/mini03.html#mini-project-objectives",
    "title": "STA 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Mini-Project Objectives",
    "text": "Mini-Project Objectives\nIn this project, you will use historical congressional election data to see how the outcome of US presidential elections would have changed under different allocation rules. Like any retrodiction2 task, this analysis has limitations. Notably, if the “rules” had been different, politicians may have run different campaigns and received different vote counts. Still, it is my hope that this is an interesting and informative exercise.\nAs noted above, your final submission should take the form of a “Fact Check”:\n\nTake a statement from a well-known politician or political commentator describing (claimed) bias of the electoral college system\nAnalyze presidential election results under different allocations for presence or abscence of bias (however you define it - see below)\nSummarize your retrodictive findings\nAward a “truthfulness” score to the claim you evaluated. (You may use the scale of an existing political fact-check operation or create your own.)"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini03.html#student-responsbilities",
    "href": "archive/AY-2024-FALL/miniprojects/mini03.html#student-responsbilities",
    "title": "STA 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Student Responsbilities",
    "text": "Student Responsbilities\nRecall our basic analytic workflow and table of student responsibilities:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn this mini-project, you will be working with relatively “clean” electoral data and your main focus should be on the analysis and visualization supporting your fact check. As an analysis of political data, I expect your final submission to have quite a few “red state/blue state” maps.3 Data cleaning and import will play a larger role in Mini-Project #04.\nIn this project, I am no longer providing code to download and read the necessary data files. The data files I have selected for this mini-project are relatively easy to work with and should not provide a significant challenge, particularly after our in-class discussion of Data Import. See the modified rubric below which now includes a grade for data import.\nRubric\nSTA 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.\nCode completes all instructor-provided tasks satisfactorially.\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three or ore instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nReport exhibits particularly creative insights drawn from thorough student-initiated analyses.\n\n\nFormatting & Display\n\nTables and figures are full ‘publication-quality’.\nReport includes at least one animated visualization designed to effectively communicate findings.\n\n\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation.\nFigures are ‘publication-quality’, with suitable axis labels, well-chosen structure, attractive color schemes, titles, subtitles, and captions, etc.\n\n\nTables are well-formatted, but still have room for improvement.\nFigures are above ‘exploratory-quality’, but do not reach full ‘publication-quality’.\n\n\nTables lack significant ‘polish’ and need improvement in substance (filtering and down-selecting of presented data) or style.\nFigures are suitable to support claims made, but are ‘exploratory-quality’, reflecting minimal effort to customize and ‘polish’ beyond ggplot2 defaults.\n\n\nUnfiltered ‘data dump’ instead of curated table.\nBaseline figures that do not fully support claims made.\n\nReport includes interactive (not just animated) visual elements.\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nData import is fully-automated and efficient, taking care to only download from web-sources if not available locally.\nData is imported and prepared effectively, in an automated fashion with minimal hard-coding of URLs and file paths.\nData is imported and prepared effectively, though source and destination file names are hard-coded.\nData is imported in a manner likely to have errors.\nData is hard-coded and not imported from an external source.\nReport uses additional data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp03.qmd so the rendered document can be found at docs/mp03.html in the student’s repository and served at the URL:\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2025-SPRING/mp03.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2025-SPRING/issues/new .\n\nTitle the issue STA 9750 &lt;GITHUB_USERNAME&gt; MiniProject #03 and fill in the following text for the issue:\nHi @michaelweylandt!\n\n\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2025-SPRING/mp03.html\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini03.html#set-up-and-initial-exploration",
    "href": "archive/AY-2024-FALL/miniprojects/mini03.html#set-up-and-initial-exploration",
    "title": "STA 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Set-Up and Initial Exploration",
    "text": "Set-Up and Initial Exploration\nData I: US House Election Votes from 1976 to 2022\nThe MIT Election Data Science Lab collects votes from all biennial congressional races in all 50 states here. Download this data as a CSV file using your web browser. Note that you will need to provide your contact info and agree to cite this data set in your final report.4 Make sure to include this citation!\nAdditionally, download statewide presidential vote counts from 1976 to 2022 here. As before, it will likely be easiest to download this data by hand using your web browser.\nData II: Congressional Boundary Files 1976 to 2012\nJeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis have created shapefiles for all US congressional districts from 1789 to 2012; they generously make these available here.\n\n\n\n\n\n\nTask 1: Download Congressional Shapefiles 1976-2012\n\n\n\nDownload congressional shapefiles from Lewis et al. for all US Congresses5 from 1976 to 2012.\nYour download code should:\n\nBe fully automated (no “hand-downloading”);\nDownload files with a systematic and interpretable naming convention\nOnly download files as needed out of courtesy for the data provider’s web sever. That is, if you already have a copy of the file, do not re-download it repeatedly.\n\nAs with the other Mini-Projects, make sure you do not store these data files in git. It will be sufficient to include the qmd file with the download code.\n\n\nNote that the shape files are distributed as zip folders, containing several files in a directory structure. We will be interested in the shp files within each zip.\nData III: Congressional Boundary Files 2014 to Present\nTo get district boundaries for more recent congressional elections, we can turn to the US Census Bureau. Unfortunately, these data - while authoritative and highly detailed - are not in quite the same format as our previous congressional boundary files. We can review the US Census Bureau shapefiles online. To download them automatically, I recommend exploring the FTP Archive link near the bottom of the page. In Census-jargon, the CD directory will have shapefiles for Congressional Districts for each year.6\n\n\n\n\n\n\nTask 2: Download Congressional Shapefiles 2014-2022\n\n\n\nDownload congressional shapefiles from the US Census Bureau for all US Congresses from 2014 to 2022.\nYour download code should:\n\nBe fully automated (no “hand-downloading”);\nDownload files with a systematic and interpretable naming convention\nOnly download files as needed out of courtesy for the data provider’s web sever. That is, if you already have a copy of the file, do not re-download it repeatedly.\n\nAs with the other Mini-Projects, make sure you do not store these data files in git. It will be sufficient to include the qmd file with the download code.\n\n\nInitial Exploration of Vote Count Data\n\n\n\n\n\n\nTask 3: Exploration of Vote Count Data\n\n\n\nAnswer the following using the vote count data files from the MIT Election Data Science Lab. You may answer each with a table or plot as you feel is appropriate.\n\nWhich states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\nNew York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent).\nAre there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\n\n\nDo presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state?\nDoes this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\n\n\n\nImporting and Plotting Shape File Data\nAs mentioned above, the shape files you downloaded above are distributed in zip archives, with several files. We only need the shp file within each archive. In this section, we’ll practice extracting the shp file, reading it, and using it to create a plot. The key library we need is the sf (“simple features”) library. It provides the read_sf() function which we can use to read it into R. I download how this works below:\n\nlibrary(ggplot2)\nlibrary(sf)\n\nif(!file.exists(\"nyc_borough_boundaries.zip\")){\n    download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\", \n              destfile=\"nyc_borough_boundaries.zip\")\n}\n\n##-\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"nyc_borough_boundaries.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_sf &lt;- read_sf(fname_shp)\nnyc_sf\n\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49613 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS84(DD)\n# A tibble: 5 × 5\n  boro_code boro_name      shape_area shape_leng                        geometry\n      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;              &lt;MULTIPOLYGON [°]&gt;\n1         3 Brooklyn      1934142776.    728147. (((-73.86327 40.58388, -73.863…\n2         5 Staten Island 1623618684.    325910. (((-74.05051 40.56642, -74.050…\n3         1 Manhattan      636646082.    360038. (((-74.01093 40.68449, -74.011…\n4         2 Bronx         1187174772.    463181. (((-73.89681 40.79581, -73.896…\n5         4 Queens        3041418004.    888197. (((-73.82645 40.59053, -73.826…\n\n\n\n\n\n\n\n\nZipfile Download Corruption\n\n\n\n\n\nAt least one student reported difficulty running the above code on a Windows machine. The default download method (method=\"internal\") uses Windows’ built-in download code, which seems to randomly corrupt certain zip files. Adding method=\"curl\" to the download.file call seems to have helped.\nSimilarly, using http instead of https in the URL sometimes avoids issues, particularly on Windows machines with aggressive anti-virus settings.\nThese tweaks may not work for all of you, since the root cause of these errors are a subtle interplay between the opearting system, specific security software (and settings within the security software) and R’s download functionality, but these tweaks may help to resolve mysterious errors.\n\n\n\n\n\n\n\n\n\nTask 4: Automate Zip File Extraction\n\n\n\nAdapt the code after the ##- symbol above into a function read_shp_from_zip() which takes in a file name, pulls out the .shp file contained there in, and reads it into R using read_sf().\nNote: If your platform supports it, you can also use a combination of unzip(..., list=TRUE) and unzip(..., file=...) to extract only one file out of the zip directory instead of unpacking the whole file. This is a bit more efficient, but not necessary here as all files involved are pretty small.\n\n\nThe result of this is a particular sort of data frame. The most important column for us is the geometry column which is of type MULTIPOLYGON. This is, essentially, a list of GPS coordinates which outline a spatial region. Here, each row corresponds to a Borough of NYC. We can pass the geometry column to ggplot2 to make a map:\n\nggplot(nyc_sf, \n       aes(geometry=geometry)) + \n    geom_sf()\n\n\n\n\n\n\n\nHere, we use the sf geom to get the shape outlines. The sf geom plays well with the fill aesthetic.\n\nggplot(nyc_sf, \n       aes(geometry=geometry, \n           fill = shape_area)) + \n    geom_sf()\n\n\n\n\n\n\n\nThis type of plot is called a Chloropleth Map and it is commonly used to depict election results.\n\n\n\n\n\n\nTask 5: Chloropleth Visualization of the 2000 Presidential Election Electoral College Results\n\n\n\nUsing the data you downloaded earlier, create a chloropleth visualization of the electoral college results for the 2000 presidential election (Bush vs. Gore), coloring each state by the party that won the most votes in that state. Your result should look something like this:\n\nTaken from Wikipedia\nIt is not required, but to make the very best plot, you may want to look up:\n\nHow to “inset” Alaska and Hawaii instead of plotting their true map locations.\nHow to add labels to a chloropleth in ggplot2\n\nHow to label the small states in the North-East\n\nbut these steps are not required as they are a bit advanced.\n\n\n\n\n\n\n\n\nTask 6: Advanced Chloropleth Visualization of Electoral College Results\n\n\n\nModify your previous code to make either an animated faceted version showing election results over time.\nYou may want to set facet_wrap or facet_grid to use a single column and adjust the figure size for the best reading experience.\n\n\n\n\n\n\n\n\nSome Subtleties of Working with Complex Shapefiles\n\n\n\n\n\nThere are some subtle issues you might need to be aware of when working with multiple complex shapefiles:\n\nbind_rows will struggle to combine shape files if they are not using the same Coordinate Reference System (CRS). You might want to use st_transform to set all CRS to be the same. CRS 4326, a.k.a. WGS 84 is a good choice.\n\nPlotting complex shapefiles may be slow due to the intricate coastlines and lots of fiddly line segments on state borders. You may want to use st_simplify to make smoother (and more quickly plotted) edge sets. I found st_simplify(dTolerance=0.01) to work decently well, but you may find different values work better.\nFor technical reasons, you may need to set sf_use_s2(FALSE) before using st_simplify.\n\nJoining two sf objects is tricky. If you don’t need geometry from both tables, it is easier to remove the geometry column from one and then apply the as_data_frame function to simplify its structure. This will allow regular (non-spatial) joins to be used.\n\n\n\n\nThe following example may be useful for you:\n\n## Animated Chloropleth using gganimate\n\n## Add some time \"structure\" to our data for \n## demonstration purposes only\nnyc_sf_repeats &lt;- bind_rows(\n    nyc_sf |&gt; mutate(value = rnorm(5), \n                     frame = 1), \n    nyc_sf |&gt; mutate(value = rnorm(5), \n                     frame = 2), \n    nyc_sf |&gt; mutate(value = rnorm(5), \n                     frame = 3), \n    nyc_sf |&gt; mutate(value = rnorm(5), \n                     frame = 4), \n    nyc_sf |&gt; mutate(value = rnorm(5), \n                     frame = 5))\n\nlibrary(gganimate)\nggplot(nyc_sf_repeats, \n       aes(geometry=geometry, \n           fill = value)) + \n    geom_sf() + \n    transition_time(frame)\n\n\n\n\n\n\n\nNow that we have finished exploring our data and building some tools for plots, we are ready to dig into our main question."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini03.html#comparing-the-effects-of-ecv-allocation-rules",
    "href": "archive/AY-2024-FALL/miniprojects/mini03.html#comparing-the-effects-of-ecv-allocation-rules",
    "title": "STA 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Comparing the Effects of ECV Allocation Rules",
    "text": "Comparing the Effects of ECV Allocation Rules\nGo through the historical voting data and assign each state’s ECVs according to various strategies:\n\nState-Wide Winner-Take-All\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nState-Wide Proportional\nNational Proportional\n\nBased on these allocation strategies, compare the winning presidential candidate with the actual historical winner.\nWhat patterns do you see? Are the results generally consistent or are one or more methods systematically more favorable to one party?\nFor the district-level winner-take-all, you may assume that the presidential candidate of the same party as the congressional representative wins that election.\n\n\n\n\n\n\nTask 7: Evaluating Fairness of ECV Allocation Schemes\n\n\n\nWrite a fact check evaluating the fairness of the different ECV electoral allocation schemes.\nTo do so, you should first determine which allocation scheme you consider “fairest”. You should then see which schemes give different results, if they ever do. To make your fact check more compelling, select one election where the ECV scheme had the largest impact–if one exists–and explain how the results would have been different under a different ECV scheme.\nAs you perform your analysis, you may assume that the District of Columbia has three ECVs, which are allocated to the Democratic candidate under all schemes except possibly national popular vote.7\n\n\nThroughout all of this, note that we are not varying the \\(R+2\\) ECV allocation scheme specified by the constitution. Our concern here is only what individual states can do to address “fairness” in presidential elections. If we allow the possibility of constitutional amendment, the possibilities are endless. The \\(R+2\\) rule has several interesting effects; some are well-known, such as the Senate’s equal treatment of small and large states, while others are less well-known, including the fact that congressional representation is based on population, not counts of voters.8"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini03.html#extra-credit-opportunities",
    "href": "archive/AY-2024-FALL/miniprojects/mini03.html#extra-credit-opportunities",
    "title": "STA 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Extra Credit Opportunities",
    "text": "Extra Credit Opportunities\n\n\n\n\n\n\nExtra Credit Opportunity (Up to 5 points)\n\n\n\nFor extra credit, extend your analysis to 2024 electoral results. You will have to find a reliable source of 2024 state- or district-wide vote counts. If the 2024 election is close, this may not be easy to do between the election and the date this mini-project is due.\n\n\n\n\n\n\n\n\nExtra Credit Opportunity (Up to 8 points)\n\n\n\nFor extra credit, create an animated plot instead of a facet plot in Task 6.\nThis is hard, due to what might be a bug a in gganimate’s treatment of changing numbers of sf geometries. If you want to pursue this path, I recommend downloading the state shapefiles from TIGER (the Census page)9 and using it instead of the congressional district shape files. Unlike the congressional district files, the state boundaries should be unchanging over time.\n\n\n\nThis work ©2024 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini03.html#footnotes",
    "href": "archive/AY-2024-FALL/miniprojects/mini03.html#footnotes",
    "title": "STA 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Footnotes",
    "text": "Footnotes\n\nI am not aware of “official” reasons from any state on why they select “winner-take-all” allocation. States clearly compete for attention in presidential elections and it seems reasonable to assume that competitive states select “winner-take-all” allocation to attract presidential candidates who will make promises to that state’s voters. By contrast, states whose legislature is dominated by a single party, e.g., New York, may be motivated to award all their votes to the more popular party in that state, denying any ECVs to the other candidate, even if a sizeable minority votes for them. If you find a history of how states select their ECV allocation strategies, I would be interested in reading it.↩︎\nMaking predictions about a counter-factual past.↩︎\nHistorically, the “Republicans Red / Democrats Blue” convention was not particularly strong in American journalism. It become standardized during coverage of the 2000 Presidential Election and subsequent Florida recount battles and has not materially changed since. For purposes of this mini-project, we will apply “Republican Red / Democrat Blue” consistently.↩︎\nWhile it may be possible to automate the browser to automatically fill in this pop-up as part of the download process, that’s beyond the scope of this assignment.↩︎\nIt may be useful to recall that each two year cycle is called “a congress” for district mapping purposes. The 2022 US Election, selecting Representatives to serve 2023-2025, corresponds to the 118th Congress. The upcoming (November 2024) election will select members for the 119th Congress.↩︎\nThe other shapefiles in this FTP archive may be useful for your final projects.↩︎\nThe District of Columbia is very Democratic.↩︎\nThis latter effect is admittedly quite small if we assume political affiliation is unrelated to probability of voting. The relationship between voting likelihood and political leanings is an important one for campaign strategists and actively debated by academics.↩︎\nE.g., https://www2.census.gov/geo/tiger/TIGER2018/STATE/↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini00.html",
    "href": "archive/AY-2024-FALL/miniprojects/mini00.html",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "",
    "text": "In lieu of traditional homework, this course has a set of four mini-projects, which will be assessed in two stages. In the first, you will complete a small data analysis project1; after submission of your analysis, it will be assigned to a classmate, who will evaluate it according to an instructor-provided rubric. This peer feedback stage is an opportunity to see how your classmates answered questions and to compare it to your own response. In doing so, you will learn to evaluate data science work product and will develop a critical eye that can be turned to your own work.\nThis mini-project, however, is a meta-mini-project, designed to help you set up the course infrastructure you will use for the four graded mini-projects.\nNB: Mini-Project #00 is not graded, but it is required. For STA 9750, it serves as the legally mandated Verification of Enrollment activity. If it is not completed on time, you may be involuntarily disenrolled from the course.\nEstimated Time: 2 hours.\nThis course will use the industry-standard code sharing platform GitHub. Mini-projects and course-projects will be submitted by posting to a relevant GitHub project and creating a world-readable HTML page. A secondary goal of this course is to help students build a web-presence and a data science portfolio, giving you a place to showcase your skills to potential employers. The four mini-projects and the final course project should form an excellent basis for a portfolio. The main aim of Mini-Project #00 is to set up the “skeleton” of this portfolio.\nYou may choose to complete these tasks under a pseudonym if you do not want current or potential employers, classmates, or the world at large to see your work. You will be required to disclose your pseudonym to the instructor. If you choose to use a pseudonym, it will be straightforward to add your name to any or all coursework after the semester ends. Within the course, you will have the option to switch to a pseudonym as desired, but it is difficult to fully anonymize anything once it has been posted on the public internet. With all those cautions, please take a moment to reflect as to whether you wish to proceed under your own name or using a pseudonym."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini00.html#stage-1-github-account-creation",
    "href": "archive/AY-2024-FALL/miniprojects/mini00.html#stage-1-github-account-creation",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 1: GitHub Account Creation",
    "text": "Stage 1: GitHub Account Creation\nTo complete this course, you will need a free GitHub personal account, which you can create here. Please note that whatever account name you use will be public, so you need to define a pseudonym here if you choose to use one."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini00.html#stage-2-course-repo-creation",
    "href": "archive/AY-2024-FALL/miniprojects/mini00.html#stage-2-course-repo-creation",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 2: Course Repo Creation",
    "text": "Stage 2: Course Repo Creation\n\nCreating GitHub Repo\nNow that you have created a GitHub account, log in and proceed to your dashboard at https://github.com. In the top right corner, click the + symbol and select “New Repository.”2\n\nCreate a new repository named STA9750-2025-SPRING with a suitable description.\n\nThis repo needs to be public. You do not need to select a README, .gitignore, or a license at this time.\nAfter you create your repo, you should see a page like this:\n\nNote the URL highlighted in the main box:\n**https://github.com/&lt;USERNAME&gt;/STA9750-2025-SPRING.git**\nYou will need this in the next step.\n\n\nConnecting GitHub Repo to RStudio\nNow that you have set up an empty repo, you need to connect it to your personal machine and to RStudio. RStudio’s concept of projects roughly map to GitHub repos and that is what we will use here.\nOpen RStudio and click the project menu in the top right corner:\n\nFollow through the menu to click:\n\nNew Project\nVersion Control\nGit\n\nThis will take you to the following screen:\n\nCopy the .git URL from the previous step into Repository URL.3\n\n\nSecuring Connections to GitHub\nAn important aspect of source code management is access control to your code repository. While it’s typically no risk to make your code world-readable, you don’t want just anyone being able to add code to your repository. Traditionally, this type of access would be controlled with a username+password scheme, but GitHub has moved to a Access Token structure.\nThis process is a “one-time” task, documented in the Connect Section of the Happy Git with R book, but we’ll cover the highlights here.\n\nGit Credential Manager\nBefore starting, you will want to make sure you have installed the Git Credential Manager (GCM). If you used the Git for Windows bundle, you already have GCM installed. If you are on a Mac, you can download the GCM installer directly or install GCM via the GitHub Desktop Client.\n\n\nGitHub Personal Access Token\nNext, you need to create a Personal Access Token (PAT) for GitHub. You can think of this as a “special-purpose” password. Unlike your general account password, a PAT can be restricted to only perform certain activities or only for a limited time period.\nAfter logging in to GitHub via a web browser, visit https://github.com/settings/personal-access-tokens/new to begin the token creation process. Give the token a meaningful name and description and set an expiration date after (at least) the end of the semester.\nSet the “Repository Access” to “All repositories” and, under “Permissions &gt; Repository Permissions”, set “Contents” to “Read and Write”. This will now let anyone using your token read and write to all your repositories. After you create your token, you will be given only one opportunity to copy it. (Note that you can change permissions later, but you can only copy the token once.) Copy this and save it for later use. If you loose this token, you may need to generate a new one.\nWhen you make your first push to GitHub (as described below), use this token as your password. If everything is set up correctly, GCM will save this token and use it to authenticate you every time you push to GitHub. You should not need to paste this token every time.\n\n\n\nInitial Push\nNow, to make sure everything is working, let’s save a basic README file and push it to GitHub. This is a plain text file with no particular structure.\nTo create it, click the new file button in RStudio (top left; piece of paper with a green plus) and select Text File. RStudio will open this file in the editor: type some basic content, e.g.,\nSubmission materials for STA 9750 at Baruch College. \n\nOwner: &lt;YOURNAME&gt;\n(It doesn’t matter what you push: whatever you type will be the default text appearing when someone visits your repo.)\nSave the file and open the Git pane in RStudio.\n\nCheck the box next to the README file to stage it for git.\n\nThen click the Commit button a type a brief message (Initial commit is fine).\n\nFinally, push the Push button. If everything works, you should see a screen like the below:\n\nTo confirm everything worked, return to the GitHub repo in your browser. You should see the text of your README file displayed at the bottom of the page."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini00.html#stage-3-personal-website-creation",
    "href": "archive/AY-2024-FALL/miniprojects/mini00.html#stage-3-personal-website-creation",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 3: Personal Website Creation",
    "text": "Stage 3: Personal Website Creation\nNow that you created a place where you can push files to GitHub and have successfully pushed a basic README, it’s time to build a webpage using quarto.\nWe will need three pages to build a website:\n\nA configuration file, _quarto.yml, used to specify the look and layout of your website.\nAn index.qmd file used to create the homepage.\nA build script to create the website.\n\n\nConfiguration File\nOpen a new text file and save it as _quarto.yml. This is a configuration file used by quarto to control the layout of your site. For a barebones site, copy the following into _quarto.yml:\nproject:\n  type: website\n  output-dir: docs\n\nwebsite:\n  title: \"STA 9750 2024 Submission Material\"\n  description:\n    Course Projects for STA 9750 at Baruch College\n  site-url: \"https://&lt;GITHUB_NAME&gt;.github.io/STA9750-2025-SPRING/\"\n  navbar:\n    background: primary\n    search: false\n    \nformat:\n  html:\n    theme: &lt;THEME&gt;\n    toc: false\nNote that the indentation pattern is important so copy this exactly.\nReplace &lt;GITHUB_NAME&gt; with your GitHub user name.\nFor &lt;THEME&gt;, visit the Bootswatch theme gallery and pick your preferred theme. Replace &lt;THEME&gt; with a lower case version of the theme name; if you want to use the Sandstone theme used for this course website, &lt;THEME&gt; will be sandstone.\nOnce you have created this _quarto.yml, stage it (click the check mark) in RStudio’s git pane.\n\n\nindex.qmd\nNext, we’ll build your home page, conventionally called index.html. We will not write the HTML code by hand - it’s quite cumbersome - and will instead let quarto create it for us. Create another plain text file and save it as index.qmd.\nThis file will be divided into two parts, a header giving the metadata for the site, and a body, giving the content of the site.\nFirst write the header, separated by three horizontal bars (minus signs) above and below. For now, all you need to specify is a title:\n---\ntitle: \"YOUR TITLE GOES HERE\"\n---\nBelow the header, write the basic content of your website: a brief introduction of who you are.4 You can use markdown here for formatting. Basic text will suffice, but this is also a great opportunity to include things like a personal headshot, a link to a full resume, or similar.\nAs you work on this, click the “Render” button at the top of the editor pane to see what your site will look like.\nOnce you are happy with this landing page, stage it and we’ll move on to building the website properly.\n\n\nbuild_site.R\nFinally, open a new file - but now it’s an R script, not a text file, in RStudio. Copy the following into build_site.R:\n#!/usr/bin/env Rscript\nif(!require(\"quarto\")){\n    install.packages(\"quarto\")\n}\nlibrary(quarto)\nif(!quarto::quarto_binary_sitrep()){\n    stop(\"Something is wrong with your quarto installation.\")\n}\nquarto::quarto_render(\".\")\nsystem(\"git add docs/*\")\nif(!any(grepl(\"rstudio\", search()))){q(\"no\")}\nClick the Source button in the top-right corner of the editor pane to run this code. If everything works, it will build your website and automatically stage it. Stage build_site.R as well.\nFinally, Commit all these staged files and Push them to GitHub. You have now created a website and just need to turn on a web server so you can access it."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini00.html#stage-4-github-pages-deployment",
    "href": "archive/AY-2024-FALL/miniprojects/mini00.html#stage-4-github-pages-deployment",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 4: GitHub Pages Deployment",
    "text": "Stage 4: GitHub Pages Deployment\nReturn to the GitHub repo you created; recall that the URL is something like:\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2025-SPRING/\nOpen the “Settings” menu and proceed to the “Pages” submenu. You should see a page that looks like this:\n\nUnder Build and Deployment, set the main branch to deploy and select the docs directory on that branch. Hit save and your website will go live!\nTo check your website is working, proceed to\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2025-SPRING\nIf everything works, you will see your site! (If you used the Render feature in RStudio, it should look familiar.)\nIf you get stuck, use the course discussion board to seek help from your classmates and, if necessary, the instructor."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini00.html#stage-5-submission",
    "href": "archive/AY-2024-FALL/miniprojects/mini00.html#stage-5-submission",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 5: Submission",
    "text": "Stage 5: Submission\nOnce your site is live, you will submit it to the instructor in two ways:\n\nLog into the course discussion board (Piazza) and send me your GitHub name so I can link it to my gradebook.\nTag @michaelweylandt on GitHub to make sure I can access your repo.\n\nThese both must be completed to complete the assignment and verify enrollment.\n\nDiscussion Board (Piazza)\nFirst, send me a private message through the course discussion board with the following details:\n\nReal Name\nCUNY EmplID (8 digit ID code)\nCUNY email\nGitHub user name\nWhich course section you are enrolled in: STA 9750 or OPR 9750\n\nThis is the only place where you are required to connect your GitHub ID with your real name and CUNY credentials. I need this information to connect your public activity with my (private) gradebook and the CUNY system.\nIf all your information looks good, I might not reply through the discussion board. When I reply through GitHub, I’m acknowledging both parts of your submission.\n\n\nInstructor Tagging\nFinally, you’re going to contact me through GitHub: go to\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2025-SPRING/issues/new\nto open a new issue. Title the issue STA 9750 &lt;GITHUB_USERNAME&gt; MiniProject #00 and fill in the following text for the issue:\nHi @michaelweylandt!\n\nI've created my STA 9750 website - check it out!\n\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2025-SPRING/\n(Replace &lt;GITHUB_USERNAME&gt; with your username throughout.)\n\nThis will send me a notification through GitHub and I will confirm that I can access your repository and website. If you don’t do this, I may not be able to access your graded assignments when you submit them! I will confirm that I have your real ID verified as well."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini00.html#wrap-up",
    "href": "archive/AY-2024-FALL/miniprojects/mini00.html#wrap-up",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nOnce I acknowledge receipt of your ID and website, you’re done with Mini-Project #00! You’ve built a website and are ready for the course to begin in earnest.\nMini-Projects #01-#04 will be submitted as separate pages in your website (different quarto documents) and hosted via GitHub pages for peer feedback. We will discuss that process in more detail after Mini-Project #00 is complete."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini00.html#hints",
    "href": "archive/AY-2024-FALL/miniprojects/mini00.html#hints",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Hints",
    "text": "Hints\nIf you need help, the course discussion board should be your first stop.\nIf you want to personalize your website further, you can see how I have created mine on GitHub. Recall that the Markdown syntax used by quarto is summarized at https://www.markdownguide.org/basic-syntax/.\nYou may want to use the About Page functionality to improve the look of your home page."
  },
  {
    "objectID": "archive/AY-2024-FALL/miniprojects/mini00.html#footnotes",
    "href": "archive/AY-2024-FALL/miniprojects/mini00.html#footnotes",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEarly in the course, I will ‘scaffold’ most of the analysis, leaving only some small steps for you to fill in. As the course progresses, the mini-projects will be more self-directed.↩︎\nAlternatively, simply go to https://github.com/new after logging in.↩︎\nYou can leave the next two boxes blank or set a custom directory name and location. RStudio’s defaults are reasonable; the default directory name will simply be STA9750-2025-SPRING and it will be located in your home directory.↩︎\nIf you choose to complete the course using a pseudonym, make up something fun. If you are using your real name, this is a great place to state that you are a Baruch student, your expected graduation date, your field of employment (current or desired), and one or two personal facts. This, along with a LinkedIn page, will quickly become one of the first things that comes up when a potential employer searches your name, so make a good impression!↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa08.html",
    "href": "archive/AY-2024-FALL/preassigns/pa08.html",
    "title": "STA 9750 Week 8 Pre Assignment: More Plots",
    "section": "",
    "text": "This week, we will dive deeper into the world of ggplot2, with a focus on spatial data visualization and interactive graphics. Before we do so, let’s pause and consolidate everything we’ve done to date.\n\nIf you did not finish last week’s in-class lab, do so now.\nWatch Prof. Di Cook’s lecture “Myth busting and\napophenia in data visualisation: is what you see really there?”. As we discussed in class, plots are an excellent way to explore data, but we always want to be careful that what we think find truly exists. Prof. Cook discusses relationships between effective statistical visualization and effective statistical practice.\n\nAfter finishing Prof. Cook’s lecture, explore the R Graphics Gallery “Best Charts” collection. Pick one chart from this collection and evaluate it with a critical eye:\n\nIs it well styled?\nWhat story is it trying to tell?\nDoes it tell that story effectively?\nDo you believe that story?\nHow could it tell the story more effectively?\n\n\nAfter finishing these, complete the Weekly Pre-Assignment Quiz on Brightspace."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa02.html",
    "href": "archive/AY-2024-FALL/preassigns/pa02.html",
    "title": "STA 9750 Week 2 Pre Assignment: Getting Started with Markdown",
    "section": "",
    "text": "Due Date: 2024-09-04 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week, we are going to learn to use quarto, a data science publishing platform. quarto documents are written using Markdown, a light-weight mark-up language.12\nFor this week’s pre-assignment, complete this interactive Markdown tutorial, which should take you about 10 minutes. Once you’ve familiarized yourself with Markdown, take a look at the source code for this website and see how certain Markdown documents are rendered as web pages.\nDuring this week’s lab session, we will take particular advantage of:\nso make sure to pay attention to those parts of the tutorial.\nAfter you are done with the introduction to Markdown, log in to CUNY Brightspace and complete the Pre-Assignment 02 “Getting to Know You” quiz. As part of this quiz, you will be asked to attest that you successfully completed the Markdown tutorial."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa02.html#footnotes",
    "href": "archive/AY-2024-FALL/preassigns/pa02.html#footnotes",
    "title": "STA 9750 Week 2 Pre Assignment: Getting Started with Markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGet it? If you learn nothing else in this class, you will certainly learn that programmers love terrible puns. R itself is actually a pun as it was originally a free ‘knock-off’ of the S programming language developed by Ross and Rob.↩︎\nA mark-up language is a way of specifying the formatting applied to given text. It exists somewhere between “plain text” and a full document format like a .docx file. Other markup languages include HTML (hyper text markup language), rST (reStructured Text), LaTeX (used for scientific typesetting), and many others. Markdown is the simplest of these and the only one you will be required to write in this course. You will need to learn a bit of how HTML is structured and, if you are including math in your mini-project or final project submissions, a bit of LaTeX will go a long way.↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa11.html",
    "href": "archive/AY-2024-FALL/preassigns/pa11.html",
    "title": "STA 9750 Week 11 Pre Assignment: Intro to HTML",
    "section": "",
    "text": "Our next topic is scraping data from web pages, that is data stored in HTML. Recall our “hierarchy” of data storage preferences:\nAs the course has progressed, we have seen or worked with data in many of these formats. This week, we move to the most difficult data source of this course: HTML.1\nHTML, short for HyperText Markup Language, is the lingua franca of the web. The vast majority of websites you visit or interact with are written using HTML. As such, HTML is ubiquitous in modern life. HTML is flexible, relatively easy to write by hand or programmatically, compressible, and has near universal built-in support on modern operating systems.\nUnfortunately, this ubiquity comes with a cost: because HTML is so universal, web browsers have been designed to “try their best” to read and render even improper HTML.2 In response, web professionals and amateurs have released ever worse HTML upon the world, continuing a vicious cycle. Modern programming practice has moved away from hand-written HTML in favor of tools like Markdown and Quarto, which allow “correct” HTML to be generated automatically. While this is a welcome trend, you will almost certainly still encounter malformatted HTML in your career, as nothing - however flawed - truly leaves the internet.\nBut, for now, we begin with relatively well-formatted HTML.\nRight-click in your browser and view the source code of this page - what you’re seeing is HTML. HTML consists of a hierarchically nested set of elements that look something like this:\nThere are three key pieces of this structure:\nThe power and flexibility of HTML comes from the fact that the contents of one element can include one or more additional elements. For instance, you might encounter an element like\nwhich might render as:\nHere the a tag is used to create hyperlinks, with the target specified by the href attribute. There are many more “standard” HTML tags in addition to those websites might define for their own use. For comprehensive documentation, see the Mozilla Developer Network (MDN) Documentation.\nWhen extracting data from a website, we will typically want to select all the elements of a certain tag or with a certain attribute: e.g., all cells of a table or all bolded paragraph headers. We can do so efficiently using “CSS Selectors”.\nCSS Selectors are a special language used to select multiple elements at once: the basic elements are as follows:\nWe will use these to tell R what elements to import from a web page. A well constructed selector statement can usually highlight exactly the data we hope to extract.\nFor now, however, you will practice using CSS Selectors from within your browser.\nRight click the the following link and add its to your bookmarks. Whenever you’re on a website, you can click that bookmark to open the CSS SelectorGadget.3\nUpon clicking, you will see a toolbar at the bottom of the page. If you type a CSS selector statement into that toolbar, it will highlight all elements on the page that match that selector. For now try a simple a and hit enter: you should see all links on the page highlighted. You can also try more advanced CSS selectors: li a will select all links (a) within list items (li) of the navigation bar at the top of the page.\nYou can also use SelectorGadget to create CSS Selectors. If you click several items that you want to select, SelectorGadget will attempt to create a suitable selector command. (You might need to Clear the input area before trying this.) For instance, try clicking a link in this text and seeing what SelectorGadget automatically selects for you. In this case, SelectorGadget comes up with a for all links on the page. If we want to exclude the links in the navigation bar, we can click them again, marking them in red and SelectorGadget will attempt to exclude them. Here, it creates a CSS selector that selects only links within the main body of the page. For our purposes, two clicks are enough, but you could extend this further. SelectorGadget isn’t perfect, but it’s often a very good starting guess.\nOpen the rvest Star Wars example page in a new tab and use SelectorGadget to select the 7 movie names in the main section. We want only the movie names and not the text below them. We also don’t want the clickable links in the sidebar. We will use this selector as our first example in class.\nNext, open the Wikipedia page listing all CUNY Colleges and confirm that the tbody selector selects the entirety of the main table. Note that if you use SelectorGadget here, you might get something like .jquery-tablesorter. For reasons we will discuss in class, this won’t work in R.\nFinally, open the Baruch College Wikipage and create a selector for just the GPS coordinates in the top right corner of the page. You should try to select just the coordinates themselves and not the text “Coordinates” preceding them.\nAs you explore this, it’s worth noting that Wikipedia is actually a rather complicated web-page. If you want to practice on simpler websites, I recommend starting here.\nAfter finishing this document, complete the Weekly Pre-Assignment Quiz on Brightspace."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa11.html#footnotes",
    "href": "archive/AY-2024-FALL/preassigns/pa11.html#footnotes",
    "title": "STA 9750 Week 11 Pre Assignment: Intro to HTML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe won’t cover reading data from PDFs in this course.↩︎\nIf you are of a certain age, you will remember an era when websites would work in one browser and not others. Proper HTML should work in all browsers, but each browser had its own way of handling malformatted HTML. Developers were, in essence, requiring users to use a piece of software that would automatically correct their mistakes. These were dark times…↩︎\nAll credit to Andrew Cantino at https://selectorgadget.com/. Use here inspired by the [rvest documentation]↩︎"
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa12.html",
    "href": "archive/AY-2024-FALL/preassigns/pa12.html",
    "title": "STA 9750 Week 12 Pre-Assignment: Strings and Things",
    "section": "",
    "text": "This week, we begin to study the world of text data. While numerical data is reasonably straight-forward to deal with, text data is remarkably complex. A full discussion of text data requires understanding the vast world of human written language, but we will discuss enough of the major points to hopefully solve 95% of the challenges you will face in your career."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa12.html#goals",
    "href": "archive/AY-2024-FALL/preassigns/pa12.html#goals",
    "title": "STA 9750 Week 12 Pre-Assignment: Strings and Things",
    "section": "Goals",
    "text": "Goals\nIn our quest to understand text data, we have two major goals:\n\nUnderstanding String Encodings and Unicode\nManipulating Strings with Regular Expressions\n\nBefore we get into these, let’s begin with a basic review of the character data type in R."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa12.html#string-vectors",
    "href": "archive/AY-2024-FALL/preassigns/pa12.html#string-vectors",
    "title": "STA 9750 Week 12 Pre-Assignment: Strings and Things",
    "section": "String Vectors",
    "text": "String Vectors\nRecall that R works by default on vectors - ordered collections of the “same sort” of thing. R supports the following vector types:\n\nRaw for pure access to bytes without any additional meaning: rarely useful for pure data-analytic work, but commonly used to interact with binary file formats and with non-R software\nInteger: 32-bit signed integers, ranging from \\(-2^{30}\\) to \\(2^{30}-1\\). (If you have done low-level work before, you might ask where the extra bit went: it’s used for encoding NA values.)\nNumeric: 64-bit (double precision) floating point values, ranging from (approximately) \\(\\pm 10^{308}\\). The detailed behavior of numeric (often called double) data is beyond this course, but it is well documented elsewhwere.\nCharacter: the topic of today’s discussion.\n\nR makes no difference between a character - in the sense of a single letter - and a string: in particular, each element of a character vector is an (arbitrary length) string. Specialized functions are required for work at the true “single letter” scale. If you come from other languages, this behavior might be surprising, but it allows R to handle much of the complexity associated with characters automagically, which greatly simplifies data analysis.\nWhen speaking, we refer to R as using strings, even if R itself calls them character elements for historical reasons."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa12.html#encoding",
    "href": "archive/AY-2024-FALL/preassigns/pa12.html#encoding",
    "title": "STA 9750 Week 12 Pre-Assignment: Strings and Things",
    "section": "Encoding",
    "text": "Encoding\nHow are strings represented on a computer? The answer has evolved over time, but the current state of the art - used by almost all non-legacy software - is based on the Unicode system and the UTF-8 encoding.\nThe Unicode system is comprised of two essential parts: - A numbered list of “letter-like” elements - Rules for manipulating those elements\nWhile this seems simple, it is anything but. The history of string representations in computers is a long and painful story of programmers repeatedly underestimating the complexity of the seemingly simple task of listing “all the letters.”\nThe Unicode consortium makes a long list of characters that computers should be able to represent: the most recent version of the Unicode standard includes 149,813 characters divided into 161 scripts. These include everything from the basic (Anglo-American) Latin alphabet to the Greek and Cyrillic alphabets to Chinese and Japanese characters to the undeciphered Linear A alphabet and Tengwar, the fictional script used in the Lord of the Rings novels. The Unicode standard also includes a wide set of Emoji (approximately 4000) and many “modifying” characters.\nTo each of these, the Unicode consortium assigns a code point : a numerical identifier. Even superficially similar characters may be assigned different code points to distinguish them: for example, “H” is code point U+0048 with the official description “Latin Capital Letter H” while “Η” is U+0397, “Greek Capital Letter Eta.”\nThe difference between these characters is essential to know how to manipulate them:\n\n\n\n\n\n\n\n\nUse the tolower function to lower-case each of these:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Unicode standard defines the lower case mapping of U+0048 as the Latin lower case h, while the lower case mapping of U+0397 as the Greek lower case eta, which looks something like a streched n. \nIn general, these mappings are incredibly complicated and depend not only on the specific code point, but also the set of translation rules being used. (Certain languages have different lower/upper mappings for what are otherwise the same letter.)\nWhile you don’t need to know all of this complexity, it is essential to know that it’s out there and to rely on battle-tested libraries to perform these mappings.\nUnicode is supplemented by the UTF-8 encodings, which controls how 0/1-bit strings are actually translated to code points. (Fonts then map code points to what you see on the screen.) UTF-8 is more-or-less back-compatible with other major encodings, so it’s a good default. When dealing with modern websites or public data sources, they almost always present their contents in a UTF-8 compatible encoding (if not UTF-8 proper) so you should be ok.\nA well-formatted website will state its encoding near the top of the page:\n\nlibrary(rvest)\nread_html(\"http://baruch.cuny.edu\") |&gt;\n    html_elements(\"meta[charset]\") |&gt;\n    html_attr(\"charset\")\n\n[1] \"UTF-8\"\n\n\nAdvice: Whenever possible, make sure you are using UTF-8 strings: if your data is not UTF-8, reencode it to UTF-8 as soon as possible. This will save you much pain."
  },
  {
    "objectID": "archive/AY-2024-FALL/preassigns/pa12.html#string-manipulation",
    "href": "archive/AY-2024-FALL/preassigns/pa12.html#string-manipulation",
    "title": "STA 9750 Week 12 Pre-Assignment: Strings and Things",
    "section": "String Manipulation",
    "text": "String Manipulation\nOnce data is in R and encoded as UTF-8 Unicode points, we have several tools for dealing with strings. Your first port of call should be the stringr package.\nAll the functions of the stringr package start with str_ and take a vector of strings as the first argument, making them well suited for chained analysis.\nLet’s start with str_length which simply computes the length of each element. For the basic Latin alphabet, this more or less matches our intuition:\n\nlibrary(stringr)\nx &lt;- c(\"I\", \"am\", \"a\", \"student\", \"at\", \"Baruch.\")\nstr_length(x)\n\n[1] 1 2 1 7 2 7\n\n\nbut it can be tricky for strings that involve Unicode combining characters.\n\nstr_length(\"X̅\")\n\n[1] 2\n\n\nHere the “overbar” is a combining character which we add on to the X. This is commonly (though not always) used for languages with accents (e.g. French) or for languages where vowels are written above and below the main script (Arabic or Hebrew). This same idea is used for certain Emoji constructs:\n\nstr_length(\"👨🏿\")\n\n[1] 2\n\n\nHere, “Man with Dark Skin Tone” is the combination of “Man” and “Dark Skin Tone.” (Compare how this appears in the rendered document to how RStudio prints it.)\nWhile there is complexity in all of Unicode, str_length will behave as you might expect for “regular” text. I’m going to stop showing the “scary case” of Unicode, but you should be aware of it for the remainder of these exercises.\n\nConcatenation\nYou have already seen the base paste and paste0 functions for combining two string vectors together.\n\nx &lt;- c(\"Michael\", \"Mary\", \"Gus\")\ny &lt;- c(\"Son\", \"Daughter\", \"Dog\")\n\npaste(x, y)\n\n[1] \"Michael Son\"   \"Mary Daughter\" \"Gus Dog\"      \n\n\nBy default, paste combines strings with a space between them, while paste0 omits the space. paste is typically what you want for strings for human reading, while paste0 is a better guess for computer-oriented text (e.g., putting together a URL).\nYou can change the separator by passing a sep argument to paste:\n\npaste(x, y, sep = \" is my \")\n\n[1] \"Michael is my Son\"   \"Mary is my Daughter\" \"Gus is my Dog\"      \n\n\nYou can also combine together multiple elements of a vector using the collapse argument:\n\npaste(x, collapse = \" and \")\n\n[1] \"Michael and Mary and Gus\"\n\n\n\nExercises:\nUsing the paste function, make a vector of strings like “John’s favorite color is blue”:\n\npeople &lt;- c(\"John\", \"Jane\", \"Randy\", \"Tammi\")\ncolors &lt;- c(\"blue\", \"orange\", \"grey\", \"chartreuse\")\n\nModify your answer to write a (run-on) sentence of favorite colors: “John’s favorite color is blue and Jane’s favorite color is orange and …”\n\npeople &lt;- c(\"John\", \"Jane\", \"Randy\", \"Tammi\")\ncolors &lt;- c(\"blue\", \"orange\", \"grey\", \"chartreuse\")\n\n\n\n\nSubstring Selection\nWhen cleaning up data for analysis, it is common to need to take substrings from larger text. The str_sub function will do this:\n\nx &lt;- c(\"How\", \"much\", \"is\", \"that\", \"puppy\", \"in\", \"the\", \"window?\")\nstr_sub(x, 1, 2)\n\n[1] \"Ho\" \"mu\" \"is\" \"th\" \"pu\" \"in\" \"th\" \"wi\"\n\n\nIf you want to go all the way to the end, set the end element to -1:\n\nstr_sub(x, 2, -1)\n\n[1] \"ow\"     \"uch\"    \"s\"      \"hat\"    \"uppy\"   \"n\"      \"he\"     \"indow?\"\n\n\n\nExercises\nUsing str_sub, remove the system name (CUNY or UC) and return only the campus name:\n\nlibrary(stringr)\nuc_schools &lt;- c(\"UC Berkeley\", \"UC San Diego\", \"UC Santa Cruz\", \"UC Davis\")\ncuny_schools &lt;- c(\"Baruch College, CUNY\", \"City College, CUNY\", \"La Guardia Community College, CUNY\")\nstr_sub(uc_schools)\n\n[1] \"UC Berkeley\"   \"UC San Diego\"  \"UC Santa Cruz\" \"UC Davis\"     \n\n\n\n\n\nDetect and Matching\nOften we only need to know whether a particular substring is present in a larger string. We can use str_detect to do this:\n\nlibrary(stringr)\ndogs &lt;- c(\"basset hound\", \"greyhound\", \"labrador retreiver\", \"border collie\", \"Afgahn hound\")\nstr_detect(dogs, \"hound\")\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE\n\n\nThe str_match function will return the text of the match. Here it’s useless, but we’ll see that it becomes more powerful when we allow more flexible pattern specifications.\n\nlibrary(stringr)\ndogs &lt;- c(\"basset hound\", \"greyhound\", \"labrador retreiver\", \"border collie\", \"Afgahn hound\")\nstr_match(dogs, \"hound\")\n\n     [,1]   \n[1,] \"hound\"\n[2,] \"hound\"\n[3,] NA     \n[4,] NA     \n[5,] \"hound\"\n\n\n\nExercises\nUse str_detect to find the CUNY schools:\n\nlibrary(stringr)\nschools &lt;- c(\"UC Davis\", \n             \"UC Santa Cruz\", \n             \"City College, CUNY\", \n             \"UC Berkeley\", \n             \"La Guardia Community College, CUNY\", \n             \"Baruch College, CUNY\", \n             \"UC San Diego\")\nstr_sub(uc_schools)\n\n[1] \"UC Berkeley\"   \"UC San Diego\"  \"UC Santa Cruz\" \"UC Davis\"     \n\n\n\n\n\nSpecifying Patterns\nWhile working by individual characters is sometimes useful (for very regular data), we generally need more powerful tools: regular expressions (RE) provide a compact language for specifying patterns in strings. We’ll introduce the basics here to help with string functions and then explore some more advanced RE features.\nThe most basic pattern is a set of elements in brackets: this means “any of these”.\nFor example, we want to see which names have an “A” in them:\n\nnames &lt;- c(\"Jane\", \"Rhonda\", \"Reggie\", \"Bernie\", \"Walter\", \"Arthur\")\nstr_detect(names, \"a\") ## Wrong!\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE FALSE\n\nstr_detect(names, \"A\") ## Wrong!\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE\n\nstr_detect(names, \"[Aa]\") ## Right!\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n\n\nAlternatively, we can see which strings contain numbers:\n\nx &lt;- c(\"73 cows\", \"47 chickens\", \"a dozen eggs\")\nstr_detect(x, \"[0123456789]\")\n\n[1]  TRUE  TRUE FALSE\n\n\nIf we use str_match we can pull out the matching element:\n\nx &lt;- c(\"2 burgers\", \"3 soups\", \"5 fish\")\nstr_match(x, \"[0123456789]\")\n\n     [,1]\n[1,] \"2\" \n[2,] \"3\" \n[3,] \"5\" \n\n\nBy default, this only finds one appearance of the pattern:\n\nx &lt;- c(\"23 burgers\", \"34 soups\", \"56 fish\")\n\n# Why is this wrong?\nstr_match(x, \"[0123456789]\")\n\n     [,1]\n[1,] \"2\" \n[2,] \"3\" \n[3,] \"5\" \n\n\nWe can modify the pattern specifier to include count information. The basic behavior is to add explicit count bounds:\n\nx &lt;- c(\"2 burgers\", \"34 soups\", \"567 fish\")\nstr_match(x, \"[0123456789]{2}\")\n\n     [,1]\n[1,] NA  \n[2,] \"34\"\n[3,] \"56\"\n\nstr_match(x, \"[0123456789]{3}\")\n\n     [,1] \n[1,] NA   \n[2,] NA   \n[3,] \"567\"\n\nstr_match(x, \"[0123456789]{2,3}\")\n\n     [,1] \n[1,] NA   \n[2,] \"34\" \n[3,] \"567\"\n\nstr_match(x, \"[0123456789]{2,}\")\n\n     [,1] \n[1,] NA   \n[2,] \"34\" \n[3,] \"567\"\n\n\nHere a single number is an exact count ({2}), while pairs ({2,3}) specify a range. If one end of the range is left empty, it is 0 or infinite (depending on the direction).\nCertain count specifications are sufficiently useful to get their own syntax:\n\nOne or more: + is equivalent to {1,}\nZero or more: * is equivalent to {0,}\nOne or zero: ? is equivalent to {0,1}.\n\nUse these specifications for the next set of exercises.\n\nExercises\nWhich strings contain a three digit number?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstr_detect(x, \"\\\\d{3}\")\nstr_detect(x, \"\\\\d{3}\")\n\n\n\n\n\n\n\n\n\nCombining patterns\nYou can combine REs to make more complex patterns:\n\n(a|b) means a or b. This is like [] notation but a, b can be more complex than single characters\n\n\nx &lt;- c(\"Baruch College, CUNY\", \"UC Berkeley\", \"Harvard University\")\nstr_detect(x, \"(CUNY|UC)\")\n\n[1]  TRUE  TRUE FALSE\n\n\n\n[^abc] means anything other than a, b, c. You can often achieve a similar effect using the negate argument to str_detect, but you need this specifically for str_match\n\n\nx &lt;- c(\"10 blue fish\", \"three wet goats\", \"15 otters in hats\")\nstr_detect(x, \"[^0123456789]\")\n\n[1] TRUE TRUE TRUE\n\n\n\n^ outside of a bracket denotes the start of a line:\n\n\nx &lt;- c(\"rum\", \"white rum\", \"flavored rum\")\nstr_detect(x, \"^rum\")\n\n[1]  TRUE FALSE FALSE\n\n\n\n$ denotes the end of a line:\n\n\nx &lt;- c(\"bourbon whiskey\", \"scotch whisky\", \"whiskey liqueurs\")\nstr_detect(x, \"whisk[e]?y$\")\n\n[1]  TRUE  TRUE FALSE\n\n\nSee the stringr RE docs for more examples of regular expressions.\n\nExercises\n\nUse a regular expression to find which of these are fish species:\n\n\n\n\n\n\n\n\n\n\nUse a regular expression to find words with three or more vowels in a row:\n\n\n\n\n\n\n\n\n\n\nFind the words where “q” is not followed by a “u”\n\n\n\n\n\n\n\n\n\n\n\n\nReplacement\nThe str_replace function allows us to replace a string with something else. This is particularly useful when cleaning up text:\n\nx &lt;- c(\"Manhattan, NY\", \"Bronx, New York\", \"Brooklyn, ny\", \"Queens, nY\")\nstr_replace(x, \"([nN][yY]|New York)\", \"NY\")\n\n[1] \"Manhattan, NY\" \"Bronx, NY\"     \"Brooklyn, NY\"  \"Queens, NY\""
  },
  {
    "objectID": "archive/AY-2024-FALL/labs.html",
    "href": "archive/AY-2024-FALL/labs.html",
    "title": "STA 9750 - In-Class Labs",
    "section": "",
    "text": "Most weeks, the Thursday ‘lecture’ for STA 9750 will be dedicated to an in-class “lab”. These ungraded labs are an opportunity to see how the concepts introduced in that week’s pre-assignment are used in practice.\nLabs:\n\nLab #01: Rev your Engines! Setting-Up R and RStudio\nLab #02: Getting Down with Markdown\nLab #03: R, These are your first steps…\nLab #04: Single Table Verbs, Group-Aware Filtering\nLab #05: Let us Join our Tables Together\nLab #07: More Thoughts on Plots\nLab #08: Advanced Plotting - Chloropleths and Cartograms\nLab #09: Data Import in R\nLab #11: Manipulating and Reading HTML in R\nLab #12: Strings and Stringy-Things\nLab #13: Inference and Prediction in R"
  },
  {
    "objectID": "archive/AY-2024-FALL/index.html",
    "href": "archive/AY-2024-FALL/index.html",
    "title": "STA 9750 - Basic Software Tools for Data Analysis",
    "section": "",
    "text": "Welcome to the course website for STA 9750 (Spring 2025)!\nSTA 9750 is an Introduction to R targeted at students in the MS in Business Analytics, MS in Statistics, and MS in Quantitative Methods programs. Though listed as a double course, STA 9750 and OPR 9750 will be taught and graded jointly: students are encouraged to collaborate with classmates in either section.\nThis site hosts the unofficial Course Syllabus, Course Policies, and Course Learning Objectives. Official copies of these documents can be found on CUNY Brightspace. Course pre-assignments, labs, and mini-projects can also be found on this site.\nThis year, STA 9750 will be taught in a mixture of the flipped-classroom and experiential-learning formats. Roughly, this means that most weeks, students will be asked to complete a small pre-assignment each week to introduce the core concept(s) covered in that week’s lecture. Each class period will be split between a brief lecture covering concepts in more detail and an extended lab activity designed to build familiarity and fluency with that week’s subject matter.\nThere are quite a few moving parts to this course, so this key dates file or the list of upcoming course activities below may be useful:\n\n\n\n\n\n\n\nA CSV file suitable for import into Google Calendar with all assignment deadlines can be found here.\nInstructor: Michael Weylandt"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#mini-project-02",
    "href": "archive/AY-2024-FALL/slides/slides09.html#mini-project-02",
    "title": "STA 9750 - Week 9",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nThank you for peer feedback! I had a lot of fun reading these."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#mini-project-03",
    "href": "archive/AY-2024-FALL/slides/slides09.html#mini-project-03",
    "title": "STA 9750 - Week 9",
    "section": "STA 9750 Mini-Project #03",
    "text": "STA 9750 Mini-Project #03\nNow online\nDue November 13th\n\nGitHub post (used for peer feedback) AND Brightspace\nThree Weeks: don’t wait until the very end\nShould be less demanding than MP #01 and MP#02\n\nLots of little files. No big files!\nMaps and election retrodiction\n\n\n\nPay attention to the rubric"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#course-project",
    "href": "archive/AY-2024-FALL/slides/slides09.html#course-project",
    "title": "STA 9750 - Week 9",
    "section": "STA 9750 Course Project",
    "text": "STA 9750 Course Project\nProposal feedback sent by email this afternoon. Please contact if not received.\nNext Week: Mid-Term Check-In Presentations\n\n6 minutes\nLocking in on specific questions\nEngagement with existing literature"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#upcoming-mini-projects",
    "href": "archive/AY-2024-FALL/slides/slides09.html#upcoming-mini-projects",
    "title": "STA 9750 - Week 9",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#04: Something financial\n\nAny requests?"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#pre-assignments",
    "href": "archive/AY-2024-FALL/slides/slides09.html#pre-assignments",
    "title": "STA 9750 - Week 9",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is November 13th\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#course-support",
    "href": "archive/AY-2024-FALL/slides/slides09.html#course-support",
    "title": "STA 9750 - Week 9",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(38\\) minute average response time)\n\n\nChange: MW Thursday Zoom OH will be 4:00pm to 5:00pm"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#upcoming-week",
    "href": "archive/AY-2024-FALL/slides/slides09.html#upcoming-week",
    "title": "STA 9750 - Week 9",
    "section": "Upcoming Week",
    "text": "Upcoming Week\n\nMid-Project Check-Ins"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#faq-file-import",
    "href": "archive/AY-2024-FALL/slides/slides09.html#faq-file-import",
    "title": "STA 9750 - Week 9",
    "section": "FAQ: File Import",
    "text": "FAQ: File Import\nNo FAQs today. (New topic - data import)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#warm-up",
    "href": "archive/AY-2024-FALL/slides/slides09.html#warm-up",
    "title": "STA 9750 - Week 9",
    "section": "Warm-Up",
    "text": "Warm-Up\nFrom FiveThirtyEight"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#breakout-rooms",
    "href": "archive/AY-2024-FALL/slides/slides09.html#breakout-rooms",
    "title": "STA 9750 - Week 9",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nRoom\nTeam\n\nRoom\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#warm-up-1",
    "href": "archive/AY-2024-FALL/slides/slides09.html#warm-up-1",
    "title": "STA 9750 - Week 9",
    "section": "Warm-Up",
    "text": "Warm-Up\nData can be found at https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/candy-power-ranking/candy-data.csv\nRead into R (readr::read_csv) and make 3 plots:\n\nDo people like more sugary candy?\nDo people like more expensive candy?\nOpen-Ended"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#getting-data-into-r",
    "href": "archive/AY-2024-FALL/slides/slides09.html#getting-data-into-r",
    "title": "STA 9750 - Week 9",
    "section": "Getting Data into R",
    "text": "Getting Data into R\nTwo topics:\n\nHow internet data transfer actually works\nHow to handle non-rectangular data formats\n\n🎃End a bit early for Halloween🎃"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#urls",
    "href": "archive/AY-2024-FALL/slides/slides09.html#urls",
    "title": "STA 9750 - Week 9",
    "section": "URLs",
    "text": "URLs\nFrom abstrax.io"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#json",
    "href": "archive/AY-2024-FALL/slides/slides09.html#json",
    "title": "STA 9750 - Week 9",
    "section": "JSON",
    "text": "JSON\nJSON:\n\nShort for JavaScript Object Notation\nPopular plain-text representation for hierarchical data.\nCloser to Python objects (dicts of dicts of dicts) than R data.frames\nWidely used for Application Programming Interfaces (APIs)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#json-1",
    "href": "archive/AY-2024-FALL/slides/slides09.html#json-1",
    "title": "STA 9750 - Week 9",
    "section": "JSON",
    "text": "JSON\nExample:\n{\n    \"data\": {\n        \"id\": 27992,\n        \"title\": \"A Sunday on La Grande Jatte — 1884\",\n        \"image_id\": \"1adf2696-8489-499b-cad2-821d7fde4b33\"\n    },\n    \"config\": {\n        \"iiif_url\": \"https://www.artic.edu/iiif/2\",\n    }\n}"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#json-2",
    "href": "archive/AY-2024-FALL/slides/slides09.html#json-2",
    "title": "STA 9750 - Week 9",
    "section": "JSON",
    "text": "JSON\nRead JSON in R with jsonlite package (alternatives exist)\n\nlibrary(jsonlite)\n# A JSON array of primitives\njson &lt;- '[\"Mario\", \"Peach\", null, \"Bowser\"]'\n\n# Simplifies into an atomic vector\nfromJSON(json)\n\n[1] \"Mario\"  \"Peach\"  NA       \"Bowser\""
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#json-3",
    "href": "archive/AY-2024-FALL/slides/slides09.html#json-3",
    "title": "STA 9750 - Week 9",
    "section": "JSON",
    "text": "JSON\n\njson &lt;-\n'[\n  {\"Name\" : \"Mario\", \"Age\" : 32, \"Occupation\" : \"Plumber\"}, \n  {\"Name\" : \"Peach\", \"Age\" : 21, \"Occupation\" : \"Princess\"},\n  {},\n  {\"Name\" : \"Bowser\", \"Occupation\" : \"Koopa\"}\n]'\nmydf &lt;- fromJSON(json)\nmydf\n\n    Name Age Occupation\n1  Mario  32    Plumber\n2  Peach  21   Princess\n3   &lt;NA&gt;  NA       &lt;NA&gt;\n4 Bowser  NA      Koopa"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#json-4",
    "href": "archive/AY-2024-FALL/slides/slides09.html#json-4",
    "title": "STA 9750 - Week 9",
    "section": "JSON",
    "text": "JSON\n\nfromJSON(\"http://worldtimeapi.org/api/timezone/America/New_York\")[1:5]\n\n$utc_offset\n[1] \"-05:00\"\n\n$timezone\n[1] \"America/New_York\"\n\n$day_of_week\n[1] 3\n\n$day_of_year\n[1] 1\n\n$datetime\n[1] \"2025-01-01T04:13:36.302497-05:00\"\n\n\nCompare to browser access"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#data-transfer-download.file",
    "href": "archive/AY-2024-FALL/slides/slides09.html#data-transfer-download.file",
    "title": "STA 9750 - Week 9",
    "section": "Data Transfer: download.file",
    "text": "Data Transfer: download.file\n\nargs(download.file)\n\nfunction (url, destfile, method, quiet = FALSE, mode = \"w\", cacheOK = TRUE, \n    extra = getOption(\"download.file.extra\"), headers = NULL, \n    ...) \nNULL\n\n\nBasic file download capabilities:\n\nurl: source\ndestfile: where on your computer to store it\nmethod: what software to use in the background to download"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#data-transfer-http",
    "href": "archive/AY-2024-FALL/slides/slides09.html#data-transfer-http",
    "title": "STA 9750 - Week 9",
    "section": "Data Transfer: HTTP",
    "text": "Data Transfer: HTTP\nHTTP\n\nHyperText Transfer Protocol\nMost common (but not only) internet protocol\nAlso ftp, smtp, ssh, …\n\n“Low-level” mechanism of internet transfer\n\nMany R packages add a friendly UX\nhttr2 for low-level work (today)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#http",
    "href": "archive/AY-2024-FALL/slides/slides09.html#http",
    "title": "STA 9750 - Week 9",
    "section": "HTTP",
    "text": "HTTP\n\nHTTP has two stages:\n\n\n\nRequest\n\nURL (Host + Path)\nMethod (VERB)\nHeaders\nContent\nCookies\n\n\n\n\n\nResponse\n\nStatus Code\nHeaders\nContent\n\n\n\n\nModern (easy) APIs put most of the behavior in the URL"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#http-in-the-browser",
    "href": "archive/AY-2024-FALL/slides/slides09.html#http-in-the-browser",
    "title": "STA 9750 - Week 9",
    "section": "HTTP in the Browser",
    "text": "HTTP in the Browser\nIn Firefox: Right-Click + Inspect\nIn Chrome: Right-Click + Developer Tools"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#http-with-httr2",
    "href": "archive/AY-2024-FALL/slides/slides09.html#http-with-httr2",
    "title": "STA 9750 - Week 9",
    "section": "HTTP with httr2",
    "text": "HTTP with httr2\nhttr2 (pronounced “hitter-2”) is low-level manipulation of HTTP.\n\nlibrary(httr2)\nrequest(example_url())\n\nPretty simple so far:\n\nexample_url() starts a tiny local web host\n127.0.0.1 is localhost\n\nLive Demo Time"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#httr2-requests",
    "href": "archive/AY-2024-FALL/slides/slides09.html#httr2-requests",
    "title": "STA 9750 - Week 9",
    "section": "httr2 Requests",
    "text": "httr2 Requests\nBuild a request:\n\nreq_method\nreq_body_*\nreq_cookies_set\nreq_auth_basic / req_oauth\n\nBehaviors:\n\nreq_cache\nreq_timeout\n\nExecution:\n\nreq_perform"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#httr2-responses",
    "href": "archive/AY-2024-FALL/slides/slides09.html#httr2-responses",
    "title": "STA 9750 - Week 9",
    "section": "httr2 Responses",
    "text": "httr2 Responses\nRequest status\n\nresp_status / resp_status_desc\n\nContent: - resp_header* - resp_body_*"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides09.html#exercise",
    "href": "archive/AY-2024-FALL/slides/slides09.html#exercise",
    "title": "STA 9750 - Week 9",
    "section": "Exercise",
    "text": "Exercise\nArt Institute of Chicago API"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#mini-project-00",
    "href": "archive/AY-2024-FALL/slides/slides03.html#mini-project-00",
    "title": "STA 9750 - Week 3 Update",
    "section": "STA 9750 Mini-Project #00",
    "text": "STA 9750 Mini-Project #00\n\nMP#00 submitted\n\nA few of you didn’t submit; I’ll follow up directly for VoE\n\nMP#00 peer feedback assignments released (check GitHub)\n\nGive some feedback to your peers\nGet ideas for improving your own site"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#mini-project-01",
    "href": "archive/AY-2024-FALL/slides/slides03.html#mini-project-01",
    "title": "STA 9750 - Week 3 Update",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\n\nMP#01 released\nStart early\n\nNot too hard if everything is working (post-MP#00)\nTech support takes time"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#graduate-teaching-assistant-gta",
    "href": "archive/AY-2024-FALL/slides/slides03.html#graduate-teaching-assistant-gta",
    "title": "STA 9750 - Week 3 Update",
    "section": "Graduate Teaching Assistant (GTA)",
    "text": "Graduate Teaching Assistant (GTA)\n\nCharles Ramirez\nTwice Weekly Office Hours (Zoom)\n\nTuesdays 4-5pm\nFridays 12-1pm\n\nWill also help coordinate peer feedback (GitHub), Piazza responses, etc.\nExcellent resource for course project advice!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#piazza-participation",
    "href": "archive/AY-2024-FALL/slides/slides03.html#piazza-participation",
    "title": "STA 9750 - Week 3 Update",
    "section": "Piazza Participation",
    "text": "Piazza Participation\n\nAverage time to response &lt;9 hours\n209 posts\n\nThanks to those of you who are helping classmates!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#course-project",
    "href": "archive/AY-2024-FALL/slides/slides03.html#course-project",
    "title": "STA 9750 - Week 3 Update",
    "section": "Course Project",
    "text": "Course Project\n\n1 team already registered with me!\nPiazza discussions helping to coordinate other teams"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#upcoming-week",
    "href": "archive/AY-2024-FALL/slides/slides03.html#upcoming-week",
    "title": "STA 9750 - Week 3 Update",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nNext Wednesday at 11:45pm:\n\nNext Pre-Assignment\nMP#00 Peer Feedback due"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#faq-vector-index-printout-rules",
    "href": "archive/AY-2024-FALL/slides/slides03.html#faq-vector-index-printout-rules",
    "title": "STA 9750 - Week 3 Update",
    "section": "FAQ: Vector Index Printout Rules",
    "text": "FAQ: Vector Index Printout Rules\nDefault vector printing:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nEach line gets a new index:\n\nsqrt(1:10)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\n\nMore complex objects have alternate print styles:\n\nmatrix(1:9, nrow=3, ncol=3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nPrint width is controlled by getOption(\"width\")."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#faq-recycling-rules",
    "href": "archive/AY-2024-FALL/slides/slides03.html#faq-recycling-rules",
    "title": "STA 9750 - Week 3 Update",
    "section": "FAQ: Recycling Rules",
    "text": "FAQ: Recycling Rules\nAlignment by default:\n\nx &lt;- 1:3\ny &lt;- 4:6\nx + y\n\n[1] 5 7 9\n\n\nRecycling by default:\n\nx &lt;- 1\ny &lt;- 4:6\nx + y\n\n[1] 5 6 7\n\n\nRecycle warning when vectors don’t fit together cleanly:\n\nx &lt;- 1:2\ny &lt;- 4:6\nx + y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 5 7 7"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#faq-recycling-warning",
    "href": "archive/AY-2024-FALL/slides/slides03.html#faq-recycling-warning",
    "title": "STA 9750 - Week 3 Update",
    "section": "FAQ: Recycling Warning",
    "text": "FAQ: Recycling Warning\n\nx &lt;- 1:2\ny &lt;- 4:6\nx + y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 5 7 7\n\n\nNot a problem per se, but often a sign that something has gone wrong.\n\nscalar + vector is usually safe\n2 vectors of same size is usually safe\nvectors of different size is usually a programming mistake"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#faq-warnings-vs-errors",
    "href": "archive/AY-2024-FALL/slides/slides03.html#faq-warnings-vs-errors",
    "title": "STA 9750 - Week 3 Update",
    "section": "FAQ: Warnings vs Errors",
    "text": "FAQ: Warnings vs Errors\n\nWarnings: heuristics pointing at typical problem\n\nCode still executed without a problem\nTry to fix these unless you’re certain it’s not a problem\n\nErrors: code failed to execute\n\nYou have to fix these to run your code"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#faq-changing-built-in-functions",
    "href": "archive/AY-2024-FALL/slides/slides03.html#faq-changing-built-in-functions",
    "title": "STA 9750 - Week 3 Update",
    "section": "FAQ: Changing built-in functions",
    "text": "FAQ: Changing built-in functions\nMost built-in functions can’t / shouldn’t be changed.\nSome allow alternate behavior via additional arguments:\n\nlog(10) # Default is natural (base e) logarithm\n\n[1] 2.302585\n\nlog(10, base=10)\n\n[1] 1\n\n\nIf you want different behavior, write your own function:\n\ncosd &lt;- function(x){\n    ## Cosine in degrees\n    cos(x * pi / 180)\n}\ncosd(90)\n\n[1] 6.123234e-17\n\n\nAlways try ?name to see documentation."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides03.html#faq-git-workflow",
    "href": "archive/AY-2024-FALL/slides/slides03.html#faq-git-workflow",
    "title": "STA 9750 - Week 3 Update",
    "section": "FAQ: Git Workflow",
    "text": "FAQ: Git Workflow\nThree key commands:\n\ngit add: add some changes to a ‘box’\ngit commit: seal the ‘box’\ngit push: send the ‘box’ to GitHub\n\nGit pane in RStudio shows uncommited changes, not files.\nIf a file ‘vanishes’ after a commit, that’s good!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides10.html#mini-project-03",
    "href": "archive/AY-2024-FALL/slides/slides10.html#mini-project-03",
    "title": "STA 9750 - Week 10",
    "section": "STA 9750 Mini-Project #03",
    "text": "STA 9750 Mini-Project #03\nNow online\nDue November 13th\n\nGitHub post (used for peer feedback) AND Brightspace\nThree Weeks: don’t wait until the very end\nShould be less demanding than MP #01 and MP#02\n\nLots of little files. No big files!\nMaps and election retrodiction\n\n\n\nPay attention to the rubric"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides10.html#mini-project-03-1",
    "href": "archive/AY-2024-FALL/slides/slides10.html#mini-project-03-1",
    "title": "STA 9750 - Week 10",
    "section": "STA 9750 Mini-Project #03",
    "text": "STA 9750 Mini-Project #03\nTips:\n\ngeom_sf by default puts a thin grey boundary around each geometry. Disable this by using geom_sf(color=NA).\nYou can plot a state in a certain color by plotting all of its districts in that same color. Alternatively, use st_combine to combine geometries.\nEach state gets \\(R+2\\) ECVs where \\(R\\) is the number of congressional districts in that state.\nst_simplify can “smooth out” boundaries to make things easier to plot"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides10.html#upcoming-mini-projects",
    "href": "archive/AY-2024-FALL/slides/slides10.html#upcoming-mini-projects",
    "title": "STA 9750 - Week 10",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#04: Something financial\n\nAny requests?"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides10.html#pre-assignments",
    "href": "archive/AY-2024-FALL/slides/slides10.html#pre-assignments",
    "title": "STA 9750 - Week 10",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is November 13th\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides10.html#grading",
    "href": "archive/AY-2024-FALL/slides/slides10.html#grading",
    "title": "STA 9750 - Week 10",
    "section": "Grading",
    "text": "Grading\nWe owe you:\n\nMP#02 final average\nMP#02 peer meta-review\n\nWe will owe you:\n\nMid-Term Check-In Feedback"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides10.html#course-support",
    "href": "archive/AY-2024-FALL/slides/slides10.html#course-support",
    "title": "STA 9750 - Week 10",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(38\\) minute average response time)\n\n\nChange: MW Thursday Zoom OH now 4:00pm to 5:00pm"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides10.html#upcoming",
    "href": "archive/AY-2024-FALL/slides/slides10.html#upcoming",
    "title": "STA 9750 - Week 10",
    "section": "Upcoming",
    "text": "Upcoming\nNext Week:\n\nMini-Project #03\nPre-Assignment\n\nNov 20:\n\nMP#03 Peer Feedback\nPre Assignment\n\nNov 27 - Thanksgiving Holiday (No Class on Nov 28)\n\nCheck-In Peer Feedback (Vocat)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides10.html#check-in-presentations",
    "href": "archive/AY-2024-FALL/slides/slides10.html#check-in-presentations",
    "title": "STA 9750 - Week 10",
    "section": "Check-In Presentations",
    "text": "Check-In Presentations\nToday, we’re looking for:\n\n6 minutes\nLocking in on specific questions\nEngagement with existing literature\n\nMainly, we want to see that you will be able to succeed"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides10.html#presentation-order",
    "href": "archive/AY-2024-FALL/slides/slides10.html#presentation-order",
    "title": "STA 9750 - Week 10",
    "section": "Presentation Order",
    "text": "Presentation Order\n\n\n\nOrder\nTeam\n\nOrder\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#mini-project-01",
    "href": "archive/AY-2024-FALL/slides/slides05.html#mini-project-01",
    "title": "STA 9750 - Week 5 Update",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\nSubmission due yesterday at 11:45pm\n\n\\(\\approx 90\\%\\) submitted on time\nSubmit early and submit often\n\nLess “last minute” tech support going forward\n\nUse Piazza and use your peers\n\nVery impressed by Detailed Analyses, Code Folding and Callout Blocks, Fancy gt Tables, Graphics"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#mini-project-01---peer-feedback",
    "href": "archive/AY-2024-FALL/slides/slides05.html#mini-project-01---peer-feedback",
    "title": "STA 9750 - Week 5 Update",
    "section": "STA 9750 Mini-Project #01 - Peer Feedback",
    "text": "STA 9750 Mini-Project #01 - Peer Feedback\nPeer feedback assigned on GitHub this morning\n\n\\(\\approx 4\\) feedbacks each\nTake this seriously: around 20% of this assignment is “meta-review”\nGoal: rigorous constructive critique\n\n\nSubmissions may not map perfectly to rubric - use your best judgement\n\n\nLearn from this! What can you adapt for MP#02?"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#mini-project-02",
    "href": "archive/AY-2024-FALL/slides/slides05.html#mini-project-02",
    "title": "STA 9750 - Week 5 Update",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nMP#02 released - Hollywood Movies\n\nDue October 23rd\n\nGitHub post (used for peer feedback) AND Brightspace\nOne Month: don’t wait until the very end\n\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness, not necessarily correctness"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#upcoming-mini-projects",
    "href": "archive/AY-2024-FALL/slides/slides05.html#upcoming-mini-projects",
    "title": "STA 9750 - Week 5 Update",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#03: Political Analysis\nMP#04: Retirement Forecasting"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#pre-assignments",
    "href": "archive/AY-2024-FALL/slides/slides05.html#pre-assignments",
    "title": "STA 9750 - Week 5 Update",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is October 16th"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#course-project",
    "href": "archive/AY-2024-FALL/slides/slides05.html#course-project",
    "title": "STA 9750 - Week 5 Update",
    "section": "Course Project",
    "text": "Course Project\n6 teams already formed!\n\nBreakout rooms in teams\n\nTeam/Room 1: GZ + VF + EY + AG + TD\nTeam/Room 2: YZ + HM + TN + NG\nTeam/Room 3: SK + HA + DS\nTeam/Room 4: AC + EL + CL + WP\nTeam/Room 5: CC + AO + HS + MT + DM\nTeam/Room 6: SK + CM + MK + JV (pending confirmation)\n\nAll team commitments due via email 2024-10-02"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#course-support",
    "href": "archive/AY-2024-FALL/slides/slides05.html#course-support",
    "title": "STA 9750 - Week 5 Update",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nAsynchronous\n\nPiazza (&lt;40 minute average response time)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#upcoming-week",
    "href": "archive/AY-2024-FALL/slides/slides05.html#upcoming-week",
    "title": "STA 9750 - Week 5 Update",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nNext Wednesday at 11:45pm:\n\nMP#01 peer feedback due\nTeam membership due\n\nNo class on October 3rd"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#october-10---project-proposal-presentations",
    "href": "archive/AY-2024-FALL/slides/slides05.html#october-10---project-proposal-presentations",
    "title": "STA 9750 - Week 5 Update",
    "section": "October 10 - Project Proposal Presentations",
    "text": "October 10 - Project Proposal Presentations\nOfficial Description\n\n6 minute presentation\nKey topics:\n\nAnimating Question\nTeam Roster\n\nAlso discuss: Possible specific questions, data sources, analytical plan, anticipated challenges\n\nPeer feedback mechanism TBD"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-subqueries",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-subqueries",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: Subqueries",
    "text": "FAQ: Subqueries\n\n[W]ill we be learning how to perform joins within a subquery?\n\nYou don’t need subqueries in R since it’s an imperative language. Just create a new variable to represent the result of the subquery and use that in the next command.\nSELECT first_name, last_name\nFROM collectors\nWHERE id IN (\n    SELECT collector_id\n    FROM sales\n);\ncollector_ids &lt;- sales |&gt; pull(collector_id)\ncollectors |&gt; filter(id %in% collector_ids) |&gt; select(first_name, last_name)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-data-integrity",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-data-integrity",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: Data Integrity",
    "text": "FAQ: Data Integrity\n\n[H]ow can we ensure that the information [resulting from a join] is accurate and not repeated?\n\n\nIf you have a true unique ID, you’re usually safe\nPay attention to all warnings\nManually examine the result of any joins"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-performance-on-large-data-sets",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-performance-on-large-data-sets",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: Performance on Large Data Sets",
    "text": "FAQ: Performance on Large Data Sets\n\nWill joining large data sets […] affect performance?\n\nSomewhat - larger data sets are always slower.\nBigger danger is “bad joins” creating huge data automatically.\nNote that R is less “smart” than SQL, so won’t optimize execution order for you automatically."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-what-is-the-role-of-pivot_wider",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-what-is-the-role-of-pivot_wider",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: What is the Role of pivot_wider?",
    "text": "FAQ: What is the Role of pivot_wider?\n\nIs [pivot_wider] just for formatting?\n\n\nlibrary(dplyr); library(tidyr); library(palmerpenguins)\npenguins |&gt; drop_na() |&gt; \n    group_by(sex, species) |&gt; \n    summarize(weight = mean(body_mass_g)) |&gt;\n    pivot_wider(id_cols=species, \n                names_from=sex,\n                values_from=weight) |&gt;\n    mutate(gender_diff = male - female)\n\n# A tibble: 3 × 4\n  species   female  male gender_diff\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     3369. 4043.        675.\n2 Chinstrap  3527. 3939.        412.\n3 Gentoo     4680. 5485.        805."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-dplyr-joins-vs-sql-joins",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-dplyr-joins-vs-sql-joins",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: dplyr joins vs SQL joins",
    "text": "FAQ: dplyr joins vs SQL joins\n\nWhat is the difference between dplyr and SQL joins?\n\nNot too much - biggest difference is no INDEX or FOREIGN KEY in R so less guarantees of data integrity."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-when-to-use-anti_join",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-when-to-use-anti_join",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: When to use anti_join?",
    "text": "FAQ: When to use anti_join?\nRare: looking for unmatched rows. - Useful to find data integrity issues or ‘implicit’ missingness. - I use an anti_join to find students who haven’t submitted an assignment.\nsemi_join appears in MP #02."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-many-to-many-warning",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-many-to-many-warning",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: many-to-many Warning",
    "text": "FAQ: many-to-many Warning\nTricky to address, but fortunately pretty rare.\n\nSQL explicitly forbids many-to-many\nUsually a sign that a “key” isn’t really unique\n\nCheck for duplicates in x and y tables\nCan occur with “fancy” joins (rolling, inequality)\n\nAdd additional join variables to break “duplication”"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-how-to-check-efficiency",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-how-to-check-efficiency",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: How to Check Efficiency?",
    "text": "FAQ: How to Check Efficiency?\nNo automatic way. Some rules of thumb:\n\nDon’t create large tables just to filter down\n\nfilter before join when possible\n\nfull_outer join is a bit dangerous\ncross_join is rarely the right answer"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-tidyr-vs-dplyr",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-tidyr-vs-dplyr",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: tidyr vs dplyr",
    "text": "FAQ: tidyr vs dplyr\n\nIs tidyr more efficient than dplyr?\n\nNope - different packages from the same developers.\nDesigned to work together elegantly."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-rare-joins",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-rare-joins",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: Rare Joins",
    "text": "FAQ: Rare Joins\n\nPlease explain what cross_join, filter joins, and nest_join are?\n\n\ncross_join: dangerous.\n\nCreates “all pairs” of rows. Useful for ‘design’ problems\n\nfilter joins (anti_, semi_):\n\nHunting down quietly missing data.\nFiltering to sub-samples (see MP#02)\n\nnest_join: beyond this course.\n\nleft_join with extra structure to output."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#faq-how-to-pick-a-join",
    "href": "archive/AY-2024-FALL/slides/slides05.html#faq-how-to-pick-a-join",
    "title": "STA 9750 - Week 5 Update",
    "section": "FAQ: How to Pick a Join",
    "text": "FAQ: How to Pick a Join\n\nHow do I decide which type of join is most approriate for a given analysis?\n\nTopic of today’s work."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#other-tips",
    "href": "archive/AY-2024-FALL/slides/slides05.html#other-tips",
    "title": "STA 9750 - Week 5 Update",
    "section": "Other Tips",
    "text": "Other Tips\n\nDisable RStudio’s visual Quarto editor. It’s more trouble than it’s worth. To stop it from opening by default, add editor: source in the header of your qmd files.\nQuarto depends on file structure for organizing content. The main directory (STA9750-2025-SPRING) should hold all of your input files. You should never directly put anything in the docs/ folder. That’s where generated output should live.\nWhen I leave &lt;GITHUB_NAME&gt; or similar in instructions, put in your GitHub ID. (And make sure to remove the &lt; and &gt; symbols)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides05.html#diving-deeper-with-dplyr---joins-and-pivots",
    "href": "archive/AY-2024-FALL/slides/slides05.html#diving-deeper-with-dplyr---joins-and-pivots",
    "title": "STA 9750 - Week 5 Update",
    "section": "Diving Deeper with dplyr - Joins and Pivots",
    "text": "Diving Deeper with dplyr - Joins and Pivots\nData Set: nycflights13\nExercise: Lab #05"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#mini-project-01",
    "href": "archive/AY-2024-FALL/slides/slides07.html#mini-project-01",
    "title": "STA 9750 - Week 7",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\nGrades returned this afternoon.\nReview regrade policy and late work policy if you have questions."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#mini-project-02",
    "href": "archive/AY-2024-FALL/slides/slides07.html#mini-project-02",
    "title": "STA 9750 - Week 7",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nMP#02 - Hollywood Movies – Due October 23rd\n\nGitHub post (used for peer feedback) AND Brightspace\nStart early to avoid Git issues\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness\nUse what you learned from MP#01\nPre-processed data now available as well"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#upcoming-mini-projects",
    "href": "archive/AY-2024-FALL/slides/slides07.html#upcoming-mini-projects",
    "title": "STA 9750 - Week 7",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#03: Political Analysis\nMP#04: Retirement Forecasting"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#course-project",
    "href": "archive/AY-2024-FALL/slides/slides07.html#course-project",
    "title": "STA 9750 - Week 7",
    "section": "Course Project",
    "text": "Course Project\nProposal feedback soon (need to do 2 more before releasing)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#pre-assignments",
    "href": "archive/AY-2024-FALL/slides/slides07.html#pre-assignments",
    "title": "STA 9750 - Week 7",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is October 23rd\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#course-support",
    "href": "archive/AY-2024-FALL/slides/slides07.html#course-support",
    "title": "STA 9750 - Week 7",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday for rest of semester\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(&lt;30\\) minute average response time)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#upcoming-week",
    "href": "archive/AY-2024-FALL/slides/slides07.html#upcoming-week",
    "title": "STA 9750 - Week 7",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nDue Wednesday at 11:45pm:\n\nPre-Assignment #08 (Brightspace)\n\nAdvanced plotting with ggplot2\n\nMP #02 on GitHub AND Brightspace"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#additional-resources",
    "href": "archive/AY-2024-FALL/slides/slides07.html#additional-resources",
    "title": "STA 9750 - Week 7",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nC. Wilke. Fundamentals of Data Visualization\nK. Healy. Data Visualization\nH. Wickham ggplot2: Elegant Visualizations for Data Analysis\n\n\n\nB. Yu and R. Barter Veridical Data Science"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-ggplot2-vs-tableau",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-ggplot2-vs-tableau",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: ggplot2 vs Tableau",
    "text": "FAQ: ggplot2 vs Tableau\n\nTableau\n\n$$$\nIT department automatically integrates with data sources\nEasy, if it does what you want\n\nggplot2\n\nFree\nCan use arbitrary data sources, with effort\nFlexible / customizable"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-ggplot2-vs-matplotlib",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-ggplot2-vs-matplotlib",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: ggplot2 vs matplotlib",
    "text": "FAQ: ggplot2 vs matplotlib\n\nggplot2\n\nData visualizations\nEnforces “good practice” via gg\n\nmatplotlib\n\nScientific visualizations\nMore flexible for good or for ill\nInspired by Matlab plotting\n\n\nClosest Python analogue to ggplot2 is seaborn"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-why-use-instead-of",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-why-use-instead-of",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Why use + instead of |>",
    "text": "FAQ: Why use + instead of |&gt;\n\nggplot2 is older than |&gt;\nPer H. Wickham: if ggplot3 ever gets made, will use |&gt;\nUnlikely to change: too much code depends on it"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-performance",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-performance",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Performance",
    "text": "FAQ: Performance\n\nI tried an interactive plot with \\(n=132,000\\) points, but it brought my computer to a halt. [Ed. Paraphrased]\n\nThat’s a lot of plots!!\nggplot2 is itself pretty fast, but it depends on (possibly slow) graphics backends\n\nDifferent file types implement graphics differently.\nYou should also think about overplotting / pre-processing"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-overplotting",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-overplotting",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Overplotting",
    "text": "FAQ: Overplotting\nLarge data sets can lead to overplotting:\n\nPoints “on top of” each other\nCan also occur with “designed” experiments / rounded data\n\nWays to address:\n\ngeom_jitter\ngeom_hex"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-overplotting-1",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-overplotting-1",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Overplotting",
    "text": "FAQ: Overplotting\nJitter: add a bit of random noise so points don’t step on each other\n\nlibrary(ggplot2); library(patchwork)\np &lt;- ggplot(mpg, aes(cyl, hwy))\np1 &lt;- p + geom_point() + ggtitle(\"geom_point\")\np2 &lt;- p + geom_jitter() + ggtitle(\"geom_jitter\")\np1 + p2"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-hexagonal-binning",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-hexagonal-binning",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Hexagonal Binning",
    "text": "FAQ: Hexagonal Binning\nLittle “heatmaps” of counts. Hexagons to avoid weird rounding artifacts\n\nlibrary(ggplot2); library(patchwork)\np &lt;- ggplot(diamonds, aes(carat, price))\np1 &lt;- p + geom_point() + ggtitle(\"geom_point\")\np2 &lt;- p + geom_hex() + ggtitle(\"geom_hex\")\np1 + p2"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-inside-vs.-outside-aes",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-inside-vs.-outside-aes",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Inside vs. Outside aes()",
    "text": "FAQ: Inside vs. Outside aes()\naes maps data to values. Outside of aes, set constant value\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, \n       aes(x=bill_length_mm, y=bill_depth_mm, color=species))+ geom_point()"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-inside-vs.-outside-aes-1",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-inside-vs.-outside-aes-1",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Inside vs. Outside aes()",
    "text": "FAQ: Inside vs. Outside aes()\naes maps data to values. Outside of aes, set constant value\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, \n       aes(x=bill_length_mm, y=bill_depth_mm))+ geom_point(color=\"blue\")"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-global-vs-geom_-specific-aes",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-global-vs-geom_-specific-aes",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Global vs geom_ specific aes()",
    "text": "FAQ: Global vs geom_ specific aes()\n\nElements set in ggplot() apply to entire plot\nElements set in specific geom apply there only\n\nOverride globals\n\n\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, \n       aes(x=bill_length_mm, y=bill_depth_mm, color=species))+\n    geom_smooth() + \n    geom_point(color=\"blue\")"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-how-to-choose-plot-types",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-how-to-choose-plot-types",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: How to choose plot types",
    "text": "FAQ: How to choose plot types\nTwo “modes”\n\nExploratory data analysis. Quick, rapid iteration, for your eyes only\n\nLet the data tell you a story\nLow pre-processing: scatter plots, lines, histograms\n\n“Publication quality”. Polished,\n\nYou tell the reader a story\nMore processing, more modeling: trends, line segments, ribbons"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\nThree types of color palettes:\n\nSequential: ordered from 0 to “high”\n\nExample: rain forecast in different areas\n\nDiverging: ordered from -X to +X with meaningful 0 in the middle\n\nExample: political leaning\n\nQualitative: no ordering\n\n\nWhen mapping quantitative variables to palettes (sequential/diverging), two approaches:\n\nBinned: \\([0, 1)\\) light green, \\([1, 3)\\) medium green; \\([3, 5]\\) dark green\nContinuous"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes-1",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes-1",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_distiller(type=\"seq\") # Continuous"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes-2",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes-2",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_fermenter(type=\"seq\") # Binned"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes-3",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes-3",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_fermenter(type=\"seq\") # Binned + Sequential"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes-4",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes-4",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() +\n    scale_color_fermenter(type=\"qual\") # Binned + Qualitative"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes-5",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-color-palettes-5",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_fermenter(type=\"div\") # Binned + Diverging"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-how-to-hard-code-colors",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-how-to-hard-code-colors",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: How to “hard-code” colors",
    "text": "FAQ: How to “hard-code” colors\n\nlibrary(dplyr)\ndata &lt;- data.frame(x = rnorm(5), \n                   y = rnorm(5), \n                   group = c(\"a\", \"a\", \"b\", \"b\", \"b\"))\n\ndata |&gt; \n    group_by(group) |&gt;\n    mutate(n_count = n()) |&gt;\n    ungroup() |&gt;\n    mutate(color = ifelse(n_count == max(n_count), \"red\", \"black\")) |&gt;\n    ggplot(aes(x=x, y=y, shape=group, color=color)) + \n    geom_point() + \n    scale_color_identity()"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-how-to-customize-themes",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-how-to-customize-themes",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: How to Customize Themes",
    "text": "FAQ: How to Customize Themes\nBuilt-in themes + ggthemes package:\n\nlibrary(ggplot2); library(ggthemes); \nlibrary(palmerpenguins); library(ggpmisc)\np &lt;- ggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    facet_wrap(~species)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nDefault theme (ggplot2::theme_grey()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-1",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-1",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nBlack and White theme (ggplot2::theme_bw()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-2",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-2",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nMinimal theme (ggplot2::theme_minimal()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-3",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-3",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nLight theme (ggplot2::theme_light()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-4",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-4",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nDark theme (ggplot2::theme_dark()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-5",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-5",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nExcel theme (ggthemes::theme_excel()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-6",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-6",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nGoogle Docs theme (ggthemes::theme_gdocs()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-7",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-7",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nThe Economist theme (ggthemes::theme_economist()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-8",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-8",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nThe Economist theme (ggthemes::theme_economist()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-9",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-9",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nSolarized theme (ggthemes::theme_solarized()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-10",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-10",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nSolarized2 theme (ggthemes::theme_solarized_2()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-11",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-11",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nStata theme (ggthemes::theme_stata()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-12",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-12",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nTufte theme (ggthemes::theme_tufte()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-13",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-13",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nWall Street Journal theme (ggthemes::theme_wsj()):"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-14",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-themes-14",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nMany more online:\n\nThemePark for movie themes"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-order-of-layers",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-order-of-layers",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Order of Layers",
    "text": "FAQ: Order of Layers\nOrder of layers technically matters, but the effect is small\n\np1 &lt;- ggplot(penguins, aes(x=bill_length_mm, y=flipper_length_mm)) +\n        geom_point(color=\"black\") + \n        geom_smooth(color=\"blue\", method=\"lm\") + ggtitle(\"Line on points\")\np2 &lt;- ggplot(penguins, aes(x=bill_length_mm, y=flipper_length_mm)) +\n        geom_smooth(color=\"blue\", method=\"lm\") + \n        geom_point(color=\"black\") + ggtitle(\"Points on line\")\np1 + p2"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-order-of-layers-1",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-order-of-layers-1",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Order of layers",
    "text": "FAQ: Order of layers\nOrder matters more with theme. Adding a theme_*() will override any theme() customization you did:\n\np1 &lt;- p + theme_bw() + theme(legend.position=\"bottom\")\np2 &lt;- p + theme(legend.position=\"bottom\") + theme_bw() \np1 + p2"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-stat_poly_lineeq-vs-geom_smooth",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-stat_poly_lineeq-vs-geom_smooth",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: stat_poly_{line,eq} vs geom_smooth",
    "text": "FAQ: stat_poly_{line,eq} vs geom_smooth\nBy default geom_smooth fits a generalized additive model (GAM)\nggpmisc::stat_poly_{line,eq} fit linear models, so they can expose more machinery.\n\nWhat is a GAM? Take 9890 with me (Spring, Tuesdays at 6) to find out!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-titles-and-captions",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-titles-and-captions",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Titles and Captions",
    "text": "FAQ: Titles and Captions\n\nggplot() + \n    labs(title=\"Title\", subtitle=\"Subtitle\", caption=\"Caption\",\n         tag=\"Tag\", alt=\"Alt-Text\", alt_insight=\"Alt-Insight\")\n\n\n+ggtitle(\"text\") is just shorthand for +labs(title=\"text\")"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-relative-importance-of-aesthetics",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-relative-importance-of-aesthetics",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Relative Importance of Aesthetics",
    "text": "FAQ: Relative Importance of Aesthetics\nPerceptually:\n\nLocation &gt; Color &gt; Size &gt; Shape\n\nHumans are better at:\n\nLength &gt; Area &gt; Volume"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-when-to-use-facets",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-when-to-use-facets",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: When to Use Facets?",
    "text": "FAQ: When to Use Facets?\nFacets are group_by for plots. Useful for\n\nDistinguishing intra- vs inter-group trends\nAvoiding overplotting"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-simpsons-paradox",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-simpsons-paradox",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Simpson’s Paradox",
    "text": "FAQ: Simpson’s Paradox"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-simpsons-paradox-1",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-simpsons-paradox-1",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Simpson’s Paradox",
    "text": "FAQ: Simpson’s Paradox"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-twin-axes-plots",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-twin-axes-plots",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Twin Axes Plots",
    "text": "FAQ: Twin Axes Plots\n\nHow can I implement a dual (twin) axis plot in ggplot2?\n\nDisfavored. But if you must …\nsec.axis\nDoesn’t allow arbitrary secondary axes; allows transformed axes (e.g., Celsius and Fahrenheit)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-embedding-images-in-ggplot",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-embedding-images-in-ggplot",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Embedding images in ggplot",
    "text": "FAQ: Embedding images in ggplot\nSee the ggimage or ggflags package for images as “points”:\n\n#devtools::install_github(\"jimjam-slam/ggflags\"); \nlibrary(ggflags)\nd &lt;- data.frame(x=rnorm(50), y=rnorm(50), \n                country=sample(c(\"ar\",\"fr\", \"nz\", \"gb\", \"es\", \"ca\"), 50, TRUE), \n                stringsAsFactors = FALSE)\nggplot(d, aes(x=x, y=y, country=country, size=x)) + \n  geom_flag() + \n  scale_country()"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#faq-embedding-images",
    "href": "archive/AY-2024-FALL/slides/slides07.html#faq-embedding-images",
    "title": "STA 9750 - Week 7",
    "section": "FAQ: Embedding Images",
    "text": "FAQ: Embedding Images\nSee cowplot::draw_image() for image background:\n\nlibrary(cowplot)\np &lt;- ggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.7) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_half_open(12)\n\nlogo_file &lt;- system.file(\"extdata\", \"logo.png\", package = \"cowplot\")\nggdraw() +\n  draw_image(\n    logo_file, scale = .7\n  ) +\n  draw_plot(p)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#diving-deeper-with-ggplot2",
    "href": "archive/AY-2024-FALL/slides/slides07.html#diving-deeper-with-ggplot2",
    "title": "STA 9750 - Week 7",
    "section": "Diving Deeper with ggplot2",
    "text": "Diving Deeper with ggplot2\nData Sets:\n\ndiamonds from the ggplot2 package\ncdiac from the CVXR package\ngapminder from the gapminder package\n\nYou need to install CVXR and gapminder now.\nExercise: Lab #07"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#diving-deeper-with-ggplot2-learning-goals",
    "href": "archive/AY-2024-FALL/slides/slides07.html#diving-deeper-with-ggplot2-learning-goals",
    "title": "STA 9750 - Week 7",
    "section": "Diving Deeper with ggplot2: Learning Goals",
    "text": "Diving Deeper with ggplot2: Learning Goals\nToday:\n\nFluency with basic geoms\nAnimation\n\nNext Week:\n\nSpatial data\nInteractive graphics\nDashboards (time allowing)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides07.html#breakout-rooms",
    "href": "archive/AY-2024-FALL/slides/slides07.html#breakout-rooms",
    "title": "STA 9750 - Week 7",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nRoom\nTeam\n\nRoom\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#mini-project-04",
    "href": "archive/AY-2024-FALL/slides/slides12.html#mini-project-04",
    "title": "STA 9750 - Week 12",
    "section": "STA 9750 Mini-Project #04",
    "text": "STA 9750 Mini-Project #04\nMP#04 online now\n\nDue 2024-12-04 (\\(\\approx\\) 3 weeks - 2 remaining)\nTopic: financial modeling\n\nComparison of two retirement plans\nHistorical data + Monte Carlo (“bootstrapping”)\n\nFormat:\n\nDecision Analytics - Play the role of financial advisor\nGitHub post AND Brightspace submission"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#week-12-pre-assignment",
    "href": "archive/AY-2024-FALL/slides/slides12.html#week-12-pre-assignment",
    "title": "STA 9750 - Week 12",
    "section": "Week 12 Pre-Assignment",
    "text": "Week 12 Pre-Assignment\nDue at midnight tonight - take a moment to do it now if you haven’t already!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#pre-assignments",
    "href": "archive/AY-2024-FALL/slides/slides12.html#pre-assignments",
    "title": "STA 9750 - Week 12",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext (and final!) pre-assignment is December 4th\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#next-semester-topics",
    "href": "archive/AY-2024-FALL/slides/slides12.html#next-semester-topics",
    "title": "STA 9750 - Week 12",
    "section": "Next Semester Topics",
    "text": "Next Semester Topics\n\nNYC Open Data\nSports (Baseball?)\nSpotify / Music\nHealthcare / Pharmaceutical (might be tricky…)\nVideo Games\nQuant Finance / Time Series?\nBaruch Demographics\nJob Market\nReal Estate"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#grading",
    "href": "archive/AY-2024-FALL/slides/slides12.html#grading",
    "title": "STA 9750 - Week 12",
    "section": "Grading",
    "text": "Grading\nReturned:\n\nMP#02 grade\nMP#02 meta-review grade\nVideos now on Vocat\n\nWe owe you:\n\nMid-Term Check-In Feedback"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#grading---ex-post-adjustments",
    "href": "archive/AY-2024-FALL/slides/slides12.html#grading---ex-post-adjustments",
    "title": "STA 9750 - Week 12",
    "section": "Grading - Ex Post Adjustments",
    "text": "Grading - Ex Post Adjustments\nFYI: At the end of the course, I curve individual peer grades.\nExample: If grader \\(X\\) is on average, 5 points lower, I re-center all their grades, raising the gradees by an average of 1.25.\nTry to be consistent over the semester so I can calibrate this correctly."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#github-notifications",
    "href": "archive/AY-2024-FALL/slides/slides12.html#github-notifications",
    "title": "STA 9750 - Week 12",
    "section": "GitHub Notifications",
    "text": "GitHub Notifications\nMake sure you check GitHub notifications, via email or at https://github.com/notifications to make sure you get all peer feedback assignments.\n\nI tag you in other folks’ repo when you are supposed to review\nPeople tagged in your repo are evaluating you"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#course-support",
    "href": "archive/AY-2024-FALL/slides/slides12.html#course-support",
    "title": "STA 9750 - Week 12",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(38\\) minute average response time)\n\n\nChange: MW Thursday Zoom OH now 4:00pm to 5:00pm"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#upcoming",
    "href": "archive/AY-2024-FALL/slides/slides12.html#upcoming",
    "title": "STA 9750 - Week 12",
    "section": "Upcoming",
    "text": "Upcoming\nNov 27 - Thanksgiving Holiday (No Class on Nov 28)\n\nCheck-In Peer Feedback (Vocat)"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#comments",
    "href": "archive/AY-2024-FALL/slides/slides12.html#comments",
    "title": "STA 9750 - Week 12",
    "section": "Comments",
    "text": "Comments\nBad - Trivial:\n\n# Set x to 3\nx &lt;- 3\n\nBad - Opaque:\n\n# Follow instructions\nx &lt;- 3\n\nBad - Redundant / Explaining Code\n\n# Fit a model\nmod &lt;- model(x, y)\n\n# Build a query\nquery_build() |&gt; query_add() |&gt; query_formulate()"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#comments-1",
    "href": "archive/AY-2024-FALL/slides/slides12.html#comments-1",
    "title": "STA 9750 - Week 12",
    "section": "Comments",
    "text": "Comments\nGood - Purpose (“Business Logic”):\n\n# Regulation XX.YY requires us to apply a risk multiplier to output\n# As of 2024-11-01, CFO says risk multiplier is 3\n# Cf. Email to risk group, subject \"New Risk Multiplier\"\nRISK_MULTIPLIER &lt;- 3\n\nGood - Higher Level Structure (Example from googledrive package):\n\n# https://github.com/gaborcsardi/rencfaq#with-base-r\nwrite_utf8 &lt;- function(text, path = NULL) {\n  # sometimes we use writeLines() basically to print something for a snapshot\n  if (is.null(path)) {\n    return(base::writeLines(text))\n  }\n\n  # step 1: ensure our text is utf8 encoded\n  utf8 &lt;- enc2utf8(text)\n  upath &lt;- enc2utf8(path)\n\n  # step 2: create a connection with 'native' encoding\n  # this signals to R that translation before writing\n  # to the connection should be skipped\n  con &lt;- file(upath, open = \"w+\", encoding = \"native.enc\")\n  withr::defer(close(con))\n\n  # step 3: write to the connection with 'useBytes = TRUE',\n  # telling R to skip translation to the native encoding\n  base::writeLines(utf8, con = con, useBytes = TRUE)\n}"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#comments-2",
    "href": "archive/AY-2024-FALL/slides/slides12.html#comments-2",
    "title": "STA 9750 - Week 12",
    "section": "Comments",
    "text": "Comments\nMore Advice on StackOverflow"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-unicode-resources",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-unicode-resources",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Unicode Resources",
    "text": "FAQ: Unicode Resources\n\nUnicode Tables: unicodeplus.com/\nTaco Emoji History\nTaco Emoji Controversy"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-regular-expression-tools",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-regular-expression-tools",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Regular Expression Tools",
    "text": "FAQ: Regular Expression Tools\n\nTesting Regular Expressions Interactively: regex101.com/\nAlternative regexr.com/\nAutomated Regular Expression Builder: regex-generator\nAI Regexp Builder: hregexgo.com/"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-substrings-and-string-splitting",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-substrings-and-string-splitting",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Substrings and String Splitting",
    "text": "FAQ: Substrings and String Splitting\n\nfruits &lt;- c(\"apples and oranges and pears and bananas\", \n            \"pineapples and mangos and guavas\")\n\nstringr::str_split(fruits, \" and \")\n\n[[1]]\n[1] \"apples\"  \"oranges\" \"pears\"   \"bananas\"\n\n[[2]]\n[1] \"pineapples\" \"mangos\"     \"guavas\"    \n\n\n\nstringr::str_split_fixed(fruits, \"and\", n=2)\n\n     [,1]          [,2]                            \n[1,] \"apples \"     \" oranges and pears and bananas\"\n[2,] \"pineapples \" \" mangos and guavas\""
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#sub-strings-splitting",
    "href": "archive/AY-2024-FALL/slides/slides12.html#sub-strings-splitting",
    "title": "STA 9750 - Week 12",
    "section": "Sub-Strings / Splitting",
    "text": "Sub-Strings / Splitting\n\nx &lt;- \"Baruch College, CUNY\"\nstringr::str_sub(x, end=6) # Includes endpoints\n\n[1] \"Baruch\"\n\n\n\nstringr::str_sub(x, start=-4) # Count from end\n\n[1] \"CUNY\"\n\n\n\nx &lt;- c(\"Baruch College, CUNY\", \"Brooklyn College, CUNY\")\nstringr::str_sub(x, end=-7) # Drop last _6_\n\n[1] \"Baruch College\"   \"Brooklyn College\""
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-start-and-end-anchors",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-start-and-end-anchors",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Start and End Anchors",
    "text": "FAQ: Start and End Anchors\n\nWhen to use the ^ and $ anchors?\n\nStart and end of a line.\n\nVery useful for structured text (computer log outputs)\nIn data analysis, a bit less useful\n\nApplied to output of str_split"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-exclusion-detection",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-exclusion-detection",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Exclusion + Detection",
    "text": "FAQ: Exclusion + Detection\n\nx &lt;- c(\"10 blue fish\", \"three wet goats\")\nstringr::str_detect(x, \"[^0123456789]\")\n\n[1] TRUE TRUE\n\n\nstr_detect has a negate option:\n\nstringr::str_detect(x, \"[0-9]\", negate=TRUE)\n\n[1] FALSE  TRUE"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-str_detect-vs-str_match-vs-str_extract",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-str_detect-vs-str_match-vs-str_extract",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: str_detect vs str_match vs str_extract",
    "text": "FAQ: str_detect vs str_match vs str_extract\n\nstr_detect is there a ‘fit’?\nstr_extract extract the whole ‘fit’\nstr_match extract specific groups\n\n\nx &lt;- \"Baruch College, CUNY is a wonderful place to work!\"\nstringr::str_detect(x, \"(.*), CUNY\")\n\n[1] TRUE\n\nstringr::str_extract(x, \"(.*), CUNY\")\n\n[1] \"Baruch College, CUNY\"\n\nstringr::str_match(x, \"(.*), CUNY\")\n\n     [,1]                   [,2]            \n[1,] \"Baruch College, CUNY\" \"Baruch College\""
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-subset-selection-indexing",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-subset-selection-indexing",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Subset Selection + Indexing",
    "text": "FAQ: Subset Selection + Indexing\nstr_match(group=) is useful for complex data extraction.\n\nx &lt;- c(\"Michael Weylandt teaches STA9750\", \"KRR teaches STA9891\")\npattern &lt;- c(\"(.*) teaches (.*)\")\nstringr::str_extract(x, pattern, group=1)\n\n[1] \"Michael Weylandt\" \"KRR\"             \n\nstringr::str_extract(x, pattern, group=2)\n\n[1] \"STA9750\" \"STA9891\"\n\n\n(Not sure what negatives do here…)\nAlso allows named groups:\n\nx &lt;- c(\"Michael Weylandt teaches STA9750 on Thursday\", \"KRR teaches STA9891 on Wednesday\")\npattern &lt;- c(\"(?&lt;instructor&gt;.*) teaches (?&lt;course&gt;.*) on (?&lt;weekday&gt;.*)\")\nstringr::str_match(x, pattern) |&gt; as.data.frame()\n\n                                            V1       instructor  course\n1 Michael Weylandt teaches STA9750 on Thursday Michael Weylandt STA9750\n2             KRR teaches STA9891 on Wednesday              KRR STA9891\n    weekday\n1  Thursday\n2 Wednesday"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-homoglyphs",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-homoglyphs",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Homoglyphs",
    "text": "FAQ: Homoglyphs\n\nx &lt;- c(\"Η\", \"H\")\ntolower(x)\n\n[1] \"η\" \"h\"\n\n\nWhy?\n\n\nuni_info &lt;- Vectorize(function(x) Unicode::u_char_name(utf8ToInt(x)), \"x\")\nuni_info(x)\n\n                         Η                          H \n\"GREEK CAPITAL LETTER ETA\"   \"LATIN CAPITAL LETTER H\" \n\n\n\n\nParticularly nasty with dashes - lean on [[:punct::]] where possible.\n\nx &lt;- c(\"Em Dash —\", \"En Dash –\", \"Hyphen ‐\")\nstringr::str_remove(x, \"[[:punct:]]\") # Works\n\n[1] \"Em Dash \" \"En Dash \" \"Hyphen \" \n\nstringr::str_remove(x, \"-\")  # Keyboard minus = Fail\n\n[1] \"Em Dash —\" \"En Dash –\" \"Hyphen ‐\""
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-symbol-quantifiers",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-symbol-quantifiers",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: ? Symbol (Quantifiers)",
    "text": "FAQ: ? Symbol (Quantifiers)\nQuantifiers (multiple matches):\n\n.{a, b}: anywhere from a to b copies (inclusive)\n.{, b}: no more than b copies\n.{a,}: at least a copies\n.?: zero-or-one, same as .{0,1}\n.*: zero-or-more, same as .{0,}\n.+: one-or-more, same as {1,}"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-stringr-vs-grep-grepl",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-stringr-vs-grep-grepl",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: stringr vs grep / grepl",
    "text": "FAQ: stringr vs grep / grepl\nUltimately the same functionality, but stringr has a more consistent interface.\nConversion table online"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-working-columnwise",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-working-columnwise",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: Working Columnwise",
    "text": "FAQ: Working Columnwise\nAll stringr functions work well in dplyr pipelines (“vectorized”):\n\nlibrary(dplyr); library(stringr)\ndf &lt;- data.frame(lower_letters = letters)\ndf |&gt; mutate(upper_letters = str_to_upper(lower_letters))\n\n   lower_letters upper_letters\n1              a             A\n2              b             B\n3              c             C\n4              d             D\n5              e             E\n6              f             F\n7              g             G\n8              h             H\n9              i             I\n10             j             J\n11             k             K\n12             l             L\n13             m             M\n14             n             N\n15             o             O\n16             p             P\n17             q             Q\n18             r             R\n19             s             S\n20             t             T\n21             u             U\n22             v             V\n23             w             W\n24             x             X\n25             y             Y\n26             z             Z"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#faq-how-to-convert-to-utf-8",
    "href": "archive/AY-2024-FALL/slides/slides12.html#faq-how-to-convert-to-utf-8",
    "title": "STA 9750 - Week 12",
    "section": "FAQ: How to Convert to UTF-8",
    "text": "FAQ: How to Convert to UTF-8\nIf you know the source encoding:\n\ninconv(STRING, from=\"latin1\", to=\"UTF-8\")\n\nIf you don’t know the source, …."
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#agenda",
    "href": "archive/AY-2024-FALL/slides/slides12.html#agenda",
    "title": "STA 9750 - Week 12",
    "section": "Agenda",
    "text": "Agenda\n\nUnicode Discussion\nRegex Discussion\nRegex Exercises\nCompletion of Cocktail Exercise\nTime Permitting: More Scraping\nTime Permitting: Statistical Inference"
  },
  {
    "objectID": "archive/AY-2024-FALL/slides/slides12.html#breakout-rooms",
    "href": "archive/AY-2024-FALL/slides/slides12.html#breakout-rooms",
    "title": "STA 9750 - Week 12",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nOrder\nTeam\n\nOrder\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "archive/AY-2024-FALL/preassignments.html",
    "href": "archive/AY-2024-FALL/preassignments.html",
    "title": "STA 9750 - Pre-Assignments",
    "section": "",
    "text": "In lieu of traditional homework, STA 9750 has weekly pre-assignments designed to achieve several interlocking goals:\n\nProvide initial exposure to that week’s topic before the lecture and lab session\nAllow students with less previous programming experience more time to familiarize themselves with that week’s topic\nAllow students to submit questions to be covered in class\n\nEach Pre-Assignment will be submitted via CUNY Brightspace and due the night before class (Wednesdays at 11:45). These are short assignments, typically only a few questions, so extensions will not be given outside of exceptional circumstances.\n\nPre-Assignments\n\nPre-Assignment for Week #01\nNone.\n\n\nPre-Assignment for Week #02\nDue Dates:\n\nReleased to Students: 2024-08-29\nDue on Brightspace: 2024-09-04 at 11:45pm ET\n\nIn this Pre-Assignment, you will familiarize yourself with the basics of Markdown, an easy way to write and format documents. In class, we will use Markdown based tools to create dynamic data analysis documents seamlessly combining code, text, and graphics.\n\n\nPre-Assignment for Week #03\nDue Dates:\n\nReleased to Students: 2024-09-05\nDue on Brightspace: 2024-09-11 at 11:45pm ET\n\nIn this week’s preassignment, you will familiarize yourself with some basic “calculator math” in R. You will also see how function calls work as we get ready to start some proper R programming.\n\n\nPre-Assignment for Week #04\nDue Dates:\n\nReleased to Students: 2024-09-12\nDue on Brightspace: 2024-09-18 at 11:45pm ET\n\nIn this week’s preassignment, you will review dplyr’s “single-table” verbs. These are functions that take a single data frame and do something, typically returning another data frame. We can divide these into three major groups:\n\nSubsetting rows (filter) and columns (select);\nChanging and creating columns (mutate and less commonly, rename);\noperating with group structure (group_by, summarize)\n\n\n\nPre-Assignment for Week #05\nDue Dates:\n\nReleased to Students: 2024-09-19\nDue on Brightspace: 2024-09-25 at 11:45pm ET\n\nIn this week’s preassignment, you will review dplyr’s most important “multi-table” verbs, the join operators. These are functions that take multiple data frames and combine them together. You will need to use this type of functionality to combine data from different sources together in a principled and organized fashion. You will also learn a bit about the pivot_longer and pivot_wider functions used to change the shape of data frames. These are particularly useful in conjunction with joins: you will often need to reshape two tables to “join” properly (typically, lengthening them with pivot_longer) and then reshape them for downstream presentation (typically with pivot_wider).\n\n\nPre-Assignment for Week #06\nNone.\nThe 2024-10-10 class session will be dedicated to Course Project Proposals.\n\n\nPre-Assignment for Week #07\nDue Dates:\n\nReleased to Students: 2024-10-10\nDue on Brightspace: 2024-10-16 at 11:45pm ET\n\nIn this week’s preassignment, we begin to explore the wonderful world of statistical graphics.\n\n\nPre-Assignment for Week #08\nDue Dates:\n\nReleased to Students: 2024-10-17\nDue on Brightspace: 2024-10-23 at 11:45pm ET\n\nIn this week’s preassignment, we dive deeper into the world of statistical graphics, watching statistical graphics in the hands of a master.\n\n\nPre-Assignment for Week #09\nDue Dates:\n\nReleased to Students: 2024-10-24\nDue on Brightspace: 2024-10-30 at 11:45pm ET\n\nIn this week’s preassignment, we review the basics of reading data files into R.\n\n\nPre-Assignment for Week #10\nNone.\nThe 2024-11-07 class session will be dedicated to Course Project Mid-Semester Check-Ins.\n\n\nPre-Assignment for Week #11\nDue Dates:\n\nReleased to Students: 2024-11-07\nDue on Brightspace: 2024-11-13 at 11:45pm ET\n\nIn this week’s preassignment, students are introduced to the basics of CSS selectors.\n\n\nPre-Assignment for Week #12\nDue Dates:\n\nReleased to Students: 2024-11-14\nDue on Brightspace: 2024-11-20 at 11:45pm ET\n\nTBA\n\n\nPre-Assignment for Week #13\nDue Dates:\n\nReleased to Students: 2024-11-21\nDue on Brightspace: 2024-12-04 at 11:45pm ET\n\nTBA\n\n\nPre-Assignment for Week #14\nNone.\nThe 2024-12-12 class session will be dedicated to Course Project Final Presentations."
  },
  {
    "objectID": "archive/AY-2024-FALL/project.html",
    "href": "archive/AY-2024-FALL/project.html",
    "title": "STA 9750 - Final Project",
    "section": "",
    "text": "In lieu of exams, STA 9750 has an end-of-semester project, worth 40% of your final grade. This project is intended to showcase the data analysis technologies covered in this course, including - but not limited to:\nThe project will be graded out of a total 400 points, divided as follows:\nProjects can be completed in groups of 3-5 students.1 All group members are responsible for all portions of the work and will receive the same grade, except on the individual evaluation.\nGroup Membership: By 2024-10-02 at 11:45pm ET, email the instructor with a list of group members, cc-ing all group members. Once the original email is sent, other group members must reply acknowledging their intention to work with this group. After this date, group membership may only be changed for extraordinary circumstances."
  },
  {
    "objectID": "archive/AY-2024-FALL/project.html#footnotes",
    "href": "archive/AY-2024-FALL/project.html#footnotes",
    "title": "STA 9750 - Final Project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf desired, students can work in pairs or even individually. That “team” is still responsible for a minimum of three specific questions, so you will have to do extra work if you have a team of fewer than 3 people.↩︎\nMore properly, you would want to use Zip Code Tabulation Areas (ZCTAs) for this sort of analysis. The distinction is subtle, but while all ZCTAs have geographic extents, not all zip codes do. For example, there are dedicated zip codes for the IRS and the Department of Defense that have no associated geographic boundaries. Most open data sources will omit this distinction, but if you see it, you should be aware of it.↩︎\nIf students choose to take on multiple specific questions (perhaps because they were in a small group or if a classmate had to drop the course), they may submit multiple individual reports (one per question). If doing so, please modify the GitHub message template to link all reports.↩︎"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "STA 9750 - In-Class Labs",
    "section": "",
    "text": "Most weeks, the Thursday ‘lecture’ for STA 9750 will be dedicated to an in-class “lab”. These ungraded labs are an opportunity to see how the concepts introduced in that week’s pre-assignment are used in practice.\nLabs:\n\nLab #01: Rev your Engines! Setting-Up R and RStudio\nLab #02: Getting Down with Markdown\nLab #03: R, These are your first steps…\nLab #04: Single Table Verbs, Group-Aware Filtering\nLab #05: Let us Join our Tables Together\nLab #07: More Thoughts on Plots\nLab #08: Advanced Plotting - Chloropleths and Cartograms\nLab #09: Data Import in R\nLab #11: Manipulating and Reading HTML in R\nLab #12: Strings and Stringy-Things\nLab #13: Inference and Prediction in R"
  },
  {
    "objectID": "TODO.html",
    "href": "TODO.html",
    "title": "STA 9750",
    "section": "",
    "text": "Course improvements for next STA9750 offering\n\nMini Project #00\n\n\nAdd .gitignore\n\nExclude caching\nExlcude data files\n\nAdd minimal quarto\n\nEnsure students can run Quarto locally\n\nMore git setup\n\nIncrease buffer size to send large figures in MP02 and later\n\n\n\nDisable “Visual” editor mode in RStudio.\nMore Git workflow resources."
  },
  {
    "objectID": "slides/slides08.html#mini-project-02",
    "href": "slides/slides08.html#mini-project-02",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nSubmission due yesterday at 11:45pm\n\nVery creative!"
  },
  {
    "objectID": "slides/slides08.html#the-shark",
    "href": "slides/slides08.html#the-shark",
    "title": "STA 9750 - Week 8",
    "section": "The Shark!",
    "text": "The Shark!"
  },
  {
    "objectID": "slides/slides08.html#mini-project-02-1",
    "href": "slides/slides08.html#mini-project-02-1",
    "title": "STA 9750 - Week 8",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nAdd the following to your .gitignore file:\n/.quarto/\n.Rproj.user\n*/*_cache/*\n*tsv*\n*csv*\n*xlsx*\n*zip\n*gz\n.DS_Store\n.Rhistory\nThis instructs git to ignore matching files - less chance of accidental “super commits.”\n(Possible to override if you necessary)"
  },
  {
    "objectID": "slides/slides08.html#mini-project-02---peer-feedback",
    "href": "slides/slides08.html#mini-project-02---peer-feedback",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "STA 9750 Mini-Project #02 - Peer Feedback",
    "text": "STA 9750 Mini-Project #02 - Peer Feedback\nPeer feedback assigned on GitHub this morning\n\n\\(\\approx 4\\) feedbacks each\nTake this seriously: around 20% of this assignment is “meta-review”\nGoal: rigorous constructive critique\n\n\nSubmissions may not map perfectly to rubric - use your best judgement\n\n\nLearn from this! What can you adapt for MP#03?"
  },
  {
    "objectID": "slides/slides08.html#mini-project-03",
    "href": "slides/slides08.html#mini-project-03",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "STA 9750 Mini-Project #03",
    "text": "STA 9750 Mini-Project #03\nNow online - Due 2025-04-23 at 11:45pm ET\nTopic: Creating the Ultimate Playlist\n\nGitHub post (used for peer feedback) AND Brightspace\nThree Weeks: don’t wait until the very end\nShould be less demanding than MP #01 and MP#02\n\nLots of little files: practice data management\nSome data limitations, particularly after inner_join\n\n\n\nPay attention to the rubric"
  },
  {
    "objectID": "slides/slides08.html#thank-you-1",
    "href": "slides/slides08.html#thank-you-1",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Thank you!",
    "text": "Thank you!\nA personal note, if you allow me:\n\n\nI’m really enjoying this class - thank you all!\n\n\n\n\nYour effort is not unnoticed - I know this class starts “pedal-to-the-metal” but hopefully you’ve seen just how powerful these tools R.\n\n\n\n\nMore than that - I appreciate your good attitude and willingness to share your frustrations and triumphs. Reading comments on PA quiz this week was uplifting."
  },
  {
    "objectID": "slides/slides08.html#continual-improvement",
    "href": "slides/slides08.html#continual-improvement",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Continual Improvement",
    "text": "Continual Improvement\nI’ve set up a TODO file with everything I want to improve for next cohort.\nSuggestions welcome.\n\nEvery semester, I create new mini-projects. Ideas and suggestions very welcome\n\nTopics and data sets are both great"
  },
  {
    "objectID": "slides/slides08.html#upcoming-mini-projects",
    "href": "slides/slides08.html#upcoming-mini-projects",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\n\nMP#04:\n\nDeadline: 2025-05-07 at 11:45pm ET\nTopic: Exploring Recent US Political Shifts"
  },
  {
    "objectID": "slides/slides08.html#course-project",
    "href": "slides/slides08.html#course-project",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Course Project",
    "text": "Course Project\nProject should be your main focus for rest of course\n\nBut you still need to do mini-projects and pre-assignments"
  },
  {
    "objectID": "slides/slides08.html#pre-assignments",
    "href": "slides/slides08.html#pre-assignments",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nThis week: getting data into R\n\nNext pre-assignment is 2025-04-02 at 11:45pm ET\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "slides/slides08.html#course-support",
    "href": "slides/slides08.html#course-support",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 2x / week\n\nMW Office Hours on Monday + Thursday for rest of semester\nNo OH during Spring Break\n\n\nAsynchronous\n\nPiazza (\\(&lt;25\\) minute average response time)"
  },
  {
    "objectID": "slides/slides08.html#upcoming-week",
    "href": "slides/slides08.html#upcoming-week",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nDue Wednesday at 11:45pm:\n\nPre-Assignment #09 (Brightspace)\n\nData Import\n\nMP #02 Peer Feedback on GitHub AND Brightspace\n\n\nNext three weeks:\n\nReading ‘clean data’ into R\nReading and parsing HTML\nParsing messy (text) data\n\n\n\nTeaching Observation by Prof. Brandwein - Next Week"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---aes",
    "href": "slides/slides08.html#faq-ggplot2---aes",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - aes()",
    "text": "FAQ: ggplot2 - aes()\nWhat is the aes function - stands between data and geom_\n\nEach geom_ takes a fixed set of “coordinates”\nEach data set has its own column names\naes ties these together"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---why-do-pie-charts-have-a-bad-reputation",
    "href": "slides/slides08.html#faq-ggplot2---why-do-pie-charts-have-a-bad-reputation",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - Why do Pie Charts have a bad reputation?",
    "text": "FAQ: ggplot2 - Why do Pie Charts have a bad reputation?\n\nUse of area and angle over length: less accurate perception\nDepends on fill to convey category - limited categories\n\n\nBut honestly - “insider smugness” and hate of Excel"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---plot-type-choice",
    "href": "slides/slides08.html#faq-ggplot2---plot-type-choice",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - Plot Type Choice",
    "text": "FAQ: ggplot2 - Plot Type Choice\nFor me:\n\nExploratory mode:\n\nSimple: line, scatter, bar, frequency\n\nPublication mode:\n\nVery context specific"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---font-sizing",
    "href": "slides/slides08.html#faq-ggplot2---font-sizing",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - Font Sizing",
    "text": "FAQ: ggplot2 - Font Sizing\nTheme machinery!"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---overplotting-scatterblobs",
    "href": "slides/slides08.html#faq-ggplot2---overplotting-scatterblobs",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - Overplotting / ScatterBlobs",
    "text": "FAQ: ggplot2 - Overplotting / ScatterBlobs\nStudent asked about “scatterblobs” - typo(?) but I love it!\n\n\nDensity based plotting: hexbins, histograms, rugplots\nData reduction: summarization or sub-sampling"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---optimizing-performance",
    "href": "slides/slides08.html#faq-ggplot2---optimizing-performance",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - Optimizing Performance",
    "text": "FAQ: ggplot2 - Optimizing Performance\nActive project of ggplot2 team - not much you can do\nPractical advice: plot less (see previous slide)"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---beyond-scatter-and-line",
    "href": "slides/slides08.html#faq-ggplot2---beyond-scatter-and-line",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - Beyond Scatter and Line",
    "text": "FAQ: ggplot2 - Beyond Scatter and Line\nSome favorite semi-advanced plot types:\n\nViolin plots: combination of boxplot and histogram\nRidgelines\nBeeswarms\n\nDeep rabbit hole"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---geospatial-visualizations",
    "href": "slides/slides08.html#faq-ggplot2---geospatial-visualizations",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - Geospatial Visualizations",
    "text": "FAQ: ggplot2 - Geospatial Visualizations\nThat’s our goal for today!"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---high-dimensional-data",
    "href": "slides/slides08.html#faq-ggplot2---high-dimensional-data",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - High-Dimensional Data",
    "text": "FAQ: ggplot2 - High-Dimensional Data\nHigh-dimensional data: measure many variables per observation (“wide”)\nHigh-dimensional data is hard to visualize\n\nApproaches:\n\nPair plots for “moderate” HDD\nPCA (or similar dimension reduction. Take 9890!)"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---creating-a-custom-theme",
    "href": "slides/slides08.html#faq-ggplot2---creating-a-custom-theme",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - Creating a Custom Theme",
    "text": "FAQ: ggplot2 - Creating a Custom Theme\n\nAdvanced:\n\ntheme_set() - change ggplot2 defaults\n.Rprofile - set code to run every time you start R"
  },
  {
    "objectID": "slides/slides08.html#faq-ggplot2---when-not-to-use",
    "href": "slides/slides08.html#faq-ggplot2---when-not-to-use",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: ggplot2 - When Not to Use",
    "text": "FAQ: ggplot2 - When Not to Use\nggplot2 is designed to make good statistical graphics. Sub-par for:\n\nAdvanced interactivity\nReally big data\nHardcore customization / “infographics”"
  },
  {
    "objectID": "slides/slides08.html#faq-git-wtf",
    "href": "slides/slides08.html#faq-git-wtf",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "FAQ: git WTF",
    "text": "FAQ: git WTF\nReference: Happy Git with R"
  },
  {
    "objectID": "slides/slides08.html#warm-up",
    "href": "slides/slides08.html#warm-up",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Warm-Up",
    "text": "Warm-Up\n“Datasaurus Dozen”:\n\ninstall.packages(\"datasauRus\") (Note capital R)\nlibrary(datasauRus); data(datasaurus_dozen)\n\nCreate an animated (gganimate) plot:\n\n\\(x, y\\) scatterplot\nAnimate different values of dataset\n\nIf you are having trouble with gganimate, facet instead."
  },
  {
    "objectID": "slides/slides08.html#warm-up-1",
    "href": "slides/slides08.html#warm-up-1",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Warm-Up",
    "text": "Warm-Up"
  },
  {
    "objectID": "slides/slides08.html#diving-deeper-with-ggplot2",
    "href": "slides/slides08.html#diving-deeper-with-ggplot2",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Diving Deeper with ggplot2",
    "text": "Diving Deeper with ggplot2\nToday: maps!\nInstall the sf package: Simple Features for Spatial Data\nExercise: Lab #08"
  },
  {
    "objectID": "slides/slides08.html#breakout-rooms",
    "href": "slides/slides08.html#breakout-rooms",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nRoom\nTeam\n\nRoom\nTeam\n\n\n\n\n1\nTeam Mystic\n\n5\nMoney Team + CWo.\n\n\n2\nSubway Metrics\n\n6\nLit Group\n\n\n3\nNoise Busters\n\n7\nCinephiles + VG\n\n\n4\nAI Imp. Coll."
  },
  {
    "objectID": "slides/slides02.html#week-2-update",
    "href": "slides/slides02.html#week-2-update",
    "title": "STA 9750 - Week 2 Update",
    "section": "STA 9750 Week 2 Update",
    "text": "STA 9750 Week 2 Update\n\nWeekly feature\nBrief updates and reminders about course logistics\nSyllabus and Brightspace are binding\n\nIf something is left out of here, it still happens!"
  },
  {
    "objectID": "slides/slides02.html#course-administration-google-calendar-help",
    "href": "slides/slides02.html#course-administration-google-calendar-help",
    "title": "STA 9750 - Week 2 Update",
    "section": "Course Administration: Google Calendar Help",
    "text": "Course Administration: Google Calendar Help\nThanks to WP (Piazza #15) for delving into Google Calendar formatting\nI’ve updated the course homepage to provide a CSV file with all course deadlines suitable for import to Google Calendar."
  },
  {
    "objectID": "slides/slides02.html#course-administration-course-project-description-released",
    "href": "slides/slides02.html#course-administration-course-project-description-released",
    "title": "STA 9750 - Week 2 Update",
    "section": "Course Administration: Course Project Description Released",
    "text": "Course Administration: Course Project Description Released\nCourse project description is now online\nDetailed discussion of:\n\nProject structure\nKey deadlines\nGrading rubrics\n\nFirst step: by October 2nd, email me your group members."
  },
  {
    "objectID": "slides/slides02.html#course-enrollment",
    "href": "slides/slides02.html#course-enrollment",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Course Enrollment",
    "text": "Course Enrollment\nFinal enrollment: 28\n\n\\(\\approx 8\\) final project teams (3-4 each)\nApprox 4 MPs to review per peer-feedback cycle"
  },
  {
    "objectID": "slides/slides02.html#graduate-teaching-assistant-gta",
    "href": "slides/slides02.html#graduate-teaching-assistant-gta",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Graduate Teaching Assistant (GTA)",
    "text": "Graduate Teaching Assistant (GTA)\nNo GTA this semester."
  },
  {
    "objectID": "slides/slides02.html#piazza",
    "href": "slides/slides02.html#piazza",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Piazza",
    "text": "Piazza\n\n11 sign-ups: 17 still need to sign up\nThank you for those of you who already posted questions!\nPost #05 - Search for Teammates\n\n\nInstructor Tip: Before committing a team with someone, you can look up their GitHub and see how they did on MP#00 and MP#01. This might be helpful to find teammates whose standards are calibrated to your own.\n\nSpecial thanks to JLL for finding and reporting issues with MP#00 instructions"
  },
  {
    "objectID": "slides/slides02.html#pre-assignments",
    "href": "slides/slides02.html#pre-assignments",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nPre-Assignment 02:\n\n20 / 28 submitted\nIgnore Brightspace’s Grading\n\nBrightspace automatically marks all short answers as wrong\nLook at “Grades” tab to see actual sub-totals (12.5 / 100 = 1/8 = full credit on 1 of 8 graded PAs)\n\nI will often give feedback through Brightspace, so make sure to go back and see if I’ve left you any comments. (I might not give comments on every question though.)\n\n. . .\nPre-Assignment 03:\n\nBefore midnight nextweek :\nAvailable on course website + Brightspace after 9pm"
  },
  {
    "objectID": "slides/slides02.html#mini-project-00",
    "href": "slides/slides02.html#mini-project-00",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Mini-Project #00",
    "text": "Mini-Project #00\nMini-Project #00\n\nDue in (slightly less than) a week\nPossible tech issues, so start early\n1 GitHub tag + 0 Piazza Messages so far\n\nVerification of Enrollment - Required to stay enrolled in class"
  },
  {
    "objectID": "slides/slides02.html#course-bot",
    "href": "slides/slides02.html#course-bot",
    "title": "STA 9750 - Week 2 Update",
    "section": "Course Bot",
    "text": "Course Bot\nBecause this course is a double, I have resources to create a “bot” to help with course organization on GitHub.\nBot will begin to acknowledge completed MP#00 over the weekend.\nCurrent name: CISSOID: CIS and Statistics bot for Organizing Instructional Delivery\n\nParticular mathematical shape\nName means “Ivy-like”:\n\nUse technology to overcome resource limitations of CUNY\n\n\nBetter name suggestions very welcome!"
  },
  {
    "objectID": "slides/slides02.html#today",
    "href": "slides/slides02.html#today",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Today",
    "text": "Today\n\nReview of Questions from Pre-Assign #02\nIntroduction to Markdown and Quarto\nIntroduction to GitHub pages\nHow to ask for help\nInteractive Use of R and RStudio"
  },
  {
    "objectID": "slides/slides02.html#q1",
    "href": "slides/slides02.html#q1",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Q1",
    "text": "Q1\n\nWhat is Markdown?\n\nPer Wikipedia: “Markdown is a light-weight, plain-text, markup language specification”\n\n\nLight-weight: relatively simple, focus on content than formatting\nPlain-text: accessible using almost any text editor (RStudio, GitHub, VS Code, etc)\n\nNot locked into specific software (e.g., MS Word)\nEasily incorporated into a variety of technologies"
  },
  {
    "objectID": "slides/slides02.html#q1-1",
    "href": "slides/slides02.html#q1-1",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Q1",
    "text": "Q1\n\nWhat is Markdown?\n\n\n\nMarkup language: a ‘mini-coding language’ for text documents\n\nOther famous examples: HTML, XML\n\nSpecification:\n\nCommonMark defines ‘standard’ Markdown\nSome software allows extensions\nPandoc often powers under the hood"
  },
  {
    "objectID": "slides/slides02.html#q2",
    "href": "slides/slides02.html#q2",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Q2",
    "text": "Q2\n\nOther than text formatting, does Markdown ha[ve] any other us[]es?\n\nOn its own, Markdown is just text formatting (but that’s a lot!)\n. . .\nWe will use Quarto which augments markdown for reproducible research. We can embed code-and its output-inside Markdown documents."
  },
  {
    "objectID": "slides/slides02.html#q3",
    "href": "slides/slides02.html#q3",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Q3",
    "text": "Q3\n\n[W]hat documents use[] Markdown?\n\nSo much! Markdown is used by Bitbucket, GitHub, OpenStreetMap, Reddit, Stack Exchange, Drupal, ChatGPT, Discord, MS Teams and many more!\n. . .\nWith tools like Pandoc/Quarto, Markdown can be rendered to:\n\n\n\nHTML\nPDF\nWeb Slides\nEBooks\n\n\n\nResearch Papers\nWord Documents\nPowerPoint slides\nand so much more!"
  },
  {
    "objectID": "slides/slides02.html#q4",
    "href": "slides/slides02.html#q4",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Q4",
    "text": "Q4\n\n[What is] the difference between [a] Code section and [a] Nested List[? A]re they just different ways of indenting?\n\nNo. Nested lists are ‘just’ text\nCode formatting enables much more if rendering engine supports it:\n. . .\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "slides/slides02.html#q5",
    "href": "slides/slides02.html#q5",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Q5",
    "text": "Q5\n\n[H]ow are we going to use Markdown?\n\nAll written work (mini-projects and final project) in this course will be submitted using Markdown (by way of Quarto).\n. . .\nSpecifically:\n\nSubmission pages for 5 mini-projects\nIndividual reports for course project\nSummary (team) report for final project\n\nYou are also encouraged (but not required) to use Markdown for presentation slides (like these!)"
  },
  {
    "objectID": "slides/slides02.html#q6",
    "href": "slides/slides02.html#q6",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Q6",
    "text": "Q6\n\nHow can I create Tables in Markdown?\n\nMarkdown has two table syntaxes:\n\nan easy one with minimal control\na hard one which allows fine grained control (alignment, column widths, etc.) - “pipe tables”\n\nIf you are making complex tables, I recommend using the list-table extension.\n(See syllabus.qmd in course repo for examples.)"
  },
  {
    "objectID": "slides/slides02.html#q7",
    "href": "slides/slides02.html#q7",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Q7",
    "text": "Q7\n\nHow to create images and links?\n\nBasic hyperlinks look like this:\n[link text](https://the.url/goes/here)\n. . .\nIf you want to embed the contents of a link, prepend it with an exclamation point. This is most useful for images:\n![Image Caption](https://the.url/goes/here.png)\n. . .\nYou can even put a link inside an image to be fancy:\n[![Elephant](elephant.png)](https://en.wikipedia.org/wiki/Elephant)"
  },
  {
    "objectID": "slides/slides02.html#q7-1",
    "href": "slides/slides02.html#q7-1",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Q7",
    "text": "Q7\n\nHow to create images and links?\n\nQuarto automatically embeds the results of plotting code:\n\nplot(1:5, main=\"Behold, a Plot!\", col=2:6, cex=5, \n     pch=16, xlab=\"\", cex.main=5)\n\n\n\n\n\n\n\n\nHere, Quarto handles all the file creation and link targeting for us. If I change the code, the figure will change automatically."
  },
  {
    "objectID": "slides/slides02.html#course-project",
    "href": "slides/slides02.html#course-project",
    "title": "STA 9750 - Week 2 Update",
    "section": "Course Project",
    "text": "Course Project\n\nTeams: 3-5 classmates (either section)\nStages:\n\nProposal (in class presentation)\nMid-semester check-in (in class presentation)\nFinal: in class presentation, individual report, summary report\n\nStructure:\n\nShared “Overarching Question”\nIndividual “Specific Question”\n\n\nFull description online"
  },
  {
    "objectID": "slides/slides02.html#finding-data",
    "href": "slides/slides02.html#finding-data",
    "title": "STA 9750 - Week 2 Update",
    "section": "Finding Data",
    "text": "Finding Data\n\nStart early!\nNYC Open Data is great\n\nSee also: FRED, Federal Open Data, Nasa EarthData, Kaggle\nAsk on Piazza for pointers\nLots of data hidden in Wikipedia\n\nNothing paid or private without express instructor submission\nEveryone loves spatial data!"
  },
  {
    "objectID": "slides/slides02.html#presentation-hints",
    "href": "slides/slides02.html#presentation-hints",
    "title": "STA 9750 - Week 2 Update",
    "section": "Presentation Hints",
    "text": "Presentation Hints\n\nLongest time \\(\\neq\\) most important\nStory, story, story! Why are you making these choices?\nHourglass Structure\n\nStart big\nMotivate your overarching question\nSpecific questions\nTie specific to overarching\nFrom overarching back to big motivation\n\nNo less than one figure every other slide"
  },
  {
    "objectID": "slides/slides02.html#markdown-and-quarto-1",
    "href": "slides/slides02.html#markdown-and-quarto-1",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Markdown and Quarto",
    "text": "Markdown and Quarto\n\nQuarto implements Markdown with data-analytic extensions\nSeamless (ideally!) integration of code and text\nNo more copy and paste\n\nQuarto user guide is fantastic!\nSee also source for course materials."
  },
  {
    "objectID": "slides/slides02.html#lab-activity-part-0",
    "href": "slides/slides02.html#lab-activity-part-0",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Lab Activity: Part 0",
    "text": "Lab Activity: Part 0\nIf you haven’t already, install Quarto."
  },
  {
    "objectID": "slides/slides02.html#lab-activity-part-1",
    "href": "slides/slides02.html#lab-activity-part-1",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Lab Activity: Part 1",
    "text": "Lab Activity: Part 1\nCreate a simple PDF quarto document using the RStudio wizard.\n(Note that you may need to install tinytex for this to work properly, but Quarto should install it for you automatically.)"
  },
  {
    "objectID": "slides/slides02.html#lab-activity-part-2",
    "href": "slides/slides02.html#lab-activity-part-2",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Lab Activity: Part 2",
    "text": "Lab Activity: Part 2\nCreate a 5 slide presentation showing the Houston housing market. This should include:\n\nA title slide\nThree body slides with a figure and some text\nA conclusion slide\n\nYou may use the following code snippets:\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\ntxhousing |&gt; filter(city==\"Houston\") |&gt; \n             group_by(year) |&gt; \n             summarize(sales=sum(sales)) |&gt; \n             ggplot(aes(x=year, y=sales)) + \n                geom_line() + \n                ggtitle(\"Annual Houses Sold in Houston, TX\")\nRecall that this code needs to be between three backticks on each end (and start with r in curly braces as well.)\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\ntxhousing |&gt; filter(city==\"Houston\") |&gt; \n    group_by(month) |&gt; \n    summarize(avg_price=sum(volume) / sum(sales)) |&gt; \n    mutate(month=factor(month.abb[month], \n                 levels=month.abb, ordered=TRUE)) |&gt;\n    ggplot(aes(x=month, y=avg_price)) + \n    geom_bar(stat=\"identity\") + \n    ggtitle(\"Average Price of Houses Sold in Texas by Month\") + \n    xlab(\"Month\") + \n    ylab(\"Average Sale Price\") + \n    scale_y_continuous(labels = scales::dollar)\nRecall that this code needs to be between three backticks on each end (and start with r in curly braces as well.)\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\ntxhousing |&gt; filter(year==2015) |&gt; \n    group_by(city) |&gt; \n    summarize(avg_price=sum(volume) / sum(sales),\n              num_sales=sum(sales)) |&gt; \n    slice_max(num_sales, n=10) |&gt;\n    ggplot(aes(x=city, y=avg_price)) + \n    geom_bar(stat=\"identity\") + \n    ggtitle(\"Average Price of Houses Sold in 2015 by City in Texas\") + \n    xlab(\"City\") + \n    ylab(\"Average Sale Price\") + \n    scale_y_continuous(labels = scales::dollar)\nRecall that this code needs to be between three backticks on each end (and start with r in curly braces as well.)"
  },
  {
    "objectID": "slides/slides02.html#lab-activity-part-3",
    "href": "slides/slides02.html#lab-activity-part-3",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Lab Activity: Part 3",
    "text": "Lab Activity: Part 3\nView the Quarto Demo Slides and add one new element to your slides from the previous section."
  },
  {
    "objectID": "slides/slides02.html#github-pages-1",
    "href": "slides/slides02.html#github-pages-1",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "GitHub Pages",
    "text": "GitHub Pages\nIn-class discussion of what a static web page is and the role of GitHub Pages as a static web server."
  },
  {
    "objectID": "slides/slides02.html#how-to-ask-for-help-1",
    "href": "slides/slides02.html#how-to-ask-for-help-1",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "How to Ask for Help",
    "text": "How to Ask for Help\nProfessional programming is at least half looking things up; at beginning stages, the fraction is even higher.\nSo it’s important to know how to see help the smart way:\n\nOfficial documentation. Free software almost never becomes famous without great documentation: R and its packages are no exception. Everything we will use in this class has solid documentation.\n\n\nTidyverse.org\n\n\n\nSearch Engine.\n\nMost programming challenges have been faced by somebody before, so Google it!\nTips:\n\nInclude R or rstats in your search query\nIt’s better to search what you want to do rather than how you think you should do it.\nSearch programming Q&A sites like StackOverflow for specific code questions; blogs and course materials are better for “big picture” questions\n\n\n\nAsk on a Forum with a Reproducible Example\n\nProgramming fora, like StackOverflow, are full of great resources. Most of what you need is already there. But if you need to ask a new question, make sure to create a minimal reproducible example\nMake it easy for your helper to help you.\n\nMinimal: narrow down to as few lines of code as possible\nReproducible: self-contained without dependencies on libraries (if can be avoided); load all packages needed; use standard data\n\nPro-Tip: You’ll solve over 50% of your problems in trying to create an MRE.\n\nTips:\n\nShow the code, even if it doesn’t work\nSend code as text, not screenshot (so your helper can run it)\nSmaller examples help narrow down problems\nAvoid IO (file input and output) unless specifically relevant to problem\nRemove everything you can\n\nThe reprex R package helps with this: see this talk.\nFor this class, rely on Piazza!"
  },
  {
    "objectID": "slides/slides14.html#mini-projects",
    "href": "slides/slides14.html#mini-projects",
    "title": "STA 9750 - Week 14",
    "section": "STA 9750 Mini-Projects",
    "text": "STA 9750 Mini-Projects\n\nCongratulations! Done with all Mini-Projects!\n\nFantastic work this semester!"
  },
  {
    "objectID": "slides/slides14.html#week-13-pre-assignment",
    "href": "slides/slides14.html#week-13-pre-assignment",
    "title": "STA 9750 - Week 14",
    "section": "Week 13 Pre-Assignment",
    "text": "Week 13 Pre-Assignment\n\nReflection on course:\n\nHow far have you come?\nWhat have you learned?\nWhat was helpful? What was unhelpful?\n\n\nThank you for your comments!"
  },
  {
    "objectID": "slides/slides14.html#grading",
    "href": "slides/slides14.html#grading",
    "title": "STA 9750 - Week 14",
    "section": "Grading",
    "text": "Grading\nReturned:\n\nMid-Term Check-In Feedback\nMP#03 Grades\n\nWe owe you:\n\nMP#04 Grades"
  },
  {
    "objectID": "slides/slides14.html#final-report---group",
    "href": "slides/slides14.html#final-report---group",
    "title": "STA 9750 - Week 14",
    "section": "Final Report - Group",
    "text": "Final Report - Group\nNon-Technical Presentation - Think of yourself as a “consultant” asked by a client to investigate a topic."
  },
  {
    "objectID": "slides/slides14.html#final-report---individual",
    "href": "slides/slides14.html#final-report---individual",
    "title": "STA 9750 - Week 14",
    "section": "Final Report - Individual",
    "text": "Final Report - Individual\nTechnical Appendix to Group Report. Still requires writing, context, etc. but this is in particular where I’m going to look at your code."
  },
  {
    "objectID": "slides/slides14.html#final-project-reports",
    "href": "slides/slides14.html#final-project-reports",
    "title": "STA 9750 - Week 14",
    "section": "Final Project Reports",
    "text": "Final Project Reports\nGroup and Individual Reports\n\nSubmitted via GitHub and Brightspace\n\nDeadline extended to the day of the ‘final’, December 19th\nNo late work accepted (I have to submit grades!)"
  },
  {
    "objectID": "slides/slides14.html#peer-assessment",
    "href": "slides/slides14.html#peer-assessment",
    "title": "STA 9750 - Week 14",
    "section": "Peer Assessment",
    "text": "Peer Assessment\nOn Brightspace, I have opened an additional quiz for peer evaluation of your teammates.\n\n8 questions: scale of 1 (bad) to 3 (great)\n\nPlease submit a copy for each of your teammates.\n\nBrightspace set to allow multiple submissions.\nDue on same day as reports\n\n\nIf you don’t submit these, you will receive a 0 for your peer evaluations\nNo late work accepted (I have to submit grades!)"
  },
  {
    "objectID": "slides/slides14.html#final-project-grading",
    "href": "slides/slides14.html#final-project-grading",
    "title": "STA 9750 - Week 14",
    "section": "Final Project Grading",
    "text": "Final Project Grading\nRubric is set high to give me flexibility to reward teams that take on big challenges\nHard rubric =&gt; Grades are curved generously\n\nMultiple paths to success\nIf your problem is “easy” on an element (data import in particular), that’s great! Don’t spend the effort over-complicating things. Effort is better spent elsewhere"
  },
  {
    "objectID": "slides/slides14.html#instructors-reflection",
    "href": "slides/slides14.html#instructors-reflection",
    "title": "STA 9750 - Week 14",
    "section": "Instructor’s Reflection",
    "text": "Instructor’s Reflection"
  },
  {
    "objectID": "slides/slides14.html#baruch-course-evaluations",
    "href": "slides/slides14.html#baruch-course-evaluations",
    "title": "STA 9750 - Week 14",
    "section": "Baruch Course Evaluations",
    "text": "Baruch Course Evaluations\nShort Break to complete course evaluations\nhttp://baruch.cuny.edu/EVALS\nRoughly:\n\nScores used by central administration\nComments used by faculty and departments to improve offerings"
  },
  {
    "objectID": "slides/slides14.html#hall-of-fame",
    "href": "slides/slides14.html#hall-of-fame",
    "title": "STA 9750 - Week 14",
    "section": "Hall of Fame",
    "text": "Hall of Fame\nSubmit Hall of Fame Nominations"
  },
  {
    "objectID": "slides/slides14.html#today",
    "href": "slides/slides14.html#today",
    "title": "STA 9750 - Week 14",
    "section": "Today",
    "text": "Today\nDecember 12 - Last Day of Class!\n\nFinal Project Presentations"
  },
  {
    "objectID": "slides/slides14.html#presentation-order",
    "href": "slides/slides14.html#presentation-order",
    "title": "STA 9750 - Week 14",
    "section": "Presentation Order",
    "text": "Presentation Order\n\n\n\nOrder\nTeam\n\nOrder\nTeam\n\n\n\n\n10\nRat Pack\n\n7\nCa$h VZ\n\n\n8\nSubway Surfers\n\n6\nListing Legends\n\n\n2\nChart Toppers\n\n9\nTDSSG\n\n\n5\nMetro Mindset\n\n3\nBroker T’s\n\n\n1\nApple Watch\n\n4\nEVengers"
  },
  {
    "objectID": "slides/slides04.html#mini-project-00",
    "href": "slides/slides04.html#mini-project-00",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "STA 9750 Mini-Project #00",
    "text": "STA 9750 Mini-Project #00\nThank you to those of you who provided peer feedback!\n\nA few of you still haven’t completed MP#00.\nToo late for peer feedback, but you need to get this done in order to submit MP#01.\nNo late work accepted on graded MPs."
  },
  {
    "objectID": "slides/slides04.html#mini-project-01",
    "href": "slides/slides04.html#mini-project-01",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\nMP#01 released - Welcome to the Commission to Analyze Taxpayer Spending (CATS)\n\nDue 2025-03-05 at 11:45pm ET\n\nGitHub post (used for peer feedback) AND Brightspace\nSignificant penalties for only submitting one\n\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness, not necessarily correctness"
  },
  {
    "objectID": "slides/slides04.html#mp01-corrections",
    "href": "slides/slides04.html#mp01-corrections",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "MP#01 Corrections",
    "text": "MP#01 Corrections\nThanks to EL and JA for finding two mistakes in MP statement:\n\nApples/Oranges problem in “longest average trip” (JA)\nData cleaning problem in FARES table (EL)\n\nWill fix after class:\n\nSkip longest average trip question\nBetter instructor-provided code for FARES table"
  },
  {
    "objectID": "slides/slides04.html#upcoming-mini-projects",
    "href": "slides/slides04.html#upcoming-mini-projects",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nMP#02 assigned next week:\n\nIdentifying Environmentally Responsible US Public Transit Systems due at 2025-03-26 at 11:45pm ET\nWith revised MP #01 deadline, MP #02 released before MP #01 due\n\nLater:\n\nMP#03 due at 2025-04-23 at 11:45pm ET\nMP#04 due at 2025-05-07 at 11:45pm ET"
  },
  {
    "objectID": "slides/slides04.html#pre-assignments",
    "href": "slides/slides04.html#pre-assignments",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45pm\n\nReading, typically on course website\nBrightspace auto-grades.\n\nI have to manually change to completion grading."
  },
  {
    "objectID": "slides/slides04.html#course-project",
    "href": "slides/slides04.html#course-project",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Course Project",
    "text": "Course Project\nRoster due at 2025-03-05 at 11:45pm ET by email to me.\nAll teammates need to agree, so takes a bit of time.\nOnce you set a team, start thinking about a team name!"
  },
  {
    "objectID": "slides/slides04.html#graduate-teaching-assistant-gta",
    "href": "slides/slides04.html#graduate-teaching-assistant-gta",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Graduate Teaching Assistant (GTA)",
    "text": "Graduate Teaching Assistant (GTA)\n\nCharles Ramirez\nTwice Weekly Office Hours (Zoom - Links of Brightspace)\n\nTuesdays 4-5pm\nFridays 12-1pm\n\nWill also help coordinate peer feedback (GitHub), Piazza responses, etc.\nExcellent resource for course project advice!"
  },
  {
    "objectID": "slides/slides04.html#upcoming-week",
    "href": "slides/slides04.html#upcoming-week",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nNext Wednesday at 11:45pm:\n\nNext Pre-Assignment\nMP#01 Initial Submission due"
  },
  {
    "objectID": "slides/slides04.html#faq-select-",
    "href": "slides/slides04.html#faq-select-",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: select(-)",
    "text": "FAQ: select(-)\ndata |&gt; select(colname) keeps colname, dropping everything else\ndata |&gt; select(-colname) drops colname, keeping everything else\nDropping is mainly useful for\n\nPresentation (removing unwanted columns)\nAdvanced:\n\nOperations across columns"
  },
  {
    "objectID": "slides/slides04.html#faq-filter-vs-group_by",
    "href": "slides/slides04.html#faq-filter-vs-group_by",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: filter vs group_by",
    "text": "FAQ: filter vs group_by\ngroup_by is an adverb. On its own, it does nothing; it changes the behavior of later functionality.\n\npenguins |&gt; drop_na() |&gt; print(n=2)\n\n# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n# ℹ 331 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\npenguins |&gt; drop_na() |&gt; group_by(species) |&gt; print(n=2)\n\n# A tibble: 333 × 8\n# Groups:   species [3]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n# ℹ 331 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "slides/slides04.html#faq-order-of-group_by",
    "href": "slides/slides04.html#faq-order-of-group_by",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: Order of group_by",
    "text": "FAQ: Order of group_by\n\nNo change to first “grouped” operations\nChange in grouping structure of result\nLast group “removed” by summarize\nNo impact on grouped operations performed by mutate or filter"
  },
  {
    "objectID": "slides/slides04.html#faq-ungroup",
    "href": "slides/slides04.html#faq-ungroup",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: ungroup",
    "text": "FAQ: ungroup\n\nRemove all grouping structure\nDefensive to keep group structure from “propogating” unwantedly\n\n\nsum_penguins &lt;- penguins |&gt; \n    group_by(sex, species) |&gt; \n    summarize(mbmg = mean(body_mass_g))\n\n... # Lots of code \n\nsum_penguins |&gt; filter(mbmg == max(mbmg)) # Still grouped!!"
  },
  {
    "objectID": "slides/slides04.html#faq-named-arguments-in-mutate-and-summarize",
    "href": "slides/slides04.html#faq-named-arguments-in-mutate-and-summarize",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: Named Arguments in mutate and summarize",
    "text": "FAQ: Named Arguments in mutate and summarize\nmutate and summarize create new columns:\n\nmutate creates “one-to-one”\nsummarize creates “one-per-group”"
  },
  {
    "objectID": "slides/slides04.html#faq-pipe-syntax",
    "href": "slides/slides04.html#faq-pipe-syntax",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: Pipe Syntax",
    "text": "FAQ: Pipe Syntax\nPipe syntax (|&gt;) is “syntactic sugar”\nJust makes code easier to read:\n\npenguins |&gt; group_by(species) |&gt; summarize(n_species = n())\n# vs\nsummarize(group_by(penguins, species), n_species=n())\n\nExactly the same execution: improved UX\n\n%&gt;% is an older way of doing essentially the same thing"
  },
  {
    "objectID": "slides/slides04.html#faq-assignment-of-pipeline-results",
    "href": "slides/slides04.html#faq-assignment-of-pipeline-results",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: Assignment of Pipeline Results",
    "text": "FAQ: Assignment of Pipeline Results\nWhen to start a pipeline with NAME &lt;-? Creating a new variable:\n\n\nData you intend to reuse\nAssignment operator ‘up front’ indicates important\nMy rules of thumb for names:\n\nNew names for “new complete thoughts” - whole summary in one pipeline\nOverwrite existing names for “like-for-like improvements” (USAGE &lt;- USAGE |&gt; code(...))\n\nRecoding variable names, fixing typos, etc.\nUse name repeatedly so downstream code picks up effects ‘for free’"
  },
  {
    "objectID": "slides/slides04.html#faq-comparison-with-sql-and-pandas-python",
    "href": "slides/slides04.html#faq-comparison-with-sql-and-pandas-python",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: Comparison with SQL and Pandas (Python)",
    "text": "FAQ: Comparison with SQL and Pandas (Python)\ndplyr is heavily inspired by SQL (standard query language for data bases)\n\nMW (2014): “Why bother? Can’t folks just use SQL”\n\n\npandas (in Python) inspired by R data.frame and SQL:\n\nA bit older than dplyr (cousins?)\n“New hotness” (polars) directly inspired by dplyr"
  },
  {
    "objectID": "slides/slides04.html#faq-performance",
    "href": "slides/slides04.html#faq-performance",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: Performance",
    "text": "FAQ: Performance\ndplyr is fast, but advanced options:\n\ndbplyr: translates dplyr syntax to SQL and executes in DB\ndtplyr: uses alternate data.table back-end (HFT)\n\nHard to have bad performance in single-table analysis\n\nDanger of accidentally creating ‘extra’ data in multi-table context\nWill discuss more next week"
  },
  {
    "objectID": "slides/slides04.html#diving-deeper-with-group_by-filter-and-summarize",
    "href": "slides/slides04.html#diving-deeper-with-group_by-filter-and-summarize",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Diving Deeper with group_by, filter, and summarize",
    "text": "Diving Deeper with group_by, filter, and summarize\nData Set: nycflights13\nExercise: Lab #04"
  },
  {
    "objectID": "slides/slides11.html#mini-project-03",
    "href": "slides/slides11.html#mini-project-03",
    "title": "STA 9750 - Week 11",
    "section": "STA 9750 Mini-Project #03",
    "text": "STA 9750 Mini-Project #03\nI spot-checked several results - look fantastic!\n\nFewer git questions - folks getting the hang of things!\nPeer feedback assigned via GitHub\n\nYou give comments on other students’ repos\nDue in 1 week\n\n\nPay attention to the rubric"
  },
  {
    "objectID": "slides/slides11.html#mini-project-04",
    "href": "slides/slides11.html#mini-project-04",
    "title": "STA 9750 - Week 11",
    "section": "STA 9750 Mini-Project #04",
    "text": "STA 9750 Mini-Project #04\nMP#04 released today\n\nDue 2024-12-04 (\\(\\approx\\) 3 weeks)\nTopic: financial modeling\n\nComparison of two retirement plans\nHistorical data + Monte Carlo (“bootstrapping”)\n\nFormat:\n\nDecision Analytics - Play the role of financial advisor\nGitHub post AND Brightspace submission"
  },
  {
    "objectID": "slides/slides11.html#week-11-pre-assignment",
    "href": "slides/slides11.html#week-11-pre-assignment",
    "title": "STA 9750 - Week 11",
    "section": "Week 11 Pre-Assignment",
    "text": "Week 11 Pre-Assignment\nDue at midnight tonight - take a moment to do it now if you haven’t already!"
  },
  {
    "objectID": "slides/slides11.html#pre-assignments",
    "href": "slides/slides11.html#pre-assignments",
    "title": "STA 9750 - Week 11",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is November 20th\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "slides/slides11.html#grading",
    "href": "slides/slides11.html#grading",
    "title": "STA 9750 - Week 11",
    "section": "Grading",
    "text": "Grading\nWe owe you:\n\nMP#02 final average (Need to upload to BS)\nMP#02 peer meta-review (Need to upload to BS)\nPosting videos to Vocat (Need to do video splitting)\nMid-Term Check-In Feedback"
  },
  {
    "objectID": "slides/slides11.html#course-support",
    "href": "slides/slides11.html#course-support",
    "title": "STA 9750 - Week 11",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(38\\) minute average response time)\n\n\nChange: MW Thursday Zoom OH now 4:00pm to 5:00pm"
  },
  {
    "objectID": "slides/slides11.html#upcoming",
    "href": "slides/slides11.html#upcoming",
    "title": "STA 9750 - Week 11",
    "section": "Upcoming",
    "text": "Upcoming\nNov 20:\n\nMP#03 Peer Feedback\nPre Assignment\n\nNov 27 - Thanksgiving Holiday (No Class on Nov 28)\n\nCheck-In Peer Feedback (Vocat)"
  },
  {
    "objectID": "slides/slides11.html#faq-html-vs-css",
    "href": "slides/slides11.html#faq-html-vs-css",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: HTML vs CSS",
    "text": "FAQ: HTML vs CSS\n\nWhat is the difference between HTML and CSS?\n\nHTML is substance; CSS is style\nDistinction can be a bit blurry & CSS can live “inside” HTML\nExample"
  },
  {
    "objectID": "slides/slides11.html#faq-a-in-selectorgadget",
    "href": "slides/slides11.html#faq-a-in-selectorgadget",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: a in SelectorGadget",
    "text": "FAQ: a in SelectorGadget\n\nWhy does [SelectorGadget] display “a” in the selector when selecting a web link?\n\na is for anchor.\nConfusingly, anchors are both links and destinations.\nAnchors can reference:\n\nAnother page (http://URL)\nA particular part of another page (http://URL#place)\nA particular part of the same page (#place)\n\nQuarto supports cross-linking with anchors"
  },
  {
    "objectID": "slides/slides11.html#faq-selectorgadget---multiple-clicks",
    "href": "slides/slides11.html#faq-selectorgadget---multiple-clicks",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: SelectorGadget - Multiple Clicks",
    "text": "FAQ: SelectorGadget - Multiple Clicks\n\nWhy does SelectorGadget go “unique” when I click multiple elements of interest?\n\nCan’t find a common structure:\n\nTypically a problem within lists or common element types"
  },
  {
    "objectID": "slides/slides11.html#faq-selectors---nesting",
    "href": "slides/slides11.html#faq-selectors---nesting",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: Selectors - Nesting",
    "text": "FAQ: Selectors - Nesting\n\nHow to avoid unwanted elements such as headers or sidebars, focusing only on the main content I need?\n\nNest your selectors!\nthing1 thing2 will select only thing2s inside a thing1\nStarWars page\n\nTry main h2"
  },
  {
    "objectID": "slides/slides11.html#faq-relationship-to-markdown",
    "href": "slides/slides11.html#faq-relationship-to-markdown",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: Relationship to Markdown",
    "text": "FAQ: Relationship to Markdown\n\nIs [HTML] similar to Markdown ?\n\nMarkdown is an easier way to write (a subset of) HTML\nName is a bad joke: Markup (M in HTML) vs Markdown\nHTML can (theoretically) do more, but painful to write by hand"
  },
  {
    "objectID": "slides/slides11.html#faq-messy-html",
    "href": "slides/slides11.html#faq-messy-html",
    "title": "STA 9750 - Week 11",
    "section": "FAQ: Messy HTML",
    "text": "FAQ: Messy HTML\n\nHow can we target data with CSS Selectors in messy HTML?\n\nPain and suffering - depends how messy.\nWorst case: a little bit of HTML selection + text processing (next week)"
  },
  {
    "objectID": "slides/slides11.html#selected-peer-advice",
    "href": "slides/slides11.html#selected-peer-advice",
    "title": "STA 9750 - Week 11",
    "section": "Selected Peer Advice:",
    "text": "Selected Peer Advice:\n\nMore effort into writing, motivation, and formatting (several)\nBe sure to push image files in addition to qmd\nCode-folding!\nLearning style for more experienced peers\nMore charts\nsetNames to improve table formatting. (MW: I prefer rename YMMV)\nTable of contents\nBe selective in outputs"
  },
  {
    "objectID": "slides/slides11.html#my-advice",
    "href": "slides/slides11.html#my-advice",
    "title": "STA 9750 - Week 11",
    "section": "My Advice",
    "text": "My Advice\n\nMost effort \\(\\neq\\) most important\nReader knows less and cares less than you think\nSummary / Abstract goes a long way\nCross-linking useful\nUse suggested file names\n“What have you tried?”\nHelp me help you\nPrint less"
  },
  {
    "objectID": "slides/slides11.html#agenda",
    "href": "slides/slides11.html#agenda",
    "title": "STA 9750 - Week 11",
    "section": "Agenda",
    "text": "Agenda\n\nReview of HTML / CSS Structure\nPre-Assignment Examples\n\nStar Wars\nCUNY Table\nBaruch GPS\n\nExercise\n\nCUNY Map\nCocktails (Part 1)"
  },
  {
    "objectID": "slides/slides11.html#html-review",
    "href": "slides/slides11.html#html-review",
    "title": "STA 9750 - Week 11",
    "section": "HTML Review",
    "text": "HTML Review\n\nHTML Structure\nCSS Selectors (SelectorGadget)\nIntroduction rvest"
  },
  {
    "objectID": "slides/slides11.html#pre-assignment-exercises",
    "href": "slides/slides11.html#pre-assignment-exercises",
    "title": "STA 9750 - Week 11",
    "section": "Pre-Assignment Exercises",
    "text": "Pre-Assignment Exercises\n\nStar Wars\n\n\nmain h2\n\n\n\nCUNY Table\n\n\n\ntable or tbody\n\n\n\nBaruch GPS\n\n\n\n.geo"
  },
  {
    "objectID": "slides/slides11.html#exercise-1-cuny-map",
    "href": "slides/slides11.html#exercise-1-cuny-map",
    "title": "STA 9750 - Week 11",
    "section": "Exercise 1: CUNY Map",
    "text": "Exercise 1: CUNY Map\nRecall Lab 1. Goal: extend map to all CUNYs\nSteps:\n\nRead CUNY table and extract links\nFollow links and pull coordinates\n\nTo read geo class, use this:\n\n\n\nCOORDS &lt;- html_element(\".geo\") |&gt; html_text() |&gt; str_split_1(\";\")\nLAT &lt;- as.numeric(COORDS[1])\nLON &lt;- as.numeric(COORDS[2])\n\n\nAdapt Lab 1 leaflet to show all locations"
  },
  {
    "objectID": "slides/slides11.html#breakout-rooms",
    "href": "slides/slides11.html#breakout-rooms",
    "title": "STA 9750 - Week 11",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nOrder\nTeam\n\nOrder\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "slides/slides11.html#exercise-2-cocktails",
    "href": "slides/slides11.html#exercise-2-cocktails",
    "title": "STA 9750 - Week 11",
    "section": "Exercise 2: Cocktails",
    "text": "Exercise 2: Cocktails\nGoal: create a cocktail data frame from Hadley’s Recipies\nToday: - How to find them all? - How to extract individual recepies? - How to pull items from each recipie?\nNext time: - How to convert text to numeric values + column info - Data wrangling"
  },
  {
    "objectID": "slides/slides13.html#mini-project-04",
    "href": "slides/slides13.html#mini-project-04",
    "title": "STA 9750 - Week 13",
    "section": "STA 9750 Mini-Project #04",
    "text": "STA 9750 Mini-Project #04\n\nCongratulations! Done with Mini-Projects!\nPeer reviews to be assigned by EoW"
  },
  {
    "objectID": "slides/slides13.html#week-13-pre-assignment",
    "href": "slides/slides13.html#week-13-pre-assignment",
    "title": "STA 9750 - Week 13",
    "section": "Week 13 Pre-Assignment",
    "text": "Week 13 Pre-Assignment\nDue at midnight tonight - take a moment to do it now if you haven’t already!\n\nReflection on course:\n\nHow far have you come?\nWhat have you learned?\nWhat was helpful? What was unhelpful?\n\n\nDone with pre-assignments!\nThis is in addition to the Baruch central course assesments."
  },
  {
    "objectID": "slides/slides13.html#week-13-pre-assignment-1",
    "href": "slides/slides13.html#week-13-pre-assignment-1",
    "title": "STA 9750 - Week 13",
    "section": "Week 13 Pre-Assignment",
    "text": "Week 13 Pre-Assignment\nUsed to improve future course offerings. This semester:\n\nAdded a second round of project feedback\n\nHelp students “scope” projects suitably\n\nMore applied analytics than programming exercises in HW\n\nOther programming resources already online;\nMany students have prior experience (Python, SQL)\nMore interest in Analytics than Software Engineering\n\nAdded GitHub and Portfolio Construction\n\nGive students evidence of skills to share with employers"
  },
  {
    "objectID": "slides/slides13.html#grading",
    "href": "slides/slides13.html#grading",
    "title": "STA 9750 - Week 13",
    "section": "Grading",
    "text": "Grading\nReturned:\n\nMid-Term Check-In Feedback\n\nWe owe you:\n\nMP#03 Grades in Brightspace"
  },
  {
    "objectID": "slides/slides13.html#upcoming",
    "href": "slides/slides13.html#upcoming",
    "title": "STA 9750 - Week 13",
    "section": "Upcoming",
    "text": "Upcoming\nDecember 12 - Last Day of Class!\n\nFinal Project Presentations\n\nReview the Rubric!\nNon-Technical Presentation - Think of yourself as a “consultant” asked by a client to investigate a topic."
  },
  {
    "objectID": "slides/slides13.html#final-project-presentations",
    "href": "slides/slides13.html#final-project-presentations",
    "title": "STA 9750 - Week 13",
    "section": "Final Project Presentations",
    "text": "Final Project Presentations\nExample Presentation Structure:\n\nMotivation\nHow your work relates to other previous work\nOverarching Question\nDiscussion of Data\n\nWhere? How good is it? Weaknesses / Limitations?\n\nSpecific Questions\n\nHow do they support overarching question?\nWhat did you do? What did you find?"
  },
  {
    "objectID": "slides/slides13.html#final-project-presentations-1",
    "href": "slides/slides13.html#final-project-presentations-1",
    "title": "STA 9750 - Week 13",
    "section": "Final Project Presentations",
    "text": "Final Project Presentations\nExample Presentation Structure (continued):\n\nIntegration of Findings\nMajor Conclusions\n\nHow do quantitative specific findings provide qualitative insights?\nWhat can you see be combining specific questions that you can’t see from a single specific question?\nIncluding Limitations of Current Study\n\nPotential Future Work"
  },
  {
    "objectID": "slides/slides13.html#final-project-reports",
    "href": "slides/slides13.html#final-project-reports",
    "title": "STA 9750 - Week 13",
    "section": "Final Project Reports",
    "text": "Final Project Reports\nGroup and Individual Reports\n\nSubmitted via GitHub and Brightspace\n\nDeadline extended to the day of the ‘final’\n\nRegistrar’s office has not released Final Exam schedule … grumble, grumble\nTentatively: December 19th\n\nWill confirm when exam schedule released\n\nNo late work accepted (I have to submit grades!)"
  },
  {
    "objectID": "slides/slides13.html#peer-assessment",
    "href": "slides/slides13.html#peer-assessment",
    "title": "STA 9750 - Week 13",
    "section": "Peer Assessment",
    "text": "Peer Assessment\nOn Brightspace, I have opened an additional quiz for peer evaluation of your teammates.\n\n8 questions: scale of 1 (bad) to 3 (great)\n\nPlease submit a copy for each of your teammates.\n\nBrightspace set to allow multiple submissions.\nDue on same day as reports\n\n\nIf you don’t submit these, you will receive a 0 for your peer evaluations\nNo late work accepted (I have to submit grades!)"
  },
  {
    "objectID": "slides/slides13.html#final-project-grading",
    "href": "slides/slides13.html#final-project-grading",
    "title": "STA 9750 - Week 13",
    "section": "Final Project Grading",
    "text": "Final Project Grading\nRubric is set high to give me flexibility to reward teams that take on big challenges\nHard rubric =&gt; Grades are curved generously\n\nMultiple paths to success\nIf your problem is “easy” on an element (data import in particular), that’s great! Don’t spend the effort over-complicating things. Effort is better spent elsewhere"
  },
  {
    "objectID": "slides/slides13.html#agenda",
    "href": "slides/slides13.html#agenda",
    "title": "STA 9750 - Week 13",
    "section": "Agenda",
    "text": "Agenda\n\nPredictive Modeling with tidymodels\n\nAdapted from (Case Study)[https://www.tidymodels.org/start/case-study/]"
  },
  {
    "objectID": "slides/slides13.html#breakout-rooms",
    "href": "slides/slides13.html#breakout-rooms",
    "title": "STA 9750 - Week 13",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nOrder\nTeam\n\nOrder\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "slides/slides13.html#tidymodels",
    "href": "slides/slides13.html#tidymodels",
    "title": "STA 9750 - Week 13",
    "section": "tidymodels",
    "text": "tidymodels\nStrength of R:\n\nThousands of authors contributing packages to CRAN\n\n\nWeakness of R:\n\nThousands of authors contributing slightly incompatible packages to CRAN\n\n\n\nNo two modeling packages have exactly the same API. Makes changing between interfaces cumbersome"
  },
  {
    "objectID": "slides/slides13.html#tidymodels-1",
    "href": "slides/slides13.html#tidymodels-1",
    "title": "STA 9750 - Week 13",
    "section": "tidymodels",
    "text": "tidymodels\ntidymodels attemps to provide a uniform interface to a wide variety of predictive Machine Learning tools\nAdvantages:\n\nEasy to swap out different algorithms to find the best\n\nDisadvantages:\n\nHarder to take advantage of the strengths of each approach\n\n\nI have dedicated my academic life to the differences in these methods, but 99% of the time, “black-box” prediction is good enough. In STA 9890, we get into the weeds - not here."
  },
  {
    "objectID": "slides/slides13.html#ml-vs-statistical-pipelines",
    "href": "slides/slides13.html#ml-vs-statistical-pipelines",
    "title": "STA 9750 - Week 13",
    "section": "ML vs Statistical Pipelines",
    "text": "ML vs Statistical Pipelines\nStatistics / Data Science:\n\nFind the model that fits the data best\nModel should capture all important data features\nInterpretability\nHistory: Grounded in lab sciences where experiments are expensive and data is limited"
  },
  {
    "objectID": "slides/slides13.html#ml-vs-statistical-pipelines-1",
    "href": "slides/slides13.html#ml-vs-statistical-pipelines-1",
    "title": "STA 9750 - Week 13",
    "section": "ML vs Statistical Pipelines",
    "text": "ML vs Statistical Pipelines\nMachine Learning:\n\nFind the model that predicts the data best\nNo “perfect” model - just the best one we’ve found so far\nBlack-box techniques are great, if effective\nHistory: Silicon Valley “at scale”\n\nValidation based on of-of-sample or test predictions"
  },
  {
    "objectID": "slides/slides13.html#validating-predictive-power",
    "href": "slides/slides13.html#validating-predictive-power",
    "title": "STA 9750 - Week 13",
    "section": "Validating Predictive Power",
    "text": "Validating Predictive Power\nHow to check whether a model predicts well?\n\nNeed more data! But where to get more data?\n\nActually get more data (hard, expensive, slow)\nSplit data into parts - test/training split\nCross-Validation\nResampling\n\n\n\nToday, we’ll primarily use a combination: Test/Train split & Cross-Validation!"
  },
  {
    "objectID": "slides/slides13.html#cross-validation",
    "href": "slides/slides13.html#cross-validation",
    "title": "STA 9750 - Week 13",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nCross-Validation is done on the estimator, not the fitted algorithm"
  },
  {
    "objectID": "slides/slides13.html#tidymodels-2",
    "href": "slides/slides13.html#tidymodels-2",
    "title": "STA 9750 - Week 13",
    "section": "tidymodels",
    "text": "tidymodels\ntidymodels workflow:\n\nInitial Split\nPre-Process\nFit (many) models\nSelect best\nRefit\nTest Set Assessment\n\ntidymodels is very punny, so a bit hard to tell which step is which…"
  },
  {
    "objectID": "slides/slides13.html#initial-split",
    "href": "slides/slides13.html#initial-split",
    "title": "STA 9750 - Week 13",
    "section": "Initial Split",
    "text": "Initial Split\n\n# Stratified sampling to ensure balance\nsplits      &lt;- initial_split(hotels, \n                             strata = children)\n\nhotel_train &lt;- training(splits)\nhotel_test  &lt;- testing(splits)"
  },
  {
    "objectID": "slides/slides13.html#pre-process",
    "href": "slides/slides13.html#pre-process",
    "title": "STA 9750 - Week 13",
    "section": "Pre-Process",
    "text": "Pre-Process\n\nholidays &lt;- c(\"AllSouls\", \"AshWednesday\", \"ChristmasEve\", \"Easter\", \n              \"ChristmasDay\", \"GoodFriday\", \"NewYearsDay\", \"PalmSunday\")\n\nrecipe &lt;- \n  recipe(children ~ ., data = hotel_other) |&gt; \n  step_date(arrival_date) |&gt; \n  step_holiday(arrival_date, holidays = holidays) |&gt; \n  step_rm(arrival_date) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())"
  },
  {
    "objectID": "slides/slides13.html#fit-models",
    "href": "slides/slides13.html#fit-models",
    "title": "STA 9750 - Week 13",
    "section": "Fit Models",
    "text": "Fit Models\n\nlr_model &lt;- \n  logistic_reg(penalty = tune(), mixture = 1) |&gt; \n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "slides/slides13.html#select-best",
    "href": "slides/slides13.html#select-best",
    "title": "STA 9750 - Week 13",
    "section": "Select Best",
    "text": "Select Best\nFind a grid of parameters\n\nlr_reg_grid &lt;- data.frame(penalty = 10^seq(-4, -1, length.out = 30))\n\nPerform CV splits:\n\nlr_folds &lt;- vfold_cv(hotel_train, v = 5)"
  },
  {
    "objectID": "slides/slides13.html#select-best-1",
    "href": "slides/slides13.html#select-best-1",
    "title": "STA 9750 - Week 13",
    "section": "Select Best",
    "text": "Select Best\nDefine a workflow:\n\nlr_workflow &lt;-  \n  workflow() |&gt; \n  add_model(lr_mod) |&gt; \n  add_recipe(lr_recipe)\n\nFit workflow to a grid of parameters:\n\nlr_results &lt;- \n  lr_workflow |&gt; \n  tune_grid(grid = lr_reg_grid,\n            control = control_grid(save_pred = TRUE),\n            resamples = lr_folds,\n            metrics = metric_set(roc_auc))"
  },
  {
    "objectID": "slides/slides13.html#select-best-2",
    "href": "slides/slides13.html#select-best-2",
    "title": "STA 9750 - Week 13",
    "section": "Select Best",
    "text": "Select Best\nVisual examination\n\nlr_results |&gt; \n  collect_metrics() |&gt; \n  ggplot(aes(x = penalty, y = mean)) + \n  geom_point() + \n  geom_line() + \n  ylab(\"Area under the ROC Curve\") +\n  scale_x_log10(labels = scales::label_number())"
  },
  {
    "objectID": "slides/slides13.html#select-best-3",
    "href": "slides/slides13.html#select-best-3",
    "title": "STA 9750 - Week 13",
    "section": "Select Best",
    "text": "Select Best\n\nlr_results |&gt; show_best()\nlr_best &lt;- lr_results |&gt; select_best()\n\nlr_best_fit &lt;- lr_results |&gt; fit_best()"
  },
  {
    "objectID": "slides/slides13.html#refit",
    "href": "slides/slides13.html#refit",
    "title": "STA 9750 - Week 13",
    "section": "Refit",
    "text": "Refit\n\nlr_best_fit &lt;- lr_results |&gt; fit_best()"
  },
  {
    "objectID": "slides/slides13.html#test-set-assessment",
    "href": "slides/slides13.html#test-set-assessment",
    "title": "STA 9750 - Week 13",
    "section": "Test Set Assessment",
    "text": "Test Set Assessment\n\npredict(lr_best_fit, hotel_test)"
  },
  {
    "objectID": "slides/slides13.html#exercise",
    "href": "slides/slides13.html#exercise",
    "title": "STA 9750 - Week 13",
    "section": "Exercise",
    "text": "Exercise\nWork through the random forest components of https://www.tidymodels.org/start/case-study\nYou’ll need to work through the data import elements as well"
  },
  {
    "objectID": "slides/slides13.html#other-tidymodels-tools",
    "href": "slides/slides13.html#other-tidymodels-tools",
    "title": "STA 9750 - Week 13",
    "section": "Other tidymodels tools",
    "text": "Other tidymodels tools\n\nModel Stacking\nProbabilistic Predictions\nUncertainty Bounds (Conformal Inference)\nMultilevel (Mixed-Effect) Models\nFairness Audits"
  },
  {
    "objectID": "slides/slides13.html#more-reading",
    "href": "slides/slides13.html#more-reading",
    "title": "STA 9750 - Week 13",
    "section": "More Reading",
    "text": "More Reading\nhttps://www.tidymodels.org/start/"
  },
  {
    "objectID": "slides/slides06.html#mini-project-01",
    "href": "slides/slides06.html#mini-project-01",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\n👏 Thank you to everyone who took part in peer feedback! 👏\nI will start processing grades and uploading to Brightspace soon. (Moving day for me…)\n\nI’ve started reading some submissions - really excellent work!\n\nPlots (exciting!), sophisticated and insightful analyses, creative policymaking\n\n\n\nHopefully automated checks and personalized emails helpful (new this year)\nIf you have ideas for other useful automated ‘supports’ let me know"
  },
  {
    "objectID": "slides/slides06.html#mini-project-01-1",
    "href": "slides/slides06.html#mini-project-01-1",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\nSome popular tips:\n\nThe kable function makes nice tables\nThe gt package makes very nice tables\nQuarto allows “code folding”: useful for hiding long boring code blocks\n\n\n\nShow the code\n1 + 1\n\n\n[1] 2\n\n\nYou can set this globally. You also want to keep echo: true."
  },
  {
    "objectID": "slides/slides06.html#mini-project-02",
    "href": "slides/slides06.html#mini-project-02",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nMP#02 in process now - Identifying Environmentally Responsible US Public Transit Systems\n\nDue 2025-03-26 at 11:45pm ET\n\nGitHub post (used for peer feedback) AND Brightspace\n\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness\nUse what you learned from MP #01"
  },
  {
    "objectID": "slides/slides06.html#upcoming-mini-projects",
    "href": "slides/slides06.html#upcoming-mini-projects",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTopics\n\nMP#03: Creating the Ultimate Playlist\nMP#04: Exploring Recent US Political Shifts"
  },
  {
    "objectID": "slides/slides06.html#pre-assignments",
    "href": "slides/slides06.html#pre-assignments",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is 2025-03-19 at 11:45pm ET\n\nI missed a few comments in the previous cycle (sorry!) - trying to catch up in the next few days"
  },
  {
    "objectID": "slides/slides06.html#course-support",
    "href": "slides/slides06.html#course-support",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 2x / week\n\nAsynchronous\n\nPiazza (\\(&lt;30\\) minute average response time)"
  },
  {
    "objectID": "slides/slides06.html#upcoming-week",
    "href": "slides/slides06.html#upcoming-week",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nDue Wednesday at 11:45pm:\n\nPre-Assignment #07 (Brightspace)\n\nIntroduction to plotting with ggplot2\n\n\nExpect back:\n\nMP#01 consolidated grades\nProject proposal instructor feedback"
  },
  {
    "objectID": "slides/slides06.html#course-project-presentations",
    "href": "slides/slides06.html#course-project-presentations",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Course Project Presentations",
    "text": "Course Project Presentations\n\n\n\nOrder\nMembers\n\nTeam\nMembers\n\n\n\n\n1\nVH + SG + DS + DL\n\n6\nVG\n\n\n2\nHZ + JLL + CA\n\n7\nEM + AK + GMdS + JL\n\n\n3\nMT + CW\n\n8\nSJB + JC + SB + ZS\n\n\n4\nSD + GO + CFGF\n\n9\nGS\n\n\n5\nGB + RJ + FS + MH\n\n10\nCWo."
  },
  {
    "objectID": "slides/slides06.html#project-proposal-presentations",
    "href": "slides/slides06.html#project-proposal-presentations",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Project Proposal Presentations",
    "text": "Project Proposal Presentations\nOfficial Description\n\n6 minute presentation\nKey topics:\n\nAnimating Question\nTeam Roster\n\nAlso discuss: Possible specific questions, data sources, analytical plan, anticipated challenges\n\n\nMost important: team names!\nLast semester: Rat Pack, Subway Surfers, Going for Gold, EVengers, etc."
  },
  {
    "objectID": "slides/slides06.html#proposal-peer-feedback",
    "href": "slides/slides06.html#proposal-peer-feedback",
    "title": "STA 9750 - Week 6 Update",
    "section": "Proposal Peer Feedback",
    "text": "Proposal Peer Feedback\nProject Peer Feedback Platform: Vocat\nAfter class:\n\nI upload presentation videos\nYou leave comments and scores on at least 1 presentation\n\nTry to comment on “under-discussed” presentations\nI will be looking for constructive feedback\nLess detail than mini-prorjects\n\n\nNew tool - learning experience for all. Bear with us."
  },
  {
    "objectID": "slides/slides06.html#after-proposals",
    "href": "slides/slides06.html#after-proposals",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "After Proposals",
    "text": "After Proposals\n100% optional discussion of dplyr vs SQL\n\nSQL is a very common Data Scientist interview topic so if you’re not taking a database course, might be useful"
  },
  {
    "objectID": "preassignments.html",
    "href": "preassignments.html",
    "title": "STA 9750 - Pre-Assignments",
    "section": "",
    "text": "In lieu of traditional homework, STA 9750 has weekly pre-assignments designed to achieve several interlocking goals:\n\nProvide initial exposure to that week’s topic before the lecture and lab session\nAllow students with less previous programming experience more time to familiarize themselves with that week’s topic\nAllow students to submit questions to be covered in class\n\nEach Pre-Assignment will be submitted via CUNY Brightspace and due the night before class (Wednesdays at 11:45). These are short assignments, typically only a few questions, so extensions will not be given outside of exceptional circumstances.\n\nPre-Assignments\n\nPre-Assignment for Week #01\nNone.\n\n\nPre-Assignment for Week #02 - Getting Started with Markdown\nDue Dates:\n\nReleased to Students: 2025-01-30\nDue on Brightspace: 2025-02-05 at 11:45pm ET\n\nIn this Pre-Assignment, you will familiarize yourself with the basics of Markdown, an easy way to write and format documents. In class, we will use Markdown based tools to create dynamic data analysis documents seamlessly combining code, text, and graphics.\n\n\nPre-Assignment for Week #03 - Calculator Work with R\nDue Dates:\n\nReleased to Students: 2025-02-06\nDue on Brightspace: 2025-02-12 at 11:45pm ET\n\nIn this week’s preassignment, you will familiarize yourself with some basic “calculator math” in R. You will also see how function calls work as we get ready to start some proper R programming.\n\n\nPre-Assignment for Week #04 - Single-Table dplyr Verbs\nDue Dates:\n\nReleased to Students: 2025-02-13\nDue on Brightspace: 2025-02-19 at 11:45pm ET\n\nIn this week’s preassignment, you will review dplyr’s “single-table” verbs. These are functions that take a single data frame and do something, typically returning another data frame. We can divide these into three major groups:\n\nSubsetting rows (filter) and columns (select);\nChanging and creating columns (mutate and less commonly, rename);\noperating with group structure (group_by, summarize)\n\n\n\nPre-Assignment for Week #05 - Multi-Table dplyr Verbs\nDue Dates:\n\nReleased to Students: 2025-02-20\nDue on Brightspace: 2025-02-26 at 11:45pm ET\n\nIn this week’s preassignment, you will review dplyr’s most important “multi-table” verbs, the join operators. These are functions that take multiple data frames and combine them together. You will need to use this type of functionality to combine data from different sources together in a principled and organized fashion. You will also learn a bit about the pivot_longer and pivot_wider functions used to change the shape of data frames. These are particularly useful in conjunction with joins: you will often need to reshape two tables to “join” properly (typically, lengthening them with pivot_longer) and then reshape them for downstream presentation (typically with pivot_wider).\n\n\nPre-Assignment for Week #06\nNone.\nThe 2025-03-13 class session will be dedicated to Course Project Proposals.\n\n\nPre-Assignment for Week #07 - Lots of Plots\nDue Dates:\n\nReleased to Students: 2025-03-13\nDue on Brightspace: 2025-03-19 at 11:45pm ET\n\nIn this week’s preassignment, we begin to explore the wonderful world of statistical graphics.\n\n\nPre-Assignment for Week #08 - More Plots\nDue Dates:\n\nReleased to Students: 2025-03-20\nDue on Brightspace: 2025-03-26 at 11:45pm ET\n\nIn this week’s preassignment, we dive deeper into the world of statistical graphics, watching statistical graphics in the hands of a master.\n\n\nPre-Assignment for Week #09 - Flat-File Data Ingest\nDue Dates:\n\nReleased to Students: 2025-03-27\nDue on Brightspace: 2025-04-02 at 11:45pm ET\n\nIn this week’s preassignment, we review the basics of reading data files into R.\n\n\nPre-Assignment for Week #10\nNone.\nThe 2025-04-10 class session will be dedicated to Course Project Mid-Semester Check-Ins.\n\n\nPre-Assignment for Week #11 - Intro to HTML\nDue Dates:\n\nReleased to Students: 2025-04-10\nDue on Brightspace: 2025-04-16 at 11:45pm ET\n\nIn this week’s preassignment, students are introduced to the basics of CSS selectors.\n\n\nPre-Assignment for Week #12 - Strings and Things\nDue Dates:\n\nReleased to Students: 2025-04-24\nDue on Brightspace: 2025-04-30 at 11:45pm ET\n\nTBA\n\n\nPre-Assignment for Week #13 - Introduction to tidymodels\nDue Dates:\n\nReleased to Students: 2025-05-01\nDue on Brightspace: 2025-05-07 at 11:45pm ET\n\nTBA\n\n\nPre-Assignment for Week #14\nNone.\nThe 2025-05-15 class session will be dedicated to Course Project Final Presentations."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "STA9750",
    "section": "",
    "text": "flowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n  A(Stephanie) --&gt; B(Born in Brooklyn, NY)\n  B --&gt; C{Education}\n  C --&gt; D(Bachelor's of Finance)\n  C --&gt; E(Master's of Science)"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "",
    "text": "R for Data Science (R4DS) is an excellent free textbook covering much of the material of this course.\nThe tidyverse packages used throughout this course have excellent documentation:\n\nreadr\ndplyr\ntidyr\nggplot2\nrvest\n\nThe quarto guide is particularly useful.\nStudents may also benefit from the Unofficial Solutions for R4DS, the Posit R Cheatsheets, Statistical Infernece via Data Science, and the book Elegrant Graphics for Data Analysis.\nThe book Happy Git with R is particularly useful for git usage. General git usage is also covered by the Git Book.\nThe book Veridical Data Science by Yu and Barter has lots of useful advice on applied data analytics that may help with the course project.\nStudents are encouraged to ask the instructor for additional resources as needed.\n\n\nAfter you create your GitHub account in Mini-Project #00, you should register for the GitHub Student Developer Pack. This will give you increased GitHub functionality as well as free access to GitHub CoPilot, a coding assistance tool that students in previous offerings of this course have found helpful.\nThe Student Developer Pack also gives you access to several helpful online tutorials, including GitHub’s official Understanding Markdown and Intro to GitHub courses.\n\n\n\nSTA 9750 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nWritten and oral communication is an important element of this course.\nThe Baruch Writing Center offers free support to all Baruch students; students can meet with a professional writing consultant one-to-one (in person, in NVC 8-185, or online, by video, audio, and text-based chat) and in group workshops. Writing Center consultants will work collaboratively with you to deepen your writing and English language skills. At any step in the process, they’ll help you become a more independent, confident, and versatile writer.1\nBaruch’s Tools for Clear Speech program improves the pronunciation, fluency, and pragmatic abilities of English language learners and non-native English speakers at Baruch College. TfCS participants achieve more effective and intelligible communication, developing skills that empower them to succeed in their classrooms, careers, and beyond. TfCS offers a range of free face-to-face and online services with our professional Speech Consultants, including One-to-One Sessions, small-group Focused Skills Series sessions, large-group Overview Workshops, interview and career preparation, and weekly Conversation Hours.\n\n\n\nAll software used in this course is Free and Open-Source Software that can be installed on your personal machine without cost. Students will need to install, at a minimum,\n\nR\nrstudio Desktop Edition\nquarto\n\nThanks to the Binder project, we are also able to provide free virtual machines equipped with all course software pre-installed:\n\nRStudio\nCommand Line Access\n\nCUNY also provides a Windows-based RStudio virtual machine through Apporto.\nPlease note that these are transient instances and any work saved on these machines may be lost without warning.\n\n\n\nSeveral “helper” functions may be found here. These include functionality to verify that assignments and feedback have been properly submitted on GitHub, to count words on a page, etc. Note that these are a new addition to this course, so please contact the instructor if they do not appear to be working properly."
  },
  {
    "objectID": "resources.html#course-resources",
    "href": "resources.html#course-resources",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "",
    "text": "R for Data Science (R4DS) is an excellent free textbook covering much of the material of this course.\nThe tidyverse packages used throughout this course have excellent documentation:\n\nreadr\ndplyr\ntidyr\nggplot2\nrvest\n\nThe quarto guide is particularly useful.\nStudents may also benefit from the Unofficial Solutions for R4DS, the Posit R Cheatsheets, Statistical Infernece via Data Science, and the book Elegrant Graphics for Data Analysis.\nThe book Happy Git with R is particularly useful for git usage. General git usage is also covered by the Git Book.\nThe book Veridical Data Science by Yu and Barter has lots of useful advice on applied data analytics that may help with the course project.\nStudents are encouraged to ask the instructor for additional resources as needed.\n\n\nAfter you create your GitHub account in Mini-Project #00, you should register for the GitHub Student Developer Pack. This will give you increased GitHub functionality as well as free access to GitHub CoPilot, a coding assistance tool that students in previous offerings of this course have found helpful.\nThe Student Developer Pack also gives you access to several helpful online tutorials, including GitHub’s official Understanding Markdown and Intro to GitHub courses.\n\n\n\nSTA 9750 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nWritten and oral communication is an important element of this course.\nThe Baruch Writing Center offers free support to all Baruch students; students can meet with a professional writing consultant one-to-one (in person, in NVC 8-185, or online, by video, audio, and text-based chat) and in group workshops. Writing Center consultants will work collaboratively with you to deepen your writing and English language skills. At any step in the process, they’ll help you become a more independent, confident, and versatile writer.1\nBaruch’s Tools for Clear Speech program improves the pronunciation, fluency, and pragmatic abilities of English language learners and non-native English speakers at Baruch College. TfCS participants achieve more effective and intelligible communication, developing skills that empower them to succeed in their classrooms, careers, and beyond. TfCS offers a range of free face-to-face and online services with our professional Speech Consultants, including One-to-One Sessions, small-group Focused Skills Series sessions, large-group Overview Workshops, interview and career preparation, and weekly Conversation Hours.\n\n\n\nAll software used in this course is Free and Open-Source Software that can be installed on your personal machine without cost. Students will need to install, at a minimum,\n\nR\nrstudio Desktop Edition\nquarto\n\nThanks to the Binder project, we are also able to provide free virtual machines equipped with all course software pre-installed:\n\nRStudio\nCommand Line Access\n\nCUNY also provides a Windows-based RStudio virtual machine through Apporto.\nPlease note that these are transient instances and any work saved on these machines may be lost without warning.\n\n\n\nSeveral “helper” functions may be found here. These include functionality to verify that assignments and feedback have been properly submitted on GitHub, to count words on a page, etc. Note that these are a new addition to this course, so please contact the instructor if they do not appear to be working properly."
  },
  {
    "objectID": "resources.html#course-policies",
    "href": "resources.html#course-policies",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic Integrity Policy\nI fully support CUNY’s Policy on Academic Integrity, which states, in part:\n\nAcademic dishonesty is prohibited in The City University of New York. Penalties for academic dishonesty include academic sanctions, such as failing or otherwise reduced grades, and/or disciplinary sanctions, including suspension or expulsion.\n\n\nAcademic integrity is at the core of a college or university education. Faculty assign essays, exams, quizzes, projects, and so on both to extend the learning done in the classroom and as a means of assessing that learning. When students violate the academic integrity policy (i.e., “cheat”), they are committing an act of theft that can cause real harm to themselves and others including, but not limited to, their classmates, their faculty, and the caregivers who may be funding their education. Academic dishonesty confers an unfair advantage over others, which undermines educational equity and fairness. Students who cheat place their college’s accreditation and their own future prospects in jeopardy.\n\nAcademic sanctions in this class will range from an F on the Assignment to an F in this Course. A report of suspected academic dishonesty will be sent to the Office of the Dean of Students.\nStudents are encouraged to contact the instructor with any questions or concerns related to matters of academic integrity.\n\n\nExternal Resources Use Policy\nFor the coding elements of this course, students are encouraged to use freely available online resources, including question-and-answer fora such as StackOverflow. You may also use AI-driven developer tools such as GitHub Co-Pilot. Paid services are not allowed. On each assignment, you will be asked to list external resources used on each assignment. You are ultimately responsible for the correctness of any submitted materials - “the AI told me so”” is not a valid defense.\nNote on ChatGPT and Related Large-Language Models: You may not use large-language models to complete any assignment in this course. Specifically, you may not use tools where you describe the course assignment in natural language and receive (pseudo-)code output. While these tools are powerful, and often surprisingly accurate, for this task, using them in this manner will undermine the learning objectives of this course.\nFor the written elements of this course (e.g. Project Final Report), standard academic expectations of attribution and citation are in place. This will be covered in more detail in the course project documents.\nStudents are highly encouraged to collaborate on homework assignments, but each student is required to individually and complete each assignment. If substantially identical assignments are submitted, the instructor may require each student to individually demonstrate their understanding of the material. Collaborators should be listed at the end of each submitted assignment along with a statement of contributions.\n\n\nUnexcused Abscence Policy\nAttendance is not required, but lecture recordings will not be provided. Students are responsible for the content of all sessions missed.\n\n\nLate Work Policy\nLate work will not be accepted except in extraordinary and unforeseeable circumstances. Students submitting late work should provide supporting documentation to the Office of the Dean of Students; ODS will provide the instructor with a letter authorizing late work submission as appropriate.\nAll assignment submission technology used in this course allows multiple submissions, so students are encouraged to submit early and often to avoid any technology troubles associated with late submission.\nNote that late work is allowed consistent with specific pre-arranged course accommodations as noted below."
  },
  {
    "objectID": "resources.html#course-accommodations",
    "href": "resources.html#course-accommodations",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "Course Accommodations",
    "text": "Course Accommodations\n\nDisability Services\nIt is CUNY policy to provide Accommodations and Academic Adjustments to students with disabilities.\nAny student who has a disability who may need accommodations in this class should register as early as possible with Student Disability Services. Your registration with Student Disability Services is confidential, and is not recorded on your Baruch Academic Record. SDS can be reached by email at disability.services@baruch.cuny.edu, by phone at 646-312-4590, or in person at NVC 2-272.\nPlease note that the instructor cannot provide accommodations unless requested by SDS.\n\n\nReligious Accomodations\nIt is CUNY policy to provide accommodations for students’ sincerely held religious beliefs. If a religious accommodation is requested, please contact the instructor at least two weeks in advance."
  },
  {
    "objectID": "resources.html#care-resources-for-students1",
    "href": "resources.html#care-resources-for-students1",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "Care Resources for Students2",
    "text": "Care Resources for Students2\nTake care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress.\nAll of us benefit from support during times of struggle. You are not alone. Asking for support sooner rather than later is often helpful.\nThis course is intended to be demanding, but not difficult. If you feel like you are struggling, please reach out sooner rather than later. Swimming long-distances in choppy waters builds strength: drowning doesn’t.\n\nMental Health Resources\nIf you or anyone you know experiences significant academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support.\nThe Baruch Counselling Center is here to help. You can visit them in person at 137 E 25th St, 9th floor or call them at 646-312-2155 during normal business hours; you can make an appointment online here. For more immediate support, please call NYC WELL (1-888-NYC-WELL or 1-888-692-9355).\nAsking for help is often difficult: consider reaching out to a friend, family, or a member of the faculty you trust for help getting connected to support that can help.\nIf you are worried about a friend or classmate, consider reaching out to the Baruch Campus Intervention Team.\n\n\nPhysical Health\nHealthy CUNY promotes well-being and a culture of health in order to foster the academic and life success of all CUNY students. They can connect you with a variety of campus- and community-based healthcare providers.\nBaruch Health Services provides students with a full range of clinical health services. Call 646-312-2040 or email StudentHealthCareCenter@baruch.cuny.edu to make an appointment.\n\n\nFood Security\nAll CUNY students have access to CUNY Food Pantries located throughout the five boroughs, thanks to the CUNY CARES program. CUNY CARES is also able to help qualifying students with SNAP (“Food Stamps”) enrollment.\nOn campus, you can also access the Bearcat Food Pantry.\n\nFinancial Security\nBaruch students experiencing heightened financial stress have access to Student Emergency Grants administered through the Office of the Dean of Students. Note that funds are also available for students experiencing immigration-related financial stress.\n\n\n\nImmigration Status\nCUNY Citizenship Now! provides confidential, high-quality immigration law services to all CUNY students.\nNote that Citizenship Now!’s primary Manhattan office is located in the Heights, not on the Baruch campus and that an appointment is strongly recommended. Call 646-664-9350 during standard business hours for more information or to make an appointment"
  },
  {
    "objectID": "resources.html#footnotes",
    "href": "resources.html#footnotes",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDescriptions of Baruch and CUNY resources adapted from program websites.↩︎\nLanguage adapted from Professor Ryan Tibshirani (UC Berkeley).↩︎"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "STA 9750 - Basic Software Tools for Data Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInstructions for how to do so can be found here. If you use another calendar system that cannot accept CSV files in this format, let me know and I will try to provide additional file formats.↩︎"
  },
  {
    "objectID": "test.html#my-text.",
    "href": "test.html#my-text.",
    "title": "Untitled",
    "section": "",
    "text": "Last updated: r format(Sys.time(), \"%A %B %d, %Y at %I:%M %p\") by {r} 1+1.\nFish and such"
  },
  {
    "objectID": "slides/slides02.html#week-update",
    "href": "slides/slides02.html#week-update",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "",
    "text": "Today: 2025-02-06\n\nWeekly feature\nBrief updates and reminders about course logistics\nSyllabus and Brightspace are binding\n\nIf something is left out of here, it still happens!"
  },
  {
    "objectID": "slides/slides02.html#course-administration-course-project-draft-released",
    "href": "slides/slides02.html#course-administration-course-project-draft-released",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Course Administration: Course Project Draft Released",
    "text": "Course Administration: Course Project Draft Released\nCourse project draft description is now online\nDetailed discussion of:\n\nProject structure\nKey deadlines\nGrading rubrics\n\nWill be finalized next week - 2025-02-13.\nPlease send me questions in advance!\nFirst step: by 2025-03-05, email me your group members."
  },
  {
    "objectID": "slides/slides02.html#next-week",
    "href": "slides/slides02.html#next-week",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Next Week",
    "text": "Next Week\n\nPre-Assignment #03 due\nMini-Project #00 due\nMini-Project #00 peer feedback assigned\nMini-Project #01 released\nCourse Project officially released\n\nSpecial presentation on Baruch data resources"
  },
  {
    "objectID": "slides/slides02.html#looking-ahead",
    "href": "slides/slides02.html#looking-ahead",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nCourse Project:\n\nStart looking for teammates and topics\nNo in-person office hours on 2025-02-18 (MW on travel)"
  },
  {
    "objectID": "slides/slides02.html#life-on-the-command-line",
    "href": "slides/slides02.html#life-on-the-command-line",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Life on the Command Line",
    "text": "Life on the Command Line"
  },
  {
    "objectID": "slides/slides02.html#rstudio---a-useful-ide",
    "href": "slides/slides02.html#rstudio---a-useful-ide",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "RStudio - A Useful IDE",
    "text": "RStudio - A Useful IDE\nOfficial Cheat Sheets:\n\nRStudio Cheat Sheet\nQuarto Cheat Sheet\n\nData Camp RStudio Tutorial (Free)\n\nFor today, first ~half"
  },
  {
    "objectID": "slides/slides02.html#r-repl",
    "href": "slides/slides02.html#r-repl",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "R REPL",
    "text": "R REPL"
  },
  {
    "objectID": "slides/slides02.html#terminal",
    "href": "slides/slides02.html#terminal",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Terminal",
    "text": "Terminal"
  },
  {
    "objectID": "slides/slides02.html#life-tip-of-the-week",
    "href": "slides/slides02.html#life-tip-of-the-week",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Life Tip of the Week",
    "text": "Life Tip of the Week\nIt’s time to start preparing your taxes. (I know, I know …)\n. . .\n\nPreparing is not the same as filing\n\nPreparing is doing the calculations\nFiling is submitting to IRS\n\nEmployers and financial institutions should be sending you documents (W2, 1099, etc.)\n\nEasier to use them now so you don’t lose them\n\nBenefits of starting early:\n\nIf you get a refund, great!\nIf you owe money, avoid nasty surprise.\n\nYou can still make certain 2024 tax moves and get the tax benefit(IRA, HSA, etc.)\n\n. . .\nIf your income is less than ~$98K single or ~$113K married, the IRS FreeFile program means you can use TaxAct, etc. for free.1"
  },
  {
    "objectID": "advice/taxes.html",
    "href": "advice/taxes.html",
    "title": "STA 9750",
    "section": "",
    "text": "It’s time to start preparing your taxes. (I know, I know …)\n. . .\n\nPreparing is not the same as filing\nEmployers and financial institutions should be sending you documents (W2, 1099, etc.)\nBenefits of starting early:\n\nIf you get a refund, great!\nIf you owe money, avoid nasty surprise.\n\nYou can still make certain 2024 tax moves and get the tax benefit(IRA, HSA, etc.)\n\n. . .\nIf your income is less than ~$98K single or ~$113K married, the IRS FreeFile program means you can use TaxAct, etc. for free.1"
  },
  {
    "objectID": "advice/taxes.html#life-tip-of-the-week",
    "href": "advice/taxes.html#life-tip-of-the-week",
    "title": "STA 9750",
    "section": "",
    "text": "It’s time to start preparing your taxes. (I know, I know …)\n. . .\n\nPreparing is not the same as filing\nEmployers and financial institutions should be sending you documents (W2, 1099, etc.)\nBenefits of starting early:\n\nIf you get a refund, great!\nIf you owe money, avoid nasty surprise.\n\nYou can still make certain 2024 tax moves and get the tax benefit(IRA, HSA, etc.)\n\n. . .\nIf your income is less than ~$98K single or ~$113K married, the IRS FreeFile program means you can use TaxAct, etc. for free.1"
  },
  {
    "objectID": "advice/taxes.html#footnotes",
    "href": "advice/taxes.html#footnotes",
    "title": "STA 9750",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese companies don’t advertise this widely, but they have a long-standing relationship with the IRS that requires this. Use the links on irs.gov to get routed to the right place↩︎"
  },
  {
    "objectID": "slides/slides03.html",
    "href": "slides/slides03.html",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "",
    "text": "Introduction to Course Project\nSpecial Presentation\nCourse Administration\nR: Data Frames, Functions, Packages and Control Flow"
  },
  {
    "objectID": "slides/slides03.html#finding-data",
    "href": "slides/slides03.html#finding-data",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Finding Data",
    "text": "Finding Data\n\nStart early!\nNYC Open Data is great\n\nSee also: FRED, Federal Open Data, Nasa EarthData, Kaggle\nAsk on Piazza for pointers\nLots of data hidden in Wikipedia\n\nNothing paid or private without express instructor submission\nEveryone loves spatial data!"
  },
  {
    "objectID": "slides/slides03.html#presentation-hints",
    "href": "slides/slides03.html#presentation-hints",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Presentation Hints",
    "text": "Presentation Hints\n\nLongest time \\(\\neq\\) most important\nStory, story, story! Why are you making these choices?\nHourglass Structure\n\nStart big\nMotivate your overarching question\nSpecific questions\nTie specific to overarching\nFrom overarching back to big motivation\n\nNo less than one figure every other slide"
  },
  {
    "objectID": "slides/slides03.html#course-project-1",
    "href": "slides/slides03.html#course-project-1",
    "title": "STA 9750 - Week 3 Update",
    "section": "Course Project",
    "text": "Course Project\n\n1 team already registered with me!\nPiazza discussions helping to coordinate other teams"
  },
  {
    "objectID": "test.html#big-section",
    "href": "test.html#big-section",
    "title": "My Test Document",
    "section": "Big Section",
    "text": "Big Section\nText\n\nLittle Section\nLittle text"
  },
  {
    "objectID": "testslides.html#this-is-a-header",
    "href": "testslides.html#this-is-a-header",
    "title": "Test Presentation",
    "section": "This is a header",
    "text": "This is a header\nsome content"
  },
  {
    "objectID": "testslides.html#this-is-a-second-header",
    "href": "testslides.html#this-is-a-second-header",
    "title": "Test Presentation",
    "section": "This is a second header",
    "text": "This is a second header"
  },
  {
    "objectID": "miniprojects/mini00.html#step-0-install-git",
    "href": "miniprojects/mini00.html#step-0-install-git",
    "title": "STA 9750 Mini-Project #00: Course Set-Up",
    "section": "Step 0: Install git",
    "text": "Step 0: Install git\nYou first need to make sure you have git installed on your computer. You may already have it, but if you don’t the following steps can make sure it is installed. To check if it is installed, open a Terminal and run git (Mac) or git.exe (Windows). If you get a message other than “not found”, you should be good to go.\nFor more detailed instructions, see Chapter 6 of the Happy Git with R book.\n\nWindows\nIf you are on a Windows machine, install git using the Git for Windows bundle.\n\n\nMac\nIf you are on a Mac, run the following command at the Terminal:\n\nxcode-select --install\n\nThis will prompt you to install the “XCode Command Line Tools” which include git."
  },
  {
    "objectID": "slides/slides07.html",
    "href": "slides/slides07.html",
    "title": "STA 9750 - Week 7",
    "section": "",
    "text": "Grades returned this afternoon.\nReview regrade policy and late work policy if you have questions."
  },
  {
    "objectID": "slides/slides05.html",
    "href": "slides/slides05.html",
    "title": "STA 9750 - Week 5 Update",
    "section": "",
    "text": "Submission due yesterday at 11:45pm\n\n\\(\\approx 90\\%\\) submitted on time\nSubmit early and submit often\n\nLess “last minute” tech support going forward\n\nUse Piazza and use your peers\n\nVery impressed by Detailed Analyses, Code Folding and Callout Blocks, Fancy gt Tables, Graphics"
  },
  {
    "objectID": "slides/slides06.html",
    "href": "slides/slides06.html",
    "title": "STA 9750 - Week 6 Update",
    "section": "",
    "text": "👏 Thank you to everyone who took part in peer feedback! 👏\nCharles and I have marked all grades - expect on Brightspace soon.\n(Tech issues on our end…)\n. . .\nAlso - let’s thank Charles for going through every post manually with reminders 👏"
  },
  {
    "objectID": "preassigns/pa13.html",
    "href": "preassigns/pa13.html",
    "title": "STA 9750 Week 13 Pre Assignment: Introduction to tidymodels",
    "section": "",
    "text": "Due Date: 2025-05-07 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace"
  },
  {
    "objectID": "testwebr.html",
    "href": "testwebr.html",
    "title": "TEST",
    "section": "",
    "text": "The blank should be filled with 5\n\n\n3 + 4 + 5\n3 + 4 + 5\n\n\n\n\n\n\nWait for the exercise to fully load (the blue dot next to Run Code will disappear) and then try giving correct and incorrect solutions.\n\nReplace the blank (underscores) with the number 5 and hit Run Code to check your solution. If all goes well, you’ll see a happy message.\nNow, replace the 5 with another number, e.g., 12. Hit Run Code again and see what the response you get."
  },
  {
    "objectID": "preassigns/testwebr.html",
    "href": "preassigns/testwebr.html",
    "title": "TEST",
    "section": "",
    "text": "The blank should be filled with 5\n\n\n3 + 4 + 5\n3 + 4 + 5\n\n\n\n\n\n\nWait for the exercise to fully load (the blue dot next to Run Code will disappear) and then try giving correct and incorrect solutions.\n\nReplace the blank (underscores) with the number 5 and hit Run Code to check your solution. If all goes well, you’ll see a happy message.\nNow, replace the 5 with another number, e.g., 12. Hit Run Code again and see what the response you get."
  },
  {
    "objectID": "test.html#i-am-salvatore-davi.",
    "href": "test.html#i-am-salvatore-davi.",
    "title": "STA9750",
    "section": "",
    "text": "Last Updated"
  },
  {
    "objectID": "test.html#i-am-salvatore-davi.-current-student-in-the-class-of-sta-9750-in-the-spring-2025-semester.",
    "href": "test.html#i-am-salvatore-davi.-current-student-in-the-class-of-sta-9750-in-the-spring-2025-semester.",
    "title": "STA9750",
    "section": "",
    "text": "quarto preview “Main site.qmd” –to html –no-watch-inputs –no-browse –log-level debug\n\nLast Updated: Tuesday 02 11, 2025 at 12:%p"
  },
  {
    "objectID": "slides/slides03.html#today-1",
    "href": "slides/slides03.html#today-1",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "",
    "text": "Introduction to Course Project\nSpecial Presentation\nCourse Administration\nR: Data Frames, Functions, Packages and Control Flow"
  },
  {
    "objectID": "slides/slides03.html#mp00-submissions",
    "href": "slides/slides03.html#mp00-submissions",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "MP#00 Submissions",
    "text": "MP#00 Submissions\nI now have GitHub IDs for 22 of you.\n\nIf you are in the 6 who have not sent me your GitHub ID and received my acknowledgement please do so immediately.\nWill follow up with a few of you for VoE directly\nGreat submissions: Resumes, Bios, Flowcharts (creative!), Headshots"
  },
  {
    "objectID": "slides/slides03.html#mp00-peer-feedback",
    "href": "slides/slides03.html#mp00-peer-feedback",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "MP#00 Peer Feedback",
    "text": "MP#00 Peer Feedback\nDue to a constraint of GitHub’s API, need to do issue creation and tagging on my STA9750-2025-SPRING repository.\n. . .\nInstructions updated for MP#01-#04 already.\n\nHelper scripts to be updated as well\n\nFor MP#00, I am hand-copying all issues to my repo and assigning peer feedback there (done by noon tomorrow)"
  },
  {
    "objectID": "slides/slides03.html#mp00-peer-feedback-1",
    "href": "slides/slides03.html#mp00-peer-feedback-1",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "MP#00 Peer Feedback",
    "text": "MP#00 Peer Feedback\n3 evaluators per submission\nGoals:\n\nPractice commenting on GitHub and reviewing code (ungraded assignment)\nLearn tricks to improve your own site\n\n\n“Good artists copy; great artists steal.” – Steve Jobs"
  },
  {
    "objectID": "slides/slides03.html#mp01-extended-deadlines",
    "href": "slides/slides03.html#mp01-extended-deadlines",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "MP#01 Extended Deadlines",
    "text": "MP#01 Extended Deadlines\n\nReleased: today (unchanged)\nDue: 2025-03-05 (extend one week)\nPeer Feedback Assigned: 2025-03-06 (shift one week)\nPeer Feedback Due: 2025-03-12 (shift one week)\n\nMP#01 Peer Feedback is day before Project Proposals\n\nNo Pre-Assignment that week"
  },
  {
    "objectID": "slides/slides03.html#mp00-goals",
    "href": "slides/slides03.html#mp00-goals",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "MP#00 Goals",
    "text": "MP#00 Goals\nMP#00 is ungraded to:\n\nSort out tech trouble\nShow danger of waiting to the last minute\nDemonstrate importance of closely following instructions Hopefully lessons learned will be helpful in future assignments"
  },
  {
    "objectID": "slides/slides03.html#data-frames",
    "href": "slides/slides03.html#data-frames",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Data Frames",
    "text": "Data Frames"
  },
  {
    "objectID": "slides/slides03.html#function-calls",
    "href": "slides/slides03.html#function-calls",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Function Calls",
    "text": "Function Calls"
  },
  {
    "objectID": "slides/slides03.html#packages",
    "href": "slides/slides03.html#packages",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Packages",
    "text": "Packages"
  },
  {
    "objectID": "slides/slides03.html#control-flow",
    "href": "slides/slides03.html#control-flow",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Control Flow",
    "text": "Control Flow"
  },
  {
    "objectID": "slides/slides03.html#looking-ahead",
    "href": "slides/slides03.html#looking-ahead",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nCourse Project:\n\nStart looking for teammates and topics\nNo in-person office hours on 2025-02-18 (MW on travel)"
  },
  {
    "objectID": "slides/slides03.html#life-tip-of-the-week",
    "href": "slides/slides03.html#life-tip-of-the-week",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Life Tip of the Week",
    "text": "Life Tip of the Week\n\nRecommendations from Professors\nIn the near future, you may want recommendations from your professors. Some advice on getting good recommendations:\n\nUnderstand professional vs academic recommendations\nSeek out strong recommendations\nConfidential recommendations highly preferrable"
  },
  {
    "objectID": "slides/slides03.html#professional-vs-academic-recommendations",
    "href": "slides/slides03.html#professional-vs-academic-recommendations",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Professional vs Academic Recommendations",
    "text": "Professional vs Academic Recommendations\n\nProfessional: For a job. Typically a brief survey or quick reference check.\nAcademic: For awards, fellowships, research positions, graduate admissions. A personalized letter from me.\n\nI can usually do professional recommendations quickly (4-5 business days). Academic recommendations require a minimum of two weeks."
  },
  {
    "objectID": "slides/slides03.html#strong-recommendations",
    "href": "slides/slides03.html#strong-recommendations",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Strong Recommendations",
    "text": "Strong Recommendations\nA strong recommendation is one that supplements what is already in your application.\n\nSay things that aren’t on your transcript\nAttest to character and quality of work\n\nThe better I know you, the stronger a letter I can write.\nYou can ask if a professor can write you a strong recommendation. If they say they can write, but it may not be strong, consider asking elsewhere."
  },
  {
    "objectID": "slides/slides03.html#strong-recommendations-1",
    "href": "slides/slides03.html#strong-recommendations-1",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Strong Recommendations",
    "text": "Strong Recommendations\nUgly:\n\nStudent N took my class and received an A.\n\nBad:\n\nStudent N took my XYZ class and received an A. My XYZ class covers ABC at an advanced level and provides N a strong foundation for your program.\n\nGood:\n\nI first met student N in my XYZ class. She was an active participant in class, regularly attended my office hours, and regularly asked for additional advanced material. She had a remarkable final project in which she ABC. It is clear that her drive and passion for XYZ make her an excellent candidate for your fellowship."
  },
  {
    "objectID": "slides/slides03.html#strong-recommendations-2",
    "href": "slides/slides03.html#strong-recommendations-2",
    "title": "STA 9750 - Week 3 Update",
    "section": "Strong Recommendations",
    "text": "Strong Recommendations\nDistinct from a good recommendation."
  },
  {
    "objectID": "slides/slides03.html#confidentiality",
    "href": "slides/slides03.html#confidentiality",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "Confidentiality",
    "text": "Confidentiality\nUnder some parts of US law, you have the right to see what letters for you I write unless you specifically waive that right.\nWaive the right\n. . .\nReaders will assume you have read a non-confidential letter and discount whatever I say."
  },
  {
    "objectID": "slides/slides03.html#my-policy",
    "href": "slides/slides03.html#my-policy",
    "title": "STA 9750  Week 3 Update  2025-02-13",
    "section": "My Policy",
    "text": "My Policy\nI will write a letter for any student who has passed one of my classes (C or higher) or is on track to do so.\nNo guarantee it is strong unless you ask. I understand that sometimes you are just trying to make sure you have enough.\nGive me at least 3 weeks, ideally more. A rushed letter is a short and generic letter.\nSend me:\n\nDescription of who I’m writing to\nYour CV / Resume\nYour latest transcript\nPresentations/project materials from my class\n\nOnce I have a letter written, it’s easier for me to update and re-use.\nHelp me help you."
  },
  {
    "objectID": "resources.html#additional-baruch-and-cuny-benefits",
    "href": "resources.html#additional-baruch-and-cuny-benefits",
    "title": "STA 9750 - Course Policies & Additional Resources",
    "section": "Additional Baruch and CUNY Benefits",
    "text": "Additional Baruch and CUNY Benefits\nAs Baruch students, you have access to a variety of cultural and education benefits including:\n\nFree New York Times subscription\nFree Wall Street Journal subscription\nFree Barron’s Subscription (Zicklin only)\nFree and Discounted Museum Admissions\nDiscounted Carnegie Hall performances\nGitHub Student Developer Pack\nFree and Discounted Amazon Prime\n\namong many others.\nMore will posted as I discover them."
  },
  {
    "objectID": "slides/slides04.html#mini-project-00---peer-feedback",
    "href": "slides/slides04.html#mini-project-00---peer-feedback",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Mini-Project #00 - Peer Feedback",
    "text": "Mini-Project #00 - Peer Feedback\nOver 75% of the class reported receiving useful peer feedback.\n\nInstructor’s Note: For graded MPs #01-04, be a bit more direct in peer feedback. Goal is to help your peers improve: constructive criticism.\n\n\nWhen submitting peer feedback on graded MPs, use comment template.\n\n\nIf you didn’t get useful peer feedback on MP#00, please post a follow-up comment in your thread and I’ll take a look."
  },
  {
    "objectID": "slides/slides04.html#mini-project-00---pay-attention-to-the-details",
    "href": "slides/slides04.html#mini-project-00---pay-attention-to-the-details",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Mini-Project #00 - Pay Attention to the Details",
    "text": "Mini-Project #00 - Pay Attention to the Details\nAt least one submission had title “YOUR TITLE GOES HERE”"
  },
  {
    "objectID": "slides/slides04.html#mini-project-helper-scripts",
    "href": "slides/slides04.html#mini-project-helper-scripts",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Mini-Project Helper Scripts",
    "text": "Mini-Project Helper Scripts\nRemember course helper functions\n\nmp_submission_create - Open an issue for your submission\nmp_submission_verify - Check that your issue is formatted and page is available for review\nmp_feedback_locate - Find issues on which you’re being asked to comment\nmp_feedback_verify - Check that your peer feedback comments are formatted"
  },
  {
    "objectID": "slides/slides04.html#mp-01",
    "href": "slides/slides04.html#mp-01",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "MP #01",
    "text": "MP #01\nHappy to see folks already getting started!\n\nA bit of debugging of network connection issues (possibly transient)\nTreatment of OT for per Annum and per diem employees\n\nGreat questions on this (HZ😎) - Piazza pinned\n\n\nNot everything has a single right answer - be reasonable, justify, and document"
  },
  {
    "objectID": "slides/slides04.html#today-1",
    "href": "slides/slides04.html#today-1",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Today",
    "text": "Today\nHello from New Mexico!\nI’m on hotel Wifi, so if I drop, hold on a bit and I’ll rejoin.\n\n\nCourse Administration\nPA#04 FAQs\nSingle-Table Verbs\nWrap-Up\n\nLife Tip of the Day"
  },
  {
    "objectID": "slides/slides04.html#looking-ahead",
    "href": "slides/slides04.html#looking-ahead",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Looking Ahead",
    "text": "Looking Ahead"
  },
  {
    "objectID": "slides/slides04.html#life-tip-of-the-week",
    "href": "slides/slides04.html#life-tip-of-the-week",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Life Tip of the Week",
    "text": "Life Tip of the Week\nZSB / Baruch / CUNY Benefits\nAs a student, you have many free and discounted benefits.\nI have collected some of these on the course page, but there are many more if you look."
  },
  {
    "objectID": "slides/slides04.html#professional-vs-academic-recommendations",
    "href": "slides/slides04.html#professional-vs-academic-recommendations",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Professional vs Academic Recommendations",
    "text": "Professional vs Academic Recommendations\n\nProfessional: For a job. Typically a brief survey or quick reference check.\nAcademic: For awards, fellowships, research positions, graduate admissions. A personalized letter from me.\n\nI can usually do professional recommendations quickly (4-5 business days). Academic recommendations require a minimum of two weeks."
  },
  {
    "objectID": "slides/slides04.html#strong-recommendations",
    "href": "slides/slides04.html#strong-recommendations",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Strong Recommendations",
    "text": "Strong Recommendations\nA strong recommendation is one that supplements what is already in your application.\n\nSay things that aren’t on your transcript\nAttest to character and quality of work\n\nThe better I know you, the stronger a letter I can write.\nYou can ask if a professor can write you a strong recommendation. If they say they can write, but it may not be strong, consider asking elsewhere."
  },
  {
    "objectID": "slides/slides04.html#strong-recommendations-1",
    "href": "slides/slides04.html#strong-recommendations-1",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Strong Recommendations",
    "text": "Strong Recommendations\nUgly:\n\nStudent N took my class and received an A.\n\nBad:\n\nStudent N took my XYZ class and received an A. My XYZ class covers ABC at an advanced level and provides N a strong foundation for your program.\n\nGood:\n\nI first met student N in my XYZ class. She was an active participant in class, regularly attended my office hours, and regularly asked for additional advanced material. She had a remarkable final project in which she ABC. It is clear that her drive and passion for XYZ make her an excellent candidate for your fellowship."
  },
  {
    "objectID": "slides/slides04.html#confidentiality",
    "href": "slides/slides04.html#confidentiality",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Confidentiality",
    "text": "Confidentiality\nUnder some parts of US law, you have the right to see what letters for you I write unless you specifically waive that right.\nWaive the right\n\nReaders will assume you have read a non-confidential letter and discount whatever I say."
  },
  {
    "objectID": "slides/slides04.html#my-policy",
    "href": "slides/slides04.html#my-policy",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "My Policy",
    "text": "My Policy\nI will write a letter for any student who has passed one of my classes (C or higher) or is on track to do so.\nNo guarantee it is strong unless you ask. I understand that sometimes you are just trying to make sure you have enough.\nGive me at least 3 weeks, ideally more. A rushed letter is a short and generic letter.\nSend me:\n\nDescription of who I’m writing to\nYour CV / Resume\nYour latest transcript\nPresentations/project materials from my class\n\nOnce I have a letter written, it’s easier for me to update and re-use.\nHelp me help you."
  },
  {
    "objectID": "slides/slides04.html#cuny-wide",
    "href": "slides/slides04.html#cuny-wide",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "CUNY-Wide",
    "text": "CUNY-Wide\n\nFree New York Times and Wall Street Journal\nFree and Discounted Museum Access via CUNY Arts\nDiscounted Broadway and Off-Broadway via TDF"
  },
  {
    "objectID": "slides/slides04.html#baruch-zsb",
    "href": "slides/slides04.html#baruch-zsb",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Baruch / ZSB",
    "text": "Baruch / ZSB\n\nFree Barron’s Subscription\nNewman Library Databases"
  },
  {
    "objectID": "slides/slides04.html#any-student",
    "href": "slides/slides04.html#any-student",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "Any Student",
    "text": "Any Student\n\nFree Trial and Discounted Rate Amazon Prime\nDiscounted Spotify + Free Hulu Subscription\nGitHub Student Developer Pack"
  },
  {
    "objectID": "slides/slides04.html#mp-01-1",
    "href": "slides/slides04.html#mp-01-1",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "MP #01",
    "text": "MP #01\nHow to deal with messy / incorrect data?\n\nProcess it intensely\nGo ‘robust’"
  },
  {
    "objectID": "slides/slides04.html#faq-filter-vs-group_by-1",
    "href": "slides/slides04.html#faq-filter-vs-group_by-1",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: filter vs group_by",
    "text": "FAQ: filter vs group_by\nNo group_by - full summarization:\n\npenguins |&gt; drop_na() |&gt; summarize(mean(body_mass_g))\n\n# A tibble: 1 × 1\n  `mean(body_mass_g)`\n                &lt;dbl&gt;\n1               4207.\n\n\nWith group_by - summary within groups.\n\npenguins |&gt; drop_na() |&gt; group_by(species) |&gt; summarize(mean(body_mass_g))\n\n# A tibble: 3 × 2\n  species   `mean(body_mass_g)`\n  &lt;fct&gt;                   &lt;dbl&gt;\n1 Adelie                  3706.\n2 Chinstrap               3733.\n3 Gentoo                  5092."
  },
  {
    "objectID": "slides/slides04.html#faq-filter-vs-group_by-2",
    "href": "slides/slides04.html#faq-filter-vs-group_by-2",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: filter vs group_by",
    "text": "FAQ: filter vs group_by\nWith multiple grouping - “cross-tabs” of results:\n\npenguins |&gt; drop_na() |&gt; group_by(species, sex) |&gt; summarize(mean(body_mass_g))\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    `mean(body_mass_g)`\n  &lt;fct&gt;     &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie    female               3369.\n2 Adelie    male                 4043.\n3 Chinstrap female               3527.\n4 Chinstrap male                 3939.\n5 Gentoo    female               4680.\n6 Gentoo    male                 5485.\n\n\nNote that result of multi-group_by is still grouped:\n\npenguins |&gt; drop_na() |&gt; group_by(species, sex) |&gt; summarize(mean(body_mass_g))\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    `mean(body_mass_g)`\n  &lt;fct&gt;     &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie    female               3369.\n2 Adelie    male                 4043.\n3 Chinstrap female               3527.\n4 Chinstrap male                 3939.\n5 Gentoo    female               4680.\n6 Gentoo    male                 5485."
  },
  {
    "objectID": "slides/slides04.html#faq-filter-vs-group_by-3",
    "href": "slides/slides04.html#faq-filter-vs-group_by-3",
    "title": "STA 9750  Week 4 Update  2025-02-20",
    "section": "FAQ: filter vs group_by",
    "text": "FAQ: filter vs group_by\nChanges next call to summarize:\n\npenguins |&gt; drop_na() |&gt; group_by(species) |&gt; \n    summarize(mbmg = mean(body_mass_g)) |&gt; summarize(mean(mbmg))\n\n# A tibble: 1 × 1\n  `mean(mbmg)`\n         &lt;dbl&gt;\n1        4177.\n\n\n\npenguins |&gt; drop_na() |&gt; group_by(species, sex) |&gt; \n    summarize(mbmg = mean(body_mass_g)) |&gt; summarize(mean(mbmg))\n\n# A tibble: 3 × 2\n  species   `mean(mbmg)`\n  &lt;fct&gt;            &lt;dbl&gt;\n1 Adelie           3706.\n2 Chinstrap        3733.\n3 Gentoo           5082.\n\n\n\npenguins |&gt; drop_na() |&gt; group_by(sex, species) |&gt; \n    summarize(mbmg = mean(body_mass_g)) |&gt; summarize(mean(mbmg))\n\n# A tibble: 2 × 2\n  sex    `mean(mbmg)`\n  &lt;fct&gt;         &lt;dbl&gt;\n1 female        3859.\n2 male          4489."
  },
  {
    "objectID": "slides/slides02.html",
    "href": "slides/slides02.html",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "",
    "text": "Today: 2025-02-06\n\nWeekly feature\nBrief updates and reminders about course logistics\nSyllabus and Brightspace are binding\n\nIf something is left out of here, it still happens!"
  },
  {
    "objectID": "slides/slides02.html#footnotes",
    "href": "slides/slides02.html#footnotes",
    "title": "STA 9750  Week 2 Update  2025-02-06",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese companies don’t advertise this widely, but they have a long-standing relationship with the IRS that requires this. Use the links on irs.gov to get routed to the right place↩︎"
  },
  {
    "objectID": "slides/slides05.html#today-1",
    "href": "slides/slides05.html#today-1",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Today",
    "text": "Today\n\nCourse Administration\nPA#05 Review\nPA#05 FAQs\nDiving Deeper into Multi-Table Verbs\nWrap-Up\n\nLife Tip of the Day"
  },
  {
    "objectID": "slides/slides05.html#upcoming-two-weeks",
    "href": "slides/slides05.html#upcoming-two-weeks",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Upcoming Two Weeks",
    "text": "Upcoming Two Weeks\nNext Wednesday (March 5th) at 11:45pm:\n\nMP#01 Initial Submission due\nTeam membership due\n\n\nNo class on March 6th\n\n\nFollowing Wednesday (March 12th) at 11:45pm:\n\nMP#01 Peer Feedback Due\nNo Pre-Assignment\n\n\n\nMarch 13th: Proposal Presentations"
  },
  {
    "objectID": "slides/slides05.html#march-13th---project-proposal-presentations",
    "href": "slides/slides05.html#march-13th---project-proposal-presentations",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "March 13th - Project Proposal Presentations",
    "text": "March 13th - Project Proposal Presentations\nOfficial Description\n\n6 minute presentation\nKey topics:\n\nAnimating Question\nTeam Roster\n\nAlso discuss: Possible specific questions, data sources, analytical plan, anticipated challenges"
  },
  {
    "objectID": "slides/slides05.html#mini-project-01-1",
    "href": "slides/slides05.html#mini-project-01-1",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\nMake sure your code is included in your submission\n\nCode-Folding is very useful\n\n\nFollow submission instructions\n\nYou need to have mp01.qmd and docs/mp01.html in your STA9750-2025-SPRING repository\nHelper function can verify submission is properly formatted"
  },
  {
    "objectID": "slides/slides05.html#mini-project-01---peer-feedback-1",
    "href": "slides/slides05.html#mini-project-01---peer-feedback-1",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "STA 9750 Mini-Project #01 - Peer Feedback",
    "text": "STA 9750 Mini-Project #01 - Peer Feedback\nSubmissions may not map perfectly to rubric - use your best judgement\n\nLearn from this! What can you adapt for MP#02?"
  },
  {
    "objectID": "slides/slides05.html#multi-table-analysis",
    "href": "slides/slides05.html#multi-table-analysis",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Multi-Table Analysis",
    "text": "Multi-Table Analysis\nMultiple Tables:\n\nMore insights than from a single table\nMaintain ‘tidy’ structure throughout\n\nWill create new (compound) rows - be ware of accidental over-duplication"
  },
  {
    "objectID": "slides/slides05.html#review-of-joins",
    "href": "slides/slides05.html#review-of-joins",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Review of Joins",
    "text": "Review of Joins"
  },
  {
    "objectID": "slides/slides05.html#inner-joins",
    "href": "slides/slides05.html#inner-joins",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Inner Joins",
    "text": "Inner Joins"
  },
  {
    "objectID": "slides/slides05.html#outer-joins",
    "href": "slides/slides05.html#outer-joins",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Outer Joins",
    "text": "Outer Joins"
  },
  {
    "objectID": "slides/slides05.html#life-tip-of-the-week",
    "href": "slides/slides05.html#life-tip-of-the-week",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Life Tip of the Week",
    "text": "Life Tip of the Week\nWest Virginia State Board of Education v. Barnette (March 11, 1943)\n\nStudents cannot be compelled to recite the Pleldge of Allegiance, even during a period of war\nIconic First Amendment Victory"
  },
  {
    "objectID": "slides/slides05.html#barnette",
    "href": "slides/slides05.html#barnette",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Barnette",
    "text": "Barnette\nJustice Jackson’s Opinion (6-3):\n\nIf there is any fixed star in our constitutional constellation, it is that no official, high or petty, can prescribe what shall be orthodox in politics, nationalism, religion, or other matters of opinion or force citizens to confess by word or act their faith therein."
  },
  {
    "objectID": "slides/slides05.html#story-behind-the-case",
    "href": "slides/slides05.html#story-behind-the-case",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Story Behind the Case",
    "text": "Story Behind the Case\n1940 Case Minersville School District v. Gobitis\n\nJW Students in PA refused to recite Pledge of Allegience\n\n\nJustice Frankfurter (8-1 Majority Opinion):\n\nNational Unity is the basis of National Security\n\nStudents could be forced to pledge\n\n\n\nAfter the decision, waves of violence against JW students and adults accused of “treason” against the war effort"
  },
  {
    "objectID": "slides/slides05.html#story-behind-the-case-1",
    "href": "slides/slides05.html#story-behind-the-case-1",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Story Behind the Case",
    "text": "Story Behind the Case\nJustice Stone (Dissent):\n\n[T]he guarantees of civil liberty are but guarantees of freedom of the human mind and spirit and of reasonable freedom and opportunity to express them. [… T]he very essence of the liberty which they guarantee is the freedom of the individual from compulsion as to what he shall think and what he shall say.\n\n\nA few years later, changed court wanted to revisit, leading to Barnette"
  },
  {
    "objectID": "slides/slides05.html#more-from-jackson",
    "href": "slides/slides05.html#more-from-jackson",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "More from Jackson",
    "text": "More from Jackson\n\n[F]reedom to differ is not limited to things that do not matter much. That would be a mere shadow of freedom. The test of its substance is the right to differ as to things that touch the heart of the existing order."
  },
  {
    "objectID": "slides/slides05.html#more-from-jackson-1",
    "href": "slides/slides05.html#more-from-jackson-1",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "More from Jackson",
    "text": "More from Jackson\n\n[F]reedom to differ is not limited to things that do not matter much. That would be a mere shadow of freedom. The test of its substance is the right to differ as to things that touch the heart of the existing order."
  },
  {
    "objectID": "slides/slides05.html#lessons",
    "href": "slides/slides05.html#lessons",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Lessons",
    "text": "Lessons\n\nWe get things wrong, often very wrong, in times of public fear\n\n\n\nLaw of Free Speech is necessary but not sufficient for a Culture of Free Speech\n\n\n\n\nFreedom to Dissent is at the core of a pluralistic society\n\n\n\n\nRules and norms exist for the hard cases, not the easy ones"
  },
  {
    "objectID": "slides/slides05.html#primary-keys",
    "href": "slides/slides05.html#primary-keys",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Primary Keys",
    "text": "Primary Keys\nKeys are unique identifiers for individual records\n\nPrimary (one column) or compound (multiple columns together)\n\n\nThe history of corporate IT is largely one of (failed) primary keys\n\nFinance: Tickers, Tickers + Exchange, Tickers + Share Class, CUSIP, ISIN, SEDOL, …\n\n\n\nMeaningful true keys are vanishingly rare - cherish them when you find them\nOften ‘unique enough’ for an analysis\ndplyr::count is helpful here"
  },
  {
    "objectID": "slides/slides05.html#joins",
    "href": "slides/slides05.html#joins",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Joins",
    "text": "Joins\nJoins combine tables by identity - not simple ‘stacking’\nSpecify a join key - ideally this is an actual key, but doesn’t have to be\nIn dplyr, we use the join_by function:\n\njoin_by(table_1_name == table_2_name)\n\nHere table_1_name and table_2_name are column names from two tables\n\nJoin rows where these values are equal (advanced joins possible)"
  },
  {
    "objectID": "slides/slides05.html#inner-and-outer-joins",
    "href": "slides/slides05.html#inner-and-outer-joins",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Inner and Outer Joins",
    "text": "Inner and Outer Joins\nWhen tables are perfectly matched, not an issue:\n\nlibrary(dplyr)\nx &lt;- tribble(~college, ~campus_borough, \n             \"CCNY\", \"Manhattan\",\n             \"Baruch\", \"Manhattan\", \n             \"CSI\", \"Staten Island\",\n             \"York\", \"Queens\")\n\ny &lt;- tribble(~borough_name, ~bus_code,\n             \"Manhattan\", \"M\",\n             \"Staten Island\", \"S\", \n             \"Queens\", \"Q\")\n\ninner_join(x, y, join_by(campus_borough == borough_name))\n\n# A tibble: 4 × 3\n  college campus_borough bus_code\n  &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;   \n1 CCNY    Manhattan      M       \n2 Baruch  Manhattan      M       \n3 CSI     Staten Island  S       \n4 York    Queens         Q       \n\n\nDefault to inner but irrelevant\nNote automatic repetition of \"M\" row"
  },
  {
    "objectID": "slides/slides05.html#inner-and-outer-joins-1",
    "href": "slides/slides05.html#inner-and-outer-joins-1",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Inner and Outer Joins",
    "text": "Inner and Outer Joins\nHow to handle ‘unaligned’ values?\n\nx &lt;- tribble(~college, ~campus_borough, \n             \"CCNY\", \"Manhattan\",\n             \"Baruch\", \"Manhattan\", \n             \"CSI\", \"Staten Island\",\n             \"York\", \"Queens\", \n             \"Medgar Evers\", \"Brooklyn\")\n\ninner_join(x, y, join_by(campus_borough == borough_name))\n\n# A tibble: 4 × 3\n  college campus_borough bus_code\n  &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;   \n1 CCNY    Manhattan      M       \n2 Baruch  Manhattan      M       \n3 CSI     Staten Island  S       \n4 York    Queens         Q       \n\n\nMEC vanished!"
  },
  {
    "objectID": "slides/slides05.html#inner-and-outer-joins-2",
    "href": "slides/slides05.html#inner-and-outer-joins-2",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Inner and Outer Joins",
    "text": "Inner and Outer Joins\n\nleft_join(x, y, join_by(campus_borough == borough_name))\n\n# A tibble: 5 × 3\n  college      campus_borough bus_code\n  &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;   \n1 CCNY         Manhattan      M       \n2 Baruch       Manhattan      M       \n3 CSI          Staten Island  S       \n4 York         Queens         Q       \n5 Medgar Evers Brooklyn       &lt;NA&gt;    \n\n\nMEC stays, but no bus code - NA value\n\n\ninner_join - Keep only matches\nleft_join - Keep all rows in left (first) table even w/o matches\nright_join - Keep all rows in right (second) table even w/o matches\nfull_join - Keep all rows from both tables, even w/o matches\n\nleft_ and right_ are types of ‘outer’ joins"
  },
  {
    "objectID": "slides/slides05.html#mini-project-01-2",
    "href": "slides/slides05.html#mini-project-01-2",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "STA 9750 Mini-Project #01",
    "text": "STA 9750 Mini-Project #01\nYour classmates have asked several excellent questions on Piazza.\nReminders:\n\nThis data is messy because NYC is messy\nCan handle by either (or a mix):\n\nBy fixing as many issues as possible\nBy using ‘robust’ methods\n\nDocument your choices and do your best"
  },
  {
    "objectID": "slides/slides05.html#more-from-j.-jackson",
    "href": "slides/slides05.html#more-from-j.-jackson",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "More from J. Jackson",
    "text": "More from J. Jackson\n\nAs governmental pressure toward unity becomes greater, so strife becomes more bitter as to whose unity it shall be.[…] Those who begin coercive elimination of dissent soon find themselves exterminating dissenters. Compulsory unification of opinion achieves only the unanimity of the graveyard.\n\n\n\nAuthority [in the United States] is to be controlled by public opinion, not public opinion by authority."
  },
  {
    "objectID": "slides/slides05.html#baruch-connects---civil-discourse-initiative",
    "href": "slides/slides05.html#baruch-connects---civil-discourse-initiative",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Baruch Connects - Civil Discourse Initiative",
    "text": "Baruch Connects - Civil Discourse Initiative"
  },
  {
    "objectID": "slides/slides05.html#more-from-j.-jackson-1",
    "href": "slides/slides05.html#more-from-j.-jackson-1",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "More from J. Jackson",
    "text": "More from J. Jackson\n\n[F]reedom to differ is not limited to things that do not matter much. That would be a mere shadow of freedom. The test of its substance is the right to differ as to things that touch the heart of the existing order."
  },
  {
    "objectID": "slides/slides05.html#pivots",
    "href": "slides/slides05.html#pivots",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Pivots",
    "text": "Pivots\npivot_ changes the shape of a data set\n\nGet ready for presentation\nPrep for a join\nCombine rows before looking at ‘cross-row’ structure"
  },
  {
    "objectID": "slides/slides05.html#pivots-1",
    "href": "slides/slides05.html#pivots-1",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Pivots",
    "text": "Pivots\nQ: Which penguin species has the largest between-sex mass difference?\n\nlibrary(tidyr)\nlibrary(palmerpenguins)\navg_mass_tbl &lt;- penguins |&gt; drop_na() |&gt; \n    group_by(sex, species) |&gt; \n    summarize(avg_mass = mean(body_mass_g)) |&gt; \n    ungroup()\navg_mass_tbl\n\n# A tibble: 6 × 3\n  sex    species   avg_mass\n  &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt;\n1 female Adelie       3369.\n2 female Chinstrap    3527.\n3 female Gentoo       4680.\n4 male   Adelie       4043.\n5 male   Chinstrap    3939.\n6 male   Gentoo       5485."
  },
  {
    "objectID": "slides/slides05.html#pivots-2",
    "href": "slides/slides05.html#pivots-2",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Pivots",
    "text": "Pivots\nWe want data that is wider than our current data:\n\n\n\nspecies\nmale_avg\nfemale_avg\n\n\n\n\nAdelie\n…\n…\n\n\nChinstrap\n…\n…\n\n\nGentoo\n…\n…"
  },
  {
    "objectID": "slides/slides05.html#pivots-3",
    "href": "slides/slides05.html#pivots-3",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Pivots",
    "text": "Pivots\n\npivot_wider(avg_mass_tbl, \n            id_cols = species, \n            names_from=sex, \n            values_from=avg_mass)\n\n# A tibble: 3 × 3\n  species   female  male\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.\n2 Chinstrap  3527. 3939.\n3 Gentoo     4680. 5485.\n\n\n\npivot_wider(avg_mass_tbl, \n            id_cols = species, \n            names_from=sex, \n            values_from=avg_mass) |&gt;\n    mutate(sex_diff = male - female) |&gt;\n    slice_max(sex_diff)\n\n# A tibble: 1 × 4\n  species female  male sex_diff\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Gentoo   4680. 5485.     805."
  },
  {
    "objectID": "slides/slides05.html#pivots-4",
    "href": "slides/slides05.html#pivots-4",
    "title": "STA 9750  Week 5 Update  2025-02-27",
    "section": "Pivots",
    "text": "Pivots\npivot_wider Aguments:\n\nid_cols: kept as ‘keys’ for new table\nnames_from: existing column ‘spread’ to create new columns names\nvalues_from: values in new table\n\n\npivot_longer:\n\n‘Inverse’ operation\nSpread one row + multiple columns =&gt; one col + multiple rows"
  },
  {
    "objectID": "slides/slides06.html#reminder-cuny-citizenship-now",
    "href": "slides/slides06.html#reminder-cuny-citizenship-now",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Reminder: CUNY Citizenship Now!",
    "text": "Reminder: CUNY Citizenship Now!\n\nFree and confidental immigration law services provided by CUNY\nTargeted at CUNY students and families, but open to all NYers\n\nTo access:\n\nCallback Request Form\nLocations throughout NYC"
  },
  {
    "objectID": "slides/slides06.html#course-project-presentations-1",
    "href": "slides/slides06.html#course-project-presentations-1",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Course Project Presentations",
    "text": "Course Project Presentations\n\n\n\nOrder\nMembers\n\nTeam\nMembers\n\n\n\n\n1\nVH + SG + DS + DL\n\n6\nVG\n\n\n2\nHZ + JLL + CA\n\n7\nEM + AK + GMdS + JL\n\n\n3\nMT + CW\n\n8\nSJB + JC + SB + ZS\n\n\n4\nSD + GO + CFGF\n\n9\nGS\n\n\n5\nGB + RJ + FS + MH\n\n10\nCW"
  },
  {
    "objectID": "slides/slides06.html#reminder-iss-python-workshops",
    "href": "slides/slides06.html#reminder-iss-python-workshops",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Reminder: ISS Python Workshops",
    "text": "Reminder: ISS Python Workshops\nSee Brightspace Announcement"
  },
  {
    "objectID": "slides/slides06.html#reminder-iss-workshops",
    "href": "slides/slides06.html#reminder-iss-workshops",
    "title": "STA 9750  Week 6 Update  2025-03-13",
    "section": "Reminder: ISS Workshops",
    "text": "Reminder: ISS Workshops\nSee Brightspace Announcement"
  },
  {
    "objectID": "preassigns/pa08.html#footnotes",
    "href": "preassigns/pa08.html#footnotes",
    "title": "STA 9750 Week 8 Pre Assignment: More Plots",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is an effort to run shiny fully in browser (avoiding the need for a web server). It is still a work-in-progress, but you can try it out on the r-shinylive website, with a full gallery of examples here.↩︎\nIf you are more of a Python person, you can also check out the Python versions of shiny and shinylive.↩︎"
  },
  {
    "objectID": "slides/slides07.html#mini-project-02-1",
    "href": "slides/slides07.html#mini-project-02-1",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nA few students have reported issues with rate-limiting on the EIA SEP site. Re-running the code seems to resolve issues.\nMy download code is written to save the files locally, so only needs to succeed once.\n\nOne student reported NA conversion warnings in processing the NTD data. These were harmless, but I’ve modified the provided code to suppress these warnings.\nIn brief: update to NTD now uses dashes in a few places for missing data and NA warning came up when trying to conver these to numeric values"
  },
  {
    "objectID": "slides/slides07.html#feedback",
    "href": "slides/slides07.html#feedback",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Feedback",
    "text": "Feedback\n\nI’ve started to return Proposal Feedback (via email) - will finish tomorrow\nMP#01 grades to follow"
  },
  {
    "objectID": "slides/slides07.html#teaching-observation",
    "href": "slides/slides07.html#teaching-observation",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Teaching Observation",
    "text": "Teaching Observation\nOn April 3rd, Prof. Brandwein will sit in and observe class\n\nYou don’t need to do anything different - just an FYI"
  },
  {
    "objectID": "slides/slides07.html#looking-ahead",
    "href": "slides/slides07.html#looking-ahead",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nCourse Structure:\n\nOne more week on analyzing data you already have\nThen 3 weeks on getting data into R\nOne brief week on statistical modeling\n\n\nCourse Projects:\n\nVery excited by your proposals!\nPlease send me questions anytime - my goal is to help you achieve your goals"
  },
  {
    "objectID": "slides/slides07.html#life-tip-of-the-week",
    "href": "slides/slides07.html#life-tip-of-the-week",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Life Tip of the Week",
    "text": "Life Tip of the Week\nMaking the most of Amazon\n\nFree Trial and Discounted Rate Amazon Prime for Students\nAmazon Prime Visa by Chase: No annual fee + 5% cash back (or more) on all Amazon Purchases\nCamel Camel Camel\n\nPrice history for all Amazon items (see if you’re getting a good deal)\nPrice drop alert emails (get custom messages when an item goes on sale)"
  },
  {
    "objectID": "slides/slides07.html#diving-deeper-learning-goals",
    "href": "slides/slides07.html#diving-deeper-learning-goals",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Diving Deeper: Learning Goals",
    "text": "Diving Deeper: Learning Goals\nToday:\n\nFluency with basic geoms\nAnimation (Just a bit)\n\nNext Week:\n\nInteractive graphics\nDashboards\nSpatial Data (time allowing)"
  },
  {
    "objectID": "slides/slides07.html#looking-ahead-1",
    "href": "slides/slides07.html#looking-ahead-1",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nMy Learning Goals:\n\n(IMO) Most ‘practical’ course we offer\n\nExposure to tools and, more importantly, techniques for data analysis\n\nMy goal is to give you tools to achieve your goals in 3, 6, 12, 48 months\nBiggest failure for me is catching up with you in 2-3 years and hearing you feel held back by lack of skills.\nInvestment of time now, but payoff throughout your career"
  },
  {
    "objectID": "slides/slides07.html#explanation-vs-excuse",
    "href": "slides/slides07.html#explanation-vs-excuse",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Explanation vs Excuse",
    "text": "Explanation vs Excuse\nAs a speaker: If you failed at achieving a task, just say why.\n\nDon’t ‘impose’ forgiveness on your listener - they will give it or they won’t.\n\n\n\nAs a receiver: All reasons are not necessarily valid reasons.\n\n\n As a consumer of media content:\n\nI often find myself getting annoyed at various political commentators for reacting in a way I feel is not strong enough.\nThey are doing their job - explaining the world; dispassion is not approval."
  },
  {
    "objectID": "slides/slides07.html#explanation-vs-excuse-1",
    "href": "slides/slides07.html#explanation-vs-excuse-1",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "Explanation vs Excuse",
    "text": "Explanation vs Excuse\nSeparating these concepts will make you:\n\nmore responsible in your own actions;\nmore just in your treatment of others; and\nmore understanding of the views of those whom you may disagree with"
  },
  {
    "objectID": "slides/slides07.html#faq-uc-berkeley-graduate-admissions",
    "href": "slides/slides07.html#faq-uc-berkeley-graduate-admissions",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: UC Berkeley Graduate Admissions",
    "text": "FAQ: UC Berkeley Graduate Admissions\n1973: UCB was concerned about bias in Grad School Admissions\n\nHigher fraction of men admitted than women\nBickel, Hammell, O’Connell asked to study\n\nWhen they try to find the source of this bias, there is none!\nEach department admits women at a higher rate than men\nWomen applied to more selective programs at a higher rate\n\n\nThis phenomenon occurs throughout the social sciences\n\nBHO note:\n\nWomen are shunted by their socialization and education toward fields of graduate study that are generally more crowded, less productive of completed degrees, and less well funded, and that frequently offer poorer professional employment prospects."
  },
  {
    "objectID": "slides/slides07.html#faq-ucb-graduate-admissions",
    "href": "slides/slides07.html#faq-ucb-graduate-admissions",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "FAQ: UCB Graduate Admissions",
    "text": "FAQ: UCB Graduate Admissions\n1973: UC Berkeley was concerned about bias in Grad School Admissions\n\nHigher fraction of men admitted than women\nBickel, Hammell, O’Connell asked to study\n\nWhen they try to find the source of this bias, there is none!\nEach department admits women at a higher rate than men\nWomen applied to more selective programs at a higher rate\n\n\nThis phenomenon occurs throughout the social sciences: the best doctors have the worst patient outcomes\n\nBHO note:\n\nWomen are shunted by their socialization and education toward fields of graduate study that are generally more crowded, less productive of completed degrees, and less well funded, and that frequently offer poorer professional employment prospects."
  },
  {
    "objectID": "slides/slides07.html#mini-project-02-2",
    "href": "slides/slides07.html#mini-project-02-2",
    "title": "STA 9750  Week 7 Update  2025-03-20",
    "section": "STA 9750 Mini-Project #02",
    "text": "STA 9750 Mini-Project #02\nRemember that “non-standard” column names require use of backticks:\n\nNTD_ENERGY_RAW |&gt; \n    select(-c(`Reporter Type`, \n              `Reporting Module`, \n              `Other Fuel`, \n              `Other Fuel Description`)) |&gt;\n    mutate(`Bunker Fuel` = ...)"
  },
  {
    "objectID": "admm.html",
    "href": "admm.html",
    "title": "ANOVA-PCA Cartesian Block ADMM",
    "section": "",
    "text": "\\[\\newcommand{\\bA}{\\mathbf{A}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\argmin}{\\text{arg min}} \\newcommand{\\proj}[1]{\\mathcal{P}_{\\mathcal{S}_{#1}}} \\newcommand{\\bU}{\\mathbf{U}} \\newcommand{\\bZ}{\\mathbf{Z}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\bB}{\\mathbf{B}}\\]\nSuppose we want to solve the following optimization problem:\n\\[\\argmin_{\\bX} \\frac{1}{2}\\|\\bX - \\bA\\|_F^2 + \\sum_{n=1}^n \\lambda_n \\|\\proj{n}(\\bX)\\|_*\\]\nwhere \\(\\proj{n}(\\cdot)\\) extracts a submatrix of its argument, and \\(\\bX, \\bA \\in \\R^{m^1 \\times m^2}\\). Specifically, let each \\(\\proj{n}(\\cdot)\\) take values in \\(\\R^{m^1_n \\times m^2_n}\\) We can approach this with a standard two-block ADMM as follows. Let \\(\\bU_n \\in \\text{range}(\\proj{n}) = \\R^{m^1_n \\times m^2_n}\\) be suitably sized matrices and re-writethe above problem as:\n\\[\\argmin_{\\bX} \\frac{1}{2}\\|\\bX - \\bA\\|_F^2 + \\sum_{n=1}^n \\lambda_n \\|\\bU_n\\|_* \\text{ subject to } \\proj{n}(\\bX) = \\bU_n \\text{ for } n=1, \\dots N\\]\nAt first glance, you may be tempted to write this as a multi-block ADMM, but instead, let \\[\\overline{\\bU} = [\\bU_1, \\bU_2, \\dots, \\bU_n] \\in \\prod_{n=1}^N \\R^{m^1_n \\times m^2_n} \\equiv \\overline{\\R}\\] and \\(\\mathcal{P}(\\bX) = [\\proj{1}(\\bX), \\proj{2}(\\bX), \\dots, \\proj{n}(\\bX)] \\in \\overline{\\R}\\). With this notation, the problem becomes:\n\\[\\argmin_{\\bX} \\frac{1}{2}\\|\\bX - \\bA\\|_F^2 + \\sum_{n=1}^n \\lambda_n \\|\\overline{\\bU}_n\\|_* \\text{ subject to } \\mathcal{P}(\\bX) = \\overline{\\bU}\\] so now we have a two block form with a single linear constraint. The augmented Lagrangian then becomes: \\[\\argmin_{\\bX} \\frac{1}{2}\\|\\bX - \\bA\\|_F^2 + \\sum_{n=1}^n \\lambda_n \\|\\overline{\\bU}_n\\|_* + \\frac{\\rho}{2}\\left\\|\\mathcal{P}(\\bX) - \\overline{\\bU}\\right\\|_F^2.\\]\nThe ADMM iterations for this problem (Boyd et al, Eq 3.5 - 3.7) are:\n\\[\\begin{align*}\n\\bX^{(k+1)} &= \\argmin_{\\bX} \\frac{1}{2}\\|\\bX - \\bA\\|_F^2 + \\frac{\\rho}{2}\\left\\|\\mathcal{P}(\\bX) - \\overline{\\bU^{(k)}} + \\overline{\\bZ}^{(k)}\\right\\|_F^2\\\\\n\\overline{\\bU}^{(k+1)} &= \\argmin_{\\overline{\\bU}} \\sum_{n=1}^n \\lambda_n \\|\\overline{\\bU}_n\\|_*\n+ \\frac{\\rho}{2}\\left\\|\\mathcal{P}(\\bX^{(k+1)}) - \\overline{\\bU} + \\overline{\\bZ}^{(k)}\\right\\|_F^2 \\\\\n\\overline{\\bZ}^{(k+1)}&= \\overline{\\bZ}^{(k)} + \\mathcal{P}(\\bX^{(k+1)}) - \\overline{\\bU}^{(k+1)}\n\\end{align*}\\]\nwhere \\(\\overline{\\bZ} \\in \\overline{\\R}\\) is a dual variable with the same structure as \\(\\mathcal{P}(\\bX), \\overline{\\bU}\\)."
  },
  {
    "objectID": "slides/slides08.html",
    "href": "slides/slides08.html",
    "title": "STA 9750 - Week 8",
    "section": "",
    "text": "Submission due yesterday at 11:45pm\n. . .\nY’all have it out for The Shawshank Redemption…\n. . .\n\nVery entertained by fake posters\nImpressive graphics and analysis - very nice."
  },
  {
    "objectID": "slides/slides08.html#life-tip-of-the-week",
    "href": "slides/slides08.html#life-tip-of-the-week",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Life Tip of the Week",
    "text": "Life Tip of the Week\nGet Inspired!\nThe tools of this course are powerful and flexible\nTo learn more ways to apply them, check out ‘Galleries’:\n\n\nThe R Graph Gallery\nThe shiny Gallery\nThe bokeh Gallery (Python)"
  },
  {
    "objectID": "slides/slides08.html#explanation-vs-excuse",
    "href": "slides/slides08.html#explanation-vs-excuse",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Explanation vs Excuse",
    "text": "Explanation vs Excuse\nAs a speaker: If you failed at achieving a task, just say why.\n\nDon’t ‘impose’ forgiveness on your listener - they will give it or they won’t.\n\n\n\nAs a receiver: All reasons are not necessarily valid reasons.\n\n\n As a consumer of media content:\n\nI often find myself getting annoyed at various political commentators for reacting in a way I feel is not strong enough.\nThey are doing their job - explaining the world; dispassion is not approval."
  },
  {
    "objectID": "slides/slides08.html#explanation-vs-excuse-1",
    "href": "slides/slides08.html#explanation-vs-excuse-1",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Explanation vs Excuse",
    "text": "Explanation vs Excuse\nSeparating these concepts will make you:\n\nmore responsible in your own actions;\nmore just in your treatment of others; and\nmore understanding of the views of those whom you may disagree with"
  },
  {
    "objectID": "admm.html#composite-algorithm",
    "href": "admm.html#composite-algorithm",
    "title": "ANOVA-PCA Cartesian Block ADMM",
    "section": "Composite Algorithm",
    "text": "Composite Algorithm\n\nInitialize:\n\n\\(\\bU^{(0)}_n = \\proj{n}(\\bA)\\) for all \\(n=1, \\dots, N\\)\n\\(\\bZ^{(0)}_n = \\mathbf{0}\\in\\R^{n_1^i \\times n_2^i}\\) for all \\(n=1, \\dots, N\\)\n\\(k=0\\)\n\nRepeat Until Convergence\n\n\\(\\bX^{(k+1)} = \\frac{\\bA + \\sum_{n=1}^N \\textsf{pad}_{\\R^{m^1_n \\times m^2_n} \\to \\R^{m^1 \\times m^2}}[\\bZ^{(k)}_n - \\bU^{(k)}_n] }{1+\\rho\\mathbf{C}}\\)\nIn parallel, for \\(n=1, \\dots, N\\):\n\n\\(\\bU^{(k+1)}_n = \\text{SVD-SoftThresh}_{\\lambda_n / \\rho}\\left[\\proj{n}(\\bX^{(k+1)}) + \\bZ^{(k)}_n\\right]\\)\n\\(\\bZ^{(k+1)}_n = \\bZ^{(k)}_n + \\proj{n}(\\bX^{(k+1)}) - \\bU_n^{(k+1)}\\)\n\n\\(k \\leftarrow k+1\\)\n\nReturn: \\[ \\hat{\\bX} = \\frac{\\bA + \\sum_{n=1}^N \\textsf{pad}_{\\R^{m^1_n \\times m^2_n} \\to \\R^{m^1 \\times m^2}}[\\bZ^{(k)}_n - \\bU^{(k)}_n] }{1+\\rho\\mathbf{C}}\\]\n\nUnder the reasonable assumption that \\(\\bU^{(k+1)}_n \\approx \\bU^{(k)}_n\\), clever warm-start and caching techniques can be used for the soft-threshold SVD operator."
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#introduction",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#introduction",
    "title": "STA 9750  Tidy Data",
    "section": "Introduction",
    "text": "Introduction\nIn this video:\n\n\nIntroduce the concept of Tidy Data\nIdentify Key Aspects of Tidy Data\nLearn to Diagnose Non-Tidy Data\n\n\n\nLater:\n\nManipulation of Tidy Data\n\n\n\nBut first - review!"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#data-frames",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#data-frames",
    "title": "STA 9750  Tidy Data",
    "section": "Data Frames",
    "text": "Data Frames\ndata.frame - R’s tabular data storage\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#data-frames-1",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#data-frames-1",
    "title": "STA 9750  Tidy Data",
    "section": "Data Frames",
    "text": "Data Frames\nKey features:\n\n\nTabular (rows and columns) structure\nEntries in the same column are all of the same type\nColumns may be different types"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#data-frames-2",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#data-frames-2",
    "title": "STA 9750  Tidy Data",
    "section": "Data Frames",
    "text": "Data Frames\nCan be manipulated using standard R syntax:\n\ntable(iris[iris$Petal.Length &gt; 5,\"Species\"])\n\n\n    setosa versicolor  virginica \n         0          1         41 \n\n\n\n\n$ extracts a column as a vector\nname[row index, column index]\n\nLogical row indexing to select rows where Petal Length &gt; 5\nColumn indexing by name to select a single column"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#what-is-tidy-data",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#what-is-tidy-data",
    "title": "STA 9750  Tidy Data",
    "section": "What is Tidy Data?",
    "text": "What is Tidy Data?\n“Tidy Data”: Organizational principles to make data manipulation easy and safe\n\n“Best practices”\nTidy data will decrease chance of errors and increase productivity\n\n\nWhat does it mean to be tidy?\n\n\nEverything in its proper place"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data-1",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data-1",
    "title": "STA 9750  Tidy Data",
    "section": "Tidy Data",
    "text": "Tidy Data\n\n\n  species bill_length_mm flipper_length_mm body_mass_g    sex\n1  Adelie           39.1               181        3750   male\n2  Adelie           39.5               186        3800 female\n3  Adelie           40.3               195        3250 female\n\n\n\nKey features:\n\n\nEach row is one observation (🐧)\nEach column has one and only one fact\nAll values are in the table - not hiding in row and column names"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data-2",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data-2",
    "title": "STA 9750  Tidy Data",
    "section": "Tidy Data",
    "text": "Tidy Data\n\n\nFigure from R for Data Science by H. Wickham"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#why-tidy-data",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#why-tidy-data",
    "title": "STA 9750  Tidy Data",
    "section": "Why Tidy Data?",
    "text": "Why Tidy Data?"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#who-tidy-data",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#who-tidy-data",
    "title": "STA 9750  Tidy Data",
    "section": "Who Tidy Data?",
    "text": "Who Tidy Data?"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#when-tidy-data",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#when-tidy-data",
    "title": "STA 9750  Tidy Data",
    "section": "When Tidy Data?",
    "text": "When Tidy Data?"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#where-tidy-data",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#where-tidy-data",
    "title": "STA 9750  Tidy Data",
    "section": "Where Tidy Data?",
    "text": "Where Tidy Data?"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#some-untidy-examples",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#some-untidy-examples",
    "title": "STA 9750  Tidy Data",
    "section": "Some Untidy Examples",
    "text": "Some Untidy Examples\nBaruch college business core enrollment:\n\n\n# A tibble: 6 × 4\n  Semester Course     Enrollment   Cap\n  &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 Fall     Accounting        200   250\n2 Fall     Law               100   125\n3 Fall     Statistics        200   200\n4 Spring   Accounting        300   350\n5 Spring   Law                50   100\n6 Spring   Statistics        400   400"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data---why",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data---why",
    "title": "STA 9750  Tidy Data",
    "section": "Tidy Data - Why?",
    "text": "Tidy Data - Why?\nWhy emphasize tidy data?\n\nMinimize distractions:\n\nFree to focus on analysis not code\n\n\n\nOnce data is “tidy”, you can focus on the real questions\n“Tidying up” - goal of pre-processing"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data---who-and-when",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data---who-and-when",
    "title": "STA 9750  Tidy Data",
    "section": "Tidy Data - Who (and When)?",
    "text": "Tidy Data - Who (and When)?\nThe name and principles of “tidy data” were popularized by H. Wickham (2014)\n\nCore ideas are much older, dating back to (at least) Codd’s Relational Model in the 1970s, now ubiquitous in relational databases\n\n\nNow found in Python (pandas), Julia (DataFrames), Rust (polars) and more"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data---how-and-where",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data---how-and-where",
    "title": "STA 9750  Tidy Data",
    "section": "Tidy Data - How (and Where)?",
    "text": "Tidy Data - How (and Where)?\ntidyverse - Packages for Manipulating Tidy Data:\n\n\nggplot2: Visualization\ndplyr: SQL-like operations\ntidyr: Reshaping and cleaning data\nreadr: Ingest tidy data into R\nstringr, forcats, lubridate: Tidy manipulation of different data types\ntidymodels: Tidy manipulation of statistical models\n\n\n\nMore helpers in the background (tibble, vctrs, …)"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#looking-ahead-1",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#looking-ahead-1",
    "title": "STA 9750  Tidy Data",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nFor now, we’ll provide you with tidy data\n\nUpcoming topics:\n\n\nVisualizing Tidy Data (Week 3)\nStatistical Models of Tidy Data (Week 4)\nManipulating Tidy Data (Week 5)\nTaking Untidy Web Data and Making it Tidy (Week 7)"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#some-untidy-examples-1",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#some-untidy-examples-1",
    "title": "STA 9750  Tidy Data",
    "section": "Some Untidy Examples",
    "text": "Some Untidy Examples\nMultiple pieces of information in a single cell:\n\n\n# A tibble: 6 × 3\n  Semester Course     Enrollment  \n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;       \n1 Fall     Accounting \"200 of 250\"\n2 Fall     Law        \"100 of 125\"\n3 Fall     Statistics \"200 of 200\"\n4 Spring   Accounting \"300 of 350\"\n5 Spring   Law        \" 50 of 100\"\n6 Spring   Statistics \"400 of 400\""
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#some-untidy-examples-2",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#some-untidy-examples-2",
    "title": "STA 9750  Tidy Data",
    "section": "Some Untidy Examples",
    "text": "Some Untidy Examples\nMixing distinct facts in a single column:\n\n\n# A tibble: 12 × 4\n   Semester Course     Number Type      \n   &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     \n 1 Fall     Accounting    200 Enrollment\n 2 Fall     Accounting    250 Cap       \n 3 Fall     Law           100 Enrollment\n 4 Fall     Law           125 Cap       \n 5 Fall     Statistics    200 Enrollment\n 6 Fall     Statistics    200 Cap       \n 7 Spring   Accounting    300 Enrollment\n 8 Spring   Accounting    350 Cap       \n 9 Spring   Law            50 Enrollment\n10 Spring   Law           100 Cap       \n11 Spring   Statistics    400 Enrollment\n12 Spring   Statistics    400 Cap"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#some-untidy-examples-3",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#some-untidy-examples-3",
    "title": "STA 9750  Tidy Data",
    "section": "Some Untidy Examples",
    "text": "Some Untidy Examples\nSpread across two tables:\n\n\n# A tibble: 6 × 3\n  Semester Course     Enrollment\n  &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;\n1 Fall     Accounting        200\n2 Fall     Law               100\n3 Fall     Statistics        200\n# ℹ 3 more rows\n\n\nand\n\n\n# A tibble: 6 × 3\n  Semester Course       Cap\n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1 Fall     Accounting   250\n2 Fall     Law          125\n3 Fall     Statistics   200\n# ℹ 3 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#introduction",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#introduction",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "Introduction",
    "text": "Introduction\nIn this video:\n\n\nSelecting Columns\nSelecting Rows\nModifying Columns and Creating New Columns\n\n\n\nLater:\n\nComputing Summary Statistics with Tidy Tools\n\n\n\nBut first - review!"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#tidy-data",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#tidy-data",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "Tidy Data",
    "text": "Tidy Data\nTidy data - everything in its proper place:\n\nOne and only one type of information per column\nOne and only one observation per row\nNo values ‘hiding’ in column or row names\n\n\n\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#tidyverse",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#tidyverse",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "tidyverse",
    "text": "tidyverse\ntidyverse - Tools for manipulating tidy data\n\nMake sure to load the tidyverse package before using\n\nlibrary(tidyverse)\n\nWill print some messages about conflicts - safe to ignore"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#tidyverse-1",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#tidyverse-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "tidyverse",
    "text": "tidyverse\nTidyverse tools have excellent documentation\n\nVisit tidyverse.org for overall information\nIndividual packages: ggplot2.tidyverse.org, dplyr.tidyverse.org, etc.\n\n\n\nFunction Reference - Detailed information on specific functions\nVignettes / Articles - Big picture tutorials"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#dplyr",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#dplyr",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "dplyr",
    "text": "dplyr\nYou have already seen one tidyverse package: ggplot2\n\nToday we start with dplyr - ‘grammar of data manipulation’"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nThe select function can be used to select certain columns\n\nselect(penguins, species, bill_length_mm, body_mass_g, sex)\n\n# A tibble: 344 × 4\n   species bill_length_mm body_mass_g sex   \n   &lt;fct&gt;            &lt;dbl&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie            39.1        3750 male  \n 2 Adelie            39.5        3800 female\n 3 Adelie            40.3        3250 female\n 4 Adelie            NA            NA &lt;NA&gt;  \n 5 Adelie            36.7        3450 female\n 6 Adelie            39.3        3650 male  \n 7 Adelie            38.9        3625 female\n 8 Adelie            39.2        4675 male  \n 9 Adelie            34.1        3475 &lt;NA&gt;  \n10 Adelie            42          4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\nFirst argument is data\nOther arguments are columns to be selected"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#put-that-in-your-pipe",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#put-that-in-your-pipe",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "|> - Put that in your Pipe",
    "text": "|&gt; - Put that in your Pipe\nR supports piped operations - feeding the output of one function into another\n\nselect(penguins, species, bill_length_mm, body_mass_g, sex)\n\nis equivalent to\n\npenguins |&gt; select(species, bill_length_mm, body_mass_g, sex)\n\n\nRead |&gt; as “and then”\ndplyr plays very nicely with pipes"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-1",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nContrast\n\nselect(penguins, species, bill_length_mm, body_mass_g, sex)\n\nand\n\npenguins[,c(\"species\", \"bill_length_mm\", \"body_mass_g\", \"sex\")]\n\nNo quotes around column names"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-2",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-2",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nLike vector indexing, negative sign drops columns:\n\npenguins |&gt; select(-species)\n\n# A tibble: 344 × 7\n   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Torge…           39.1          18.7               181        3750 male   2007\n 2 Torge…           39.5          17.4               186        3800 fema…  2007\n 3 Torge…           40.3          18                 195        3250 fema…  2007\n 4 Torge…           NA            NA                  NA          NA &lt;NA&gt;   2007\n 5 Torge…           36.7          19.3               193        3450 fema…  2007\n 6 Torge…           39.3          20.6               190        3650 male   2007\n 7 Torge…           38.9          17.8               181        3625 fema…  2007\n 8 Torge…           39.2          19.6               195        4675 male   2007\n 9 Torge…           34.1          18.1               193        3475 &lt;NA&gt;   2007\n10 Torge…           42            20.2               190        4250 &lt;NA&gt;   2007\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-3",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-3",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\n: operator selects a range of columns\n\npenguins |&gt; select(bill_length_mm:body_mass_g)\n\n# A tibble: 344 × 4\n   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1           39.1          18.7               181        3750\n 2           39.5          17.4               186        3800\n 3           40.3          18                 195        3250\n 4           NA            NA                  NA          NA\n 5           36.7          19.3               193        3450\n 6           39.3          20.6               190        3650\n 7           38.9          17.8               181        3625\n 8           39.2          19.6               195        4675\n 9           34.1          18.1               193        3475\n10           42            20.2               190        4250\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-4",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-4",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nselect also supports several “smart” helpers to pick columns\n\npenguins |&gt; select(contains(\"bill\"))\n\n# A tibble: 344 × 2\n   bill_length_mm bill_depth_mm\n            &lt;dbl&gt;         &lt;dbl&gt;\n 1           39.1          18.7\n 2           39.5          17.4\n 3           40.3          18  \n 4           NA            NA  \n 5           36.7          19.3\n 6           39.3          20.6\n 7           38.9          17.8\n 8           39.2          19.6\n 9           34.1          18.1\n10           42            20.2\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-5",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-5",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\n\npenguins |&gt; select(ends_with(\"mm\"))\n\n# A tibble: 344 × 3\n   bill_length_mm bill_depth_mm flipper_length_mm\n            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1           39.1          18.7               181\n 2           39.5          17.4               186\n 3           40.3          18                 195\n 4           NA            NA                  NA\n 5           36.7          19.3               193\n 6           39.3          20.6               190\n 7           38.9          17.8               181\n 8           39.2          19.6               195\n 9           34.1          18.1               193\n10           42            20.2               190\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-6",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-6",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nHelpers can be combined using & (intersection) and | (union) operations:\n\npenguins |&gt; select(contains(\"bill\") | ends_with(\"g\"))\n\n# A tibble: 344 × 3\n   bill_length_mm bill_depth_mm body_mass_g\n            &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1           39.1          18.7        3750\n 2           39.5          17.4        3800\n 3           40.3          18          3250\n 4           NA            NA            NA\n 5           36.7          19.3        3450\n 6           39.3          20.6        3650\n 7           38.9          17.8        3625\n 8           39.2          19.6        4675\n 9           34.1          18.1        3475\n10           42            20.2        4250\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-7",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-7",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nPredicate operator can select columns based on properties of values\n\npenguins |&gt; select(where(is.numeric))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-8",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#select-8",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nwhere combines very nicely with other selectors\n\npenguins |&gt; select(where(is.numeric) & ends_with(\"mm\"))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nWhile select is used for column selection, filter is used for row selection\n\npenguins |&gt; filter(bill_length_mm &gt; 40)\n\n\nSimilar syntax to select:\n\nNo need to quote column names\nData is first (piped) argument\nLater arguments specify action"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-1",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nContrast\n\npenguins |&gt; filter(bill_length_mm &gt; 40)\n\nwith\n\npenguins[penguins$bill_length_mm &gt; 40, ]"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-2",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-2",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nMultiple filters give the intersection:\n\npenguins |&gt; filter(bill_length_mm &gt; 40, sex == \"female\")\n\n\nUse logical operators to get the union:\n\npenguins |&gt; filter(bill_length_mm &gt; 40 | sex == \"female\")"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-3",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-3",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nPipes are very useful for multi-step operations\n\npenguins |&gt; filter(bill_length_mm &gt; 40) |&gt; select(species)\n\n\nCompare to\n\nselect(filter(penguins, bill_length_mm &gt; 40), species)\n\nPiped form lets us read left to right instead of inside out"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-4",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-4",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nCan use more complex comparisons in filter:\n\nfilter(penguins, body_mass_g &gt; mean(body_mass_g, na.rm=TRUE))\n\n\nWhat would happen without na.rm=TRUE?\nYou will explore much more complicated examples in the accompanying activity."
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-5",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-5",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nCommon mistake - using only a single equals sign for equality\n\npenguins |&gt; filter(sex = \"female\")\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `sex == \"female\"`?"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#mutate",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#mutate",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "mutate",
    "text": "mutate\nSo far we have only done subsetting. What about creating new columns?\n\n\npenguins |&gt; \n    mutate(body_mass_lb = body_mass_g / 453) |&gt;\n    select(contains(\"body\"))\n\n# A tibble: 344 × 2\n   body_mass_g body_mass_lb\n         &lt;int&gt;        &lt;dbl&gt;\n 1        3750         8.28\n 2        3800         8.39\n 3        3250         7.17\n 4          NA        NA   \n 5        3450         7.62\n 6        3650         8.06\n 7        3625         8.00\n 8        4675        10.3 \n 9        3475         7.67\n10        4250         9.38\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#mutate-1",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#mutate-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "mutate",
    "text": "mutate\n\npenguins |&gt; \n    mutate(body_mass_lb = body_mass_g / 453) |&gt;\n    select(contains(\"body\"))\n\nSimilar syntax to select and filter\n\nTo name the new column, put name on the left of the =.\nIf you leave it out, you get an unhelpful default:\n\npenguins |&gt; mutate(body_mass_g / 453) |&gt; select(contains(\"body\"))\n\n\n\n# A tibble: 344 × 2\n  body_mass_g `body_mass_g/453`\n        &lt;int&gt;             &lt;dbl&gt;\n1        3750              8.28\n2        3800              8.39\n3        3250              7.17\n# ℹ 341 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#mutate-2",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#mutate-2",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "mutate",
    "text": "mutate\nCan change columns “in place” by careful renaming:\n\npenguins |&gt; \n    select(where(is.character) | where(is.factor)) |&gt;\n    mutate(species = toupper(species))\n\n# A tibble: 344 × 3\n   species island    sex   \n   &lt;chr&gt;   &lt;fct&gt;     &lt;fct&gt; \n 1 ADELIE  Torgersen male  \n 2 ADELIE  Torgersen female\n 3 ADELIE  Torgersen female\n 4 ADELIE  Torgersen &lt;NA&gt;  \n 5 ADELIE  Torgersen female\n 6 ADELIE  Torgersen male  \n 7 ADELIE  Torgersen female\n 8 ADELIE  Torgersen male  \n 9 ADELIE  Torgersen &lt;NA&gt;  \n10 ADELIE  Torgersen &lt;NA&gt;  \n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-extensions",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-extensions",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter extensions",
    "text": "filter extensions\nFunctions implementing extensions of filter:\n\nslice_min\nslice_max\nslice_head\nslice_tail"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-extensions-1",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#filter-extensions-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter extensions",
    "text": "filter extensions\nslice_sample is useful for getting a random subset of your data.\n\nUseful if exploring a large data set\n\n\npenguins |&gt; slice_sample(n=10)\n\n# A tibble: 10 × 8\n   species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo    Biscoe           50            15.3               220        5550\n 2 Chinstrap Dream            46.2          17.5               187        3650\n 3 Adelie    Biscoe           35            17.9               190        3450\n 4 Adelie    Biscoe           39            17.5               186        3550\n 5 Chinstrap Dream            51.3          19.9               198        3700\n 6 Adelie    Biscoe           35.9          19.2               189        3800\n 7 Chinstrap Dream            50            19.5               196        3900\n 8 Chinstrap Dream            52            18.1               201        4050\n 9 Gentoo    Biscoe           44.9          13.8               212        4750\n10 Gentoo    Biscoe           48.8          16.2               222        6000\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#mutate-3",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#mutate-3",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "mutate",
    "text": "mutate\nCan do arbitrary vectorized operations and access recently created columns:\n\npenguins |&gt; \n    select(where(is.numeric)) |&gt;\n    mutate(weight_kg = body_mass_g / 1000,\n           height_m = body_height_mm / 1000, \n           bmi = weight_kg / height_m^2)\n\n(Sadly, don’t actually have height data)"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#mutate-4",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#mutate-4",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "mutate",
    "text": "mutate\n?mutate has several helpful functions:\n\ncumsum, cummean\nlead, lag\nif_else, case_when"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#rename",
    "href": "STA9750_ONLINE_SLIDES/selecting_and_filtering.html#rename",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "rename",
    "text": "rename\nUse of mutate to rename:\n\npenguins |&gt; mutate(Year = year) |&gt; select(-year)\n\n\nrename combines both steps:\n\npenguins |&gt; rename(Year = year) \n\nNo new functionality - clearer intent"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#introduction-1",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#introduction-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "Introduction",
    "text": "Introduction\nIn this video:\n\n\nSelecting Columns\nSelecting Rows\nModifying Columns and Creating New Columns\n\n\n\nLater:\n\nComputing Summary Statistics with Tidy Tools\n\n\n\nBut first - review!"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data-3",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#tidy-data-3",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "Tidy Data",
    "text": "Tidy Data\nTidy data - everything in its proper place:\n\nOne and only one type of information per column\nOne and only one observation per row\nNo values ‘hiding’ in column or row names\n\n\nlibrary(palmerpenguins); head(penguins, 3)"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#tidyverse",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#tidyverse",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "tidyverse",
    "text": "tidyverse\ntidyverse - Tools for manipulating tidy data\n\nMake sure to load the tidyverse package before using\n#| echo: true\n#| message: false\n#| warning: false\nlibrary(tidyverse)\nWill print some messages about conflicts - safe to ignore"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#tidyverse-1",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#tidyverse-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "tidyverse",
    "text": "tidyverse\nTidyverse tools have excellent documentation\n\nVisit tidyverse.org for overall information\nIndividual packages: ggplot2.tidyverse.org, dplyr.tidyverse.org, etc.\n\n\n\nFunction Reference - Detailed information on specific functions\nVignettes / Articles - Big picture tutorials"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#dplyr",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#dplyr",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "dplyr",
    "text": "dplyr\nYou have already seen one tidyverse package: ggplot2\n\nToday we start with dplyr - ‘grammar of data manipulation’"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#select",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#select",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nThe select function can be used to select certain columns\n#| echo: true\nselect(penguins, species, bill_length_mm, body_mass_g, sex)\n\nFirst argument is data\nOther arguments are columns to be selected"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#select-1",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#select-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nContrast\n#| echo: true\n#| eval: false\nselect(penguins, species, bill_length_mm, body_mass_g, sex)\nand\n#| eval: false\n#| echo: true\npenguins[,c(\"species\", \"bill_length_mm\", \"body_mass_g\", \"sex\")]\nNo quotes around column names"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#put-that-in-your-pipe",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#put-that-in-your-pipe",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "|> - Put that in your Pipe",
    "text": "|&gt; - Put that in your Pipe\nR supports piped operations - feeding the output of one function into another\n#| eval: false\n#| echo: true\nselect(penguins, species, bill_length_mm, body_mass_g, sex)\nis equivalent to\n#| eval: false\n#| echo: true\npenguins |&gt; select(species, bill_length_mm, body_mass_g, sex)\n\nRead |&gt; as “and then”\ndplyr plays very nicely with pipes"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#select-2",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#select-2",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nLike vector indexing, negative sign drops columns:\n#| echo: true\npenguins |&gt; select(-species)"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#select-3",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#select-3",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\n: operator selects a range of columns\n#| echo: true\npenguins |&gt; select(bill_length_mm:body_mass_g)"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#select-4",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#select-4",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nselect also supports several “smart” helpers to pick columns\n#| echo: true\npenguins |&gt; select(contains(\"bill\"))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#select-5",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#select-5",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\n#| echo: true\npenguins |&gt; select(ends_with(\"mm\"))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#select-6",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#select-6",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nHelpers can be combined using & (intersection) and | (union) operations:\n#| echo: true\npenguins |&gt; select(contains(\"bill\") | ends_with(\"g\"))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#select-7",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#select-7",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nPredicate operator can select columns based on properties of values\n#| echo: true\n#| eval: false\npenguins |&gt; select(where(is.numeric))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#select-8",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#select-8",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "select",
    "text": "select\nwhere combines very nicely with other selectors\n#| echo: true\n#| eval: false\npenguins |&gt; select(where(is.numeric) & ends_with(\"mm\"))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#filter",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#filter",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nWhile select is used for column selection, filter is used for row selection\n#| echo: true\n#| eval: false\npenguins |&gt; filter(bill_length_mm &gt; 40)\n\nSimilar syntax to select:\n\nNo need to quote column names\nData is first (piped) argument\nLater arguments specify action"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#filter-1",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#filter-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nContrast\n#| echo: true\n#| eval: false\npenguins |&gt; filter(bill_length_mm &gt; 40)\nwith\n#| echo: true\n#| eval: false\npenguins[penguins$bill_length_mm &gt; 40, ]"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#filter-2",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#filter-2",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nMultiple filters give the intersection:\n#| echo: true\n#| eval: false\npenguins |&gt; filter(bill_length_mm &gt; 40, sex == \"female\")\n\nUse logical operators to get the union:\n#| echo: true\n#| eval: false\npenguins |&gt; filter(bill_length_mm &gt; 40 | sex == \"female\")"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#filter-3",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#filter-3",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nPipes are very useful for multi-step operations\n#| echo: true\n#| eval: false\npenguins |&gt; filter(bill_length_mm &gt; 40) |&gt; select(species)\n\nCompare to\n#| echo: true\n#| eval: false\nselect(filter(penguins, bill_length_mm &gt; 40), species)\nPiped form lets us read left to right instead of inside out"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#filter-4",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#filter-4",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nCan use more complex comparisons in filter:\n#| echo: true\n#| eval: false\nfilter(penguins, body_mass_g &gt; mean(body_mass_g, na.rm=TRUE))\n\nWhat would happen without na.rm=TRUE?\nYou will explore much more complicated examples in the accompanying activity."
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#filter-5",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#filter-5",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter",
    "text": "filter\nCommon mistake - using only a single equals sign for equality\n#| error: true\n#| echo: true\npenguins |&gt; filter(sex = \"female\")"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#filter-extensions",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#filter-extensions",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter extensions",
    "text": "filter extensions\nFunctions implementing extensions of filter:\n\nslice_min\nslice_max\nslice_head\nslice_tail"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#filter-extensions-1",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#filter-extensions-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "filter extensions",
    "text": "filter extensions\nslice_sample is useful for getting a random subset of your data.\n\nUseful if exploring a large data set\n\n#| echo: true\npenguins |&gt; slice_sample(n=10)"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#mutate",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#mutate",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "mutate",
    "text": "mutate\nSo far we have only done subsetting. What about creating new columns?\n\n#| echo: true\npenguins |&gt; \n    mutate(body_mass_lb = body_mass_g / 453) |&gt;\n    select(contains(\"body\"))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#mutate-1",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#mutate-1",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "mutate",
    "text": "mutate\n#| echo: true\n#| eval: false\npenguins |&gt; \n    mutate(body_mass_lb = body_mass_g / 453) |&gt;\n    select(contains(\"body\"))\nSimilar syntax to select and filter\n\nTo name the new column, put name on the left of the =.\nIf you leave it out, you get an unhelpful default:\n#| echo: true\n#| eval: false\npenguins |&gt; mutate(body_mass_g / 453) |&gt; select(contains(\"body\"))\npenguins |&gt; mutate(body_mass_g / 453) |&gt; select(contains(\"body\")) |&gt; print(n=3)"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#mutate-2",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#mutate-2",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "mutate",
    "text": "mutate\nCan change columns “in place” by careful renaming:\n#| echo: true\npenguins |&gt; \n    select(where(is.character) | where(is.factor)) |&gt;\n    mutate(species = toupper(species))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#mutate-3",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#mutate-3",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "mutate",
    "text": "mutate\nCan do arbitrary vectorized operations and access recently created columns:\n#| echo: true\n#| eval: false\npenguins |&gt; \n    select(where(is.numeric)) |&gt;\n    mutate(weight_kg = body_mass_g / 1000,\n           height_m = body_height_mm / 1000, \n           bmi = weight_kg / height_m^2)\n(Sadly, don’t actually have height data)"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#mutate-4",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#mutate-4",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "mutate",
    "text": "mutate\n?mutate has several helpful functions:\n\ncumsum, cummean\nlead, lag\nif_else, case_when"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#rename",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#rename",
    "title": "STA 9750  Selecting, Filtering, and Modifying with Tidy Data Tools",
    "section": "rename",
    "text": "rename\nUse of mutate to rename:\n#| echo: true\n#| eval: false\npenguins |&gt; mutate(Year = year) |&gt; select(-year)\n\nrename combines both steps:\n#| echo: true\n#| eval: false\npenguins |&gt; rename(Year = year) \nNo new functionality - clearer intent"
  },
  {
    "objectID": "slides/slides08.html#mp02---peer-feedback",
    "href": "slides/slides08.html#mp02---peer-feedback",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "STA 9750 MP#02 - Peer Feedback",
    "text": "STA 9750 MP#02 - Peer Feedback\nPeer feedback assigned on GitHub + email this morning\n\n\\(\\approx 4\\) feedbacks each\nTake this seriously: around 20% of this assignment is “meta-review”\nGoal: rigorous constructive critique\n\n\nSubmissions may not map perfectly to rubric - use your best judgement\n\n\nLearn from this! What can you adapt for MP#03?"
  },
  {
    "objectID": "slides/slides08.html#diving-deeper-into-ggplot2",
    "href": "slides/slides08.html#diving-deeper-into-ggplot2",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Diving Deeper into ggplot2",
    "text": "Diving Deeper into ggplot2\nFirst topic: maps!\nInstall the sf package: Simple Features for Spatial Data\nExercise: Lab #08"
  },
  {
    "objectID": "slides/slides08.html#topic-interactive-tools-for-data-analysis",
    "href": "slides/slides08.html#topic-interactive-tools-for-data-analysis",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Topic Interactive Tools for Data Analysis",
    "text": "Topic Interactive Tools for Data Analysis"
  },
  {
    "objectID": "slides/slides08.html#looking-ahead-1",
    "href": "slides/slides08.html#looking-ahead-1",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nDue Wednesday at 11:45pm:\n\nPre-Assignment #09 (Brightspace)\n\nData Import\n\nMP #02 Peer Feedback on GitHub AND Brightspace\n\n\nNext three weeks:\n\nReading ‘clean data’ into R\nReading and parsing HTML\nParsing messy (text) data\n\n\n\nTeaching Observation by Prof. Brandwein - Next Week"
  },
  {
    "objectID": "slides/slides08.html#life-tip-of-the-week-1",
    "href": "slides/slides08.html#life-tip-of-the-week-1",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "Life Tip of the Week",
    "text": "Life Tip of the Week\nDon’t confuse an explanation with an excuse\n\nExplanation is neutral: why?\nExcuse presumes forgiveness"
  },
  {
    "objectID": "slides/slides08.html#sta-9750-hall-of-fame",
    "href": "slides/slides08.html#sta-9750-hall-of-fame",
    "title": "STA 9750  Week 8 Update  2025-03-27",
    "section": "STA 9750 Hall of Fame",
    "text": "STA 9750 Hall of Fame\nMy current side-project: STA 9750 Hall of Frame\n\nGallery of excellent STA 9750 submissions\n\n‘Signal-boost’ individual portfolios\nDemonstrate the high quality of Baruch students\nInspiration for future semesters\n2-3 per mini-project\n\n\n\nTrying to launch next week - will share a ‘sign up’ if you’re interested"
  },
  {
    "objectID": "labs/lab08.html#exercise-3---building-a-shiny-tool-for",
    "href": "labs/lab08.html#exercise-3---building-a-shiny-tool-for",
    "title": "STA 9750 Week 8 In-Class Activity: More ggplot2 + Tools for Interactive Data Analysis",
    "section": "Exercise 3 - Building a shiny tool for STA 9750",
    "text": "Exercise 3 - Building a shiny tool for STA 9750\nUse the course helper functions to build a shiny tool that can be used to verify that your mini-projects and peer feedback have been appropriately formatted and submitted.\n\nStep 0: Run shiny\nTake the following prototype and confirm that it runs on your computer. Note that this is the “single-file” format for shiny apps. It is somewhat more common to split shiny apps across two files - backend and frontend - but we are using a single file structure here for convenience.\n\nif(!require(\"shiny\")) install.packages(\"shiny\")\nlibrary(shiny)\n\nif(!require(\"glue\")) install.packages(\"glue\")\nlibrary(glue)\n\nif(!require(\"yaml\")) install.packages(\"yaml\")\nlibrary(yaml)\n\n\n# Create global variables here\nvariables &lt;- read_yaml(\"https://raw.githubusercontent.com/michaelweylandt/STA9750/refs/heads/main/_variables.yml\")\ncourse_repo  &lt;- variables$course$repo\ncourse_short &lt;- variables$course$short\n\n# Define the UI\nui &lt;- bootstrapPage(\n    textInput('github_id', \n              'What is your GitHub ID?', \n              placeholder=\"Type your GitHub ID here\"),\n    htmlOutput('gh_link')\n)\n\n# Define the server code\nserver &lt;- function(input, output) {\n    output$gh_link &lt;- renderText({\n        markdown(glue(\"My code for this course found on [GitHub](https://github.com/{input$github_id}/{course_repo})\"))\n    })\n}\n\n# Return a Shiny app object\nshinyApp(ui = ui, server = server)\n\n\n\nStep 1: Add a Link to your profile\nAdd a second output to link to the homepage you created in MP#00. Copying the treatment of gh_link in the example, you will need to\n\nAdd a second output renderText component in the server\nAdd a second output htmlOutput component in the UI\n\n\n\nStep 2: Parameterize the Mini-Project Number\nNext, extend your app to handle the mini-projects. Modify the above example to do three things:\n\nAdd a numericInput that takes a number between 1 and 4 inclusive to select a mini-project.\nAdd an output htmlOutput element that adds a link to the relevant mini-project instructions.\nThe glue function can be used to easily substitute values in URLs (as in the example).\nAdd an output htmlOutput element that adds a link to your mini-project submission. This URL should take both the numericInput and the textInput user inputs to construct the URL. (That is, if you change the GitHub name used the URL should point to a different repo)\n\n\n\nStep 3: Confirm Mini-Project Submission\nTake one of the course helper scripts and use it to determine whether a given MP submission is properly formatted. To do so:\n\nLoad the helpers by adding source(\"https://michael-weylandt.com/STA9750/load_helpers.R\") to the top of your file (outside both the UI and the server).\nAdd a reactive input that runs mp_submission_verify and captures the output. The following code snippet will be useful:\n\n\nO &lt;- capture.output({\n    E &lt;- tryCatch(mp_submission_verify(N, github_id), \n                  error=function(e) e)\n})\n\nif(isTRUE(E)){\n    MESSAGE &lt;- \"Mini-Project properly submitted.\"\n} else {\n    MESSAGE &lt;- \"Mini-Project not properly submitted. Issue appears to be:\\n\"\n    MESSAGE &lt;- paste(c(MESSAGE, O[1:2]), collapse=\"\\n\")\n}\n\nwhere you use MESSAGE in creating your output.\n\n\nStep 4: Extensions\nTime allowing, do one or more of the following:\n\nAdapt mp_feedback_locate to provide links to all peer feedback issues. Note that you will need to handle the case of later MPs, who haven’t been assigned yet (i.e., MPs #03 and #04).\nAdapt mp_feedback_verify to check whether your peer feedback has been properly formatted. This will require a new input for the peer’s GitHub ID."
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html",
    "title": "STA 9750  Statistical Modeling",
    "section": "",
    "text": "In this video:\n\n\nSelecting Columns\nSelecting Rows\nModifying Columns and Creating New Columns\n\n\n. . .\nLater:\n\nComputing Summary Statistics with Tidy Tools\n\n. . .\nBut first - review!"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#introduction",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#introduction",
    "title": "STA 9750  Statistical Modeling",
    "section": "",
    "text": "In this video:\n\n\nSelecting Columns\nSelecting Rows\nModifying Columns and Creating New Columns\n\n\n. . .\nLater:\n\nComputing Summary Statistics with Tidy Tools\n\n. . .\nBut first - review!"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#tidy-data",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#tidy-data",
    "title": "STA 9750  Statistical Modeling",
    "section": "Tidy Data",
    "text": "Tidy Data\nTidy data - everything in its proper place:\n\nOne and only one type of information per column\nOne and only one observation per row\nNo values ‘hiding’ in column or row names\n\n. . .\n\nlibrary(palmerpenguins); head(penguins, 3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#tidyverse",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#tidyverse",
    "title": "STA 9750  Statistical Modeling",
    "section": "tidyverse",
    "text": "tidyverse\ntidyverse - Tools for manipulating tidy data\n. . .\nMake sure to load the tidyverse package before using\n\nlibrary(tidyverse)\n\nWill print some messages about conflicts - safe to ignore"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#tidyverse-1",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#tidyverse-1",
    "title": "STA 9750  Statistical Modeling",
    "section": "tidyverse",
    "text": "tidyverse\nTidyverse tools have excellent documentation\n\nVisit tidyverse.org for overall information\nIndividual packages: ggplot2.tidyverse.org, dplyr.tidyverse.org, etc.\n\n. . .\n\nFunction Reference - Detailed information on specific functions\nVignettes / Articles - Big picture tutorials"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#dplyr",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#dplyr",
    "title": "STA 9750  Statistical Modeling",
    "section": "dplyr",
    "text": "dplyr\nYou have already seen one tidyverse package: ggplot2\n. . .\nToday we start with dplyr - ‘grammar of data manipulation’"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select",
    "title": "STA 9750  Statistical Modeling",
    "section": "select",
    "text": "select\nThe select function can be used to select certain columns\n\nselect(penguins, species, bill_length_mm, body_mass_g, sex)\n\n# A tibble: 344 × 4\n   species bill_length_mm body_mass_g sex   \n   &lt;fct&gt;            &lt;dbl&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie            39.1        3750 male  \n 2 Adelie            39.5        3800 female\n 3 Adelie            40.3        3250 female\n 4 Adelie            NA            NA &lt;NA&gt;  \n 5 Adelie            36.7        3450 female\n 6 Adelie            39.3        3650 male  \n 7 Adelie            38.9        3625 female\n 8 Adelie            39.2        4675 male  \n 9 Adelie            34.1        3475 &lt;NA&gt;  \n10 Adelie            42          4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n. . .\nFirst argument is data\nOther arguments are columns to be selected"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-1",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-1",
    "title": "STA 9750  Statistical Modeling",
    "section": "select",
    "text": "select\nContrast\n\nselect(penguins, species, bill_length_mm, body_mass_g, sex)\n\nand\n\npenguins[,c(\"species\", \"bill_length_mm\", \"body_mass_g\", \"sex\")]\n\nNo quotes around column names"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#put-that-in-your-pipe",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#put-that-in-your-pipe",
    "title": "STA 9750  Statistical Modeling",
    "section": "|> - Put that in your Pipe",
    "text": "|&gt; - Put that in your Pipe\nR supports piped operations - feeding the output of one function into another\n\nselect(penguins, species, bill_length_mm, body_mass_g, sex)\n\nis equivalent to\n\npenguins |&gt; select(species, bill_length_mm, body_mass_g, sex)\n\n. . .\nRead |&gt; as “and then”\ndplyr plays very nicely with pipes"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-2",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-2",
    "title": "STA 9750  Statistical Modeling",
    "section": "select",
    "text": "select\nLike vector indexing, negative sign drops columns:\n\npenguins |&gt; select(-species)\n\n# A tibble: 344 × 7\n   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Torge…           39.1          18.7               181        3750 male   2007\n 2 Torge…           39.5          17.4               186        3800 fema…  2007\n 3 Torge…           40.3          18                 195        3250 fema…  2007\n 4 Torge…           NA            NA                  NA          NA &lt;NA&gt;   2007\n 5 Torge…           36.7          19.3               193        3450 fema…  2007\n 6 Torge…           39.3          20.6               190        3650 male   2007\n 7 Torge…           38.9          17.8               181        3625 fema…  2007\n 8 Torge…           39.2          19.6               195        4675 male   2007\n 9 Torge…           34.1          18.1               193        3475 &lt;NA&gt;   2007\n10 Torge…           42            20.2               190        4250 &lt;NA&gt;   2007\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-3",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-3",
    "title": "STA 9750  Statistical Modeling",
    "section": "select",
    "text": "select\n: operator selects a range of columns\n\npenguins |&gt; select(bill_length_mm:body_mass_g)\n\n# A tibble: 344 × 4\n   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1           39.1          18.7               181        3750\n 2           39.5          17.4               186        3800\n 3           40.3          18                 195        3250\n 4           NA            NA                  NA          NA\n 5           36.7          19.3               193        3450\n 6           39.3          20.6               190        3650\n 7           38.9          17.8               181        3625\n 8           39.2          19.6               195        4675\n 9           34.1          18.1               193        3475\n10           42            20.2               190        4250\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-4",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-4",
    "title": "STA 9750  Statistical Modeling",
    "section": "select",
    "text": "select\nselect also supports several “smart” helpers to pick columns\n\npenguins |&gt; select(contains(\"bill\"))\n\n# A tibble: 344 × 2\n   bill_length_mm bill_depth_mm\n            &lt;dbl&gt;         &lt;dbl&gt;\n 1           39.1          18.7\n 2           39.5          17.4\n 3           40.3          18  \n 4           NA            NA  \n 5           36.7          19.3\n 6           39.3          20.6\n 7           38.9          17.8\n 8           39.2          19.6\n 9           34.1          18.1\n10           42            20.2\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-5",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-5",
    "title": "STA 9750  Statistical Modeling",
    "section": "select",
    "text": "select\n\npenguins |&gt; select(ends_with(\"mm\"))\n\n# A tibble: 344 × 3\n   bill_length_mm bill_depth_mm flipper_length_mm\n            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1           39.1          18.7               181\n 2           39.5          17.4               186\n 3           40.3          18                 195\n 4           NA            NA                  NA\n 5           36.7          19.3               193\n 6           39.3          20.6               190\n 7           38.9          17.8               181\n 8           39.2          19.6               195\n 9           34.1          18.1               193\n10           42            20.2               190\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-6",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-6",
    "title": "STA 9750  Statistical Modeling",
    "section": "select",
    "text": "select\nHelpers can be combined using & (intersection) and | (union) operations:\n\npenguins |&gt; select(contains(\"bill\") | ends_with(\"g\"))\n\n# A tibble: 344 × 3\n   bill_length_mm bill_depth_mm body_mass_g\n            &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1           39.1          18.7        3750\n 2           39.5          17.4        3800\n 3           40.3          18          3250\n 4           NA            NA            NA\n 5           36.7          19.3        3450\n 6           39.3          20.6        3650\n 7           38.9          17.8        3625\n 8           39.2          19.6        4675\n 9           34.1          18.1        3475\n10           42            20.2        4250\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-7",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-7",
    "title": "STA 9750  Statistical Modeling",
    "section": "select",
    "text": "select\nPredicate operator can select columns based on properties of values\n\npenguins |&gt; select(where(is.numeric))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-8",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#select-8",
    "title": "STA 9750  Statistical Modeling",
    "section": "select",
    "text": "select\nwhere combines very nicely with other selectors\n\npenguins |&gt; select(where(is.numeric) & ends_with(\"mm\"))"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter",
    "title": "STA 9750  Statistical Modeling",
    "section": "filter",
    "text": "filter\nWhile select is used for column selection, filter is used for row selection\n\npenguins |&gt; filter(bill_length_mm &gt; 40)\n\n. . .\nSimilar syntax to select:\n\nNo need to quote column names\nData is first (piped) argument\nLater arguments specify action"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-1",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-1",
    "title": "STA 9750  Statistical Modeling",
    "section": "filter",
    "text": "filter\nContrast\n\npenguins |&gt; filter(bill_length_mm &gt; 40)\n\nwith\n\npenguins[penguins$bill_length_mm &gt; 40, ]"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-2",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-2",
    "title": "STA 9750  Statistical Modeling",
    "section": "filter",
    "text": "filter\nMultiple filters give the intersection:\n\npenguins |&gt; filter(bill_length_mm &gt; 40, sex == \"female\")\n\n. . .\nUse logical operators to get the union:\n\npenguins |&gt; filter(bill_length_mm &gt; 40 | sex == \"female\")"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-3",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-3",
    "title": "STA 9750  Statistical Modeling",
    "section": "filter",
    "text": "filter\nPipes are very useful for multi-step operations\n\npenguins |&gt; filter(bill_length_mm &gt; 40) |&gt; select(species)\n\n. . .\nCompare to\n\nselect(filter(penguins, bill_length_mm &gt; 40), species)\n\nPiped form lets us read left to right instead of inside out"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-4",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-4",
    "title": "STA 9750  Statistical Modeling",
    "section": "filter",
    "text": "filter\nCan use more complex comparisons in filter:\n\nfilter(penguins, body_mass_g &gt; mean(body_mass_g, na.rm=TRUE))\n\n. . .\nWhat would happen without na.rm=TRUE?\nYou will explore much more complicated examples in the accompanying activity."
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-5",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-5",
    "title": "STA 9750  Statistical Modeling",
    "section": "filter",
    "text": "filter\nCommon mistake - using only a single equals sign for equality\n\npenguins |&gt; filter(sex = \"female\")\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `sex == \"female\"`?"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-extensions",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-extensions",
    "title": "STA 9750  Statistical Modeling",
    "section": "filter extensions",
    "text": "filter extensions\nFunctions implementing extensions of filter:\n\nslice_min\nslice_max\nslice_head\nslice_tail"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-extensions-1",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#filter-extensions-1",
    "title": "STA 9750  Statistical Modeling",
    "section": "filter extensions",
    "text": "filter extensions\nslice_sample is useful for getting a random subset of your data.\n\nUseful if exploring a large data set\n\n\npenguins |&gt; slice_sample(n=10)\n\n# A tibble: 10 × 8\n   species   island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;     &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo    Biscoe             53.4          15.8               219        5500\n 2 Chinstrap Dream              46.6          17.8               193        3800\n 3 Adelie    Biscoe             38.2          20                 190        3900\n 4 Gentoo    Biscoe             48.7          15.7               208        5350\n 5 Adelie    Torgers…           39            17.1               191        3050\n 6 Chinstrap Dream              43.2          16.6               187        2900\n 7 Adelie    Torgers…           35.1          19.4               193        4200\n 8 Adelie    Dream              39.6          18.1               186        4450\n 9 Adelie    Dream              42.3          21.2               191        4150\n10 Chinstrap Dream              49.5          19                 200        3800\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#mutate",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#mutate",
    "title": "STA 9750  Statistical Modeling",
    "section": "mutate",
    "text": "mutate\nSo far we have only done subsetting. What about creating new columns?\n. . .\n\npenguins |&gt; \n    mutate(body_mass_lb = body_mass_g / 453) |&gt;\n    select(contains(\"body\"))\n\n# A tibble: 344 × 2\n   body_mass_g body_mass_lb\n         &lt;int&gt;        &lt;dbl&gt;\n 1        3750         8.28\n 2        3800         8.39\n 3        3250         7.17\n 4          NA        NA   \n 5        3450         7.62\n 6        3650         8.06\n 7        3625         8.00\n 8        4675        10.3 \n 9        3475         7.67\n10        4250         9.38\n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#mutate-1",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#mutate-1",
    "title": "STA 9750  Statistical Modeling",
    "section": "mutate",
    "text": "mutate\n\npenguins |&gt; \n    mutate(body_mass_lb = body_mass_g / 453) |&gt;\n    select(contains(\"body\"))\n\nSimilar syntax to select and filter\n. . .\nTo name the new column, put name on the left of the =.\nIf you leave it out, you get an unhelpful default:\n\npenguins |&gt; mutate(body_mass_g / 453) |&gt; select(contains(\"body\"))\n\n\npenguins |&gt; mutate(body_mass_g / 453) |&gt; select(contains(\"body\")) |&gt; print(n=3)\n\n# A tibble: 344 × 2\n  body_mass_g `body_mass_g/453`\n        &lt;int&gt;             &lt;dbl&gt;\n1        3750              8.28\n2        3800              8.39\n3        3250              7.17\n# ℹ 341 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#mutate-2",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#mutate-2",
    "title": "STA 9750  Statistical Modeling",
    "section": "mutate",
    "text": "mutate\nCan change columns “in place” by careful renaming:\n\npenguins |&gt; \n    select(where(is.character) | where(is.factor)) |&gt;\n    mutate(species = toupper(species))\n\n# A tibble: 344 × 3\n   species island    sex   \n   &lt;chr&gt;   &lt;fct&gt;     &lt;fct&gt; \n 1 ADELIE  Torgersen male  \n 2 ADELIE  Torgersen female\n 3 ADELIE  Torgersen female\n 4 ADELIE  Torgersen &lt;NA&gt;  \n 5 ADELIE  Torgersen female\n 6 ADELIE  Torgersen male  \n 7 ADELIE  Torgersen female\n 8 ADELIE  Torgersen male  \n 9 ADELIE  Torgersen &lt;NA&gt;  \n10 ADELIE  Torgersen &lt;NA&gt;  \n# ℹ 334 more rows"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#mutate-3",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#mutate-3",
    "title": "STA 9750  Statistical Modeling",
    "section": "mutate",
    "text": "mutate\nCan do arbitrary vectorized operations and access recently created columns:\n\npenguins |&gt; \n    select(where(is.numeric)) |&gt;\n    mutate(weight_kg = body_mass_g / 1000,\n           height_m = body_height_mm / 1000, \n           bmi = weight_kg / height_m^2)\n\n(Sadly, don’t actually have height data)"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#mutate-4",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#mutate-4",
    "title": "STA 9750  Statistical Modeling",
    "section": "mutate",
    "text": "mutate\n?mutate has several helpful functions:\n\ncumsum, cummean\nlead, lag\nif_else, case_when"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/statistical_modeling.html#rename",
    "href": "STA9750_ONLINE_SLIDES/statistical_modeling.html#rename",
    "title": "STA 9750  Statistical Modeling",
    "section": "rename",
    "text": "rename\nUse of mutate to rename:\n\npenguins |&gt; mutate(Year = year) |&gt; select(-year)\n\n. . .\nrename combines both steps:\n\npenguins |&gt; rename(Year = year) \n\nNo new functionality - clearer intent"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#introduction-.data-background-imagehttpexample.comimage.png",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#introduction-.data-background-imagehttpexample.comimage.png",
    "title": "STA 9750  Tidy Data",
    "section": "Introduction {.data-background-image=“http://example.com/image.png”}",
    "text": "Introduction {.data-background-image=“http://example.com/image.png”}\nIn this video:\n\n\nIntroduce the concept of Tidy Data\nIdentify Key Aspects of Tidy Data\nLearn to Diagnose Non-Tidy Data\n\n\n\nLater:\n\nManipulation of Tidy Data\n\n\n\nBut first - review!"
  },
  {
    "objectID": "STA9750_ONLINE_SLIDES/tidy1.html#tester",
    "href": "STA9750_ONLINE_SLIDES/tidy1.html#tester",
    "title": "STA 9750  Tidy Data",
    "section": "Tester",
    "text": "Tester"
  }
]