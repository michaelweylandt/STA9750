---
mp_num: "02"
mp_title: "How Do You Do 'You Do You'?"
mp_skills: "Combining Data, Data Visualization"
mp_application: "Survey Date, Social Sciences"
mp_rhetoric: "Market Research"
mp_max_ec: 6
---

{{< include _mpsetup.qmd >}}

## Introduction

Welcome to Mini-Project #{{< meta mp_num >}}! In this mini-project, we will
explore data from the *American Time Use Survey* (ATUS). ATUS is a joint effort
of the US Bureau of Labor Statistics and the US Census Bureau, which attempts
to characterize how Americans spend their time. Like most government data 
products, ATUS data is released in two forms: 

1. *Summary Data*: these are large-sample statistical aggregates that use 
   advanced survey weighting methods to get the most accurate estimates from
   small data sizes. These are not statements about individuals, *per se*, but
   estimates about subpopulations of interest. (*E.g.*, middle aged men spend
   5 hours a week on lawn care) These results are the highest-quality and most
   accurate estimates available, but-as a consumer-you can only get the answers
   the census bureau makes available to you. 
2. *Public Use Microdata*: these are 'raw data' from a representative subset
   of respondents. Because microdata are taken from a subset of the full survey
   data, they are less accurate (smaller samples, higher variance) but they
   allow analysts to ask their own questions of the data. 
   
In this mini-project, we will explore ATUS microdata from the past 20 years. You
will see how other Americans spend their time and compare your findings with
your own time usage patterns. Your will use your findings to estimate the total
amount of unpaid labor in the US. 

In this mini-project, you will: 

1) Practice use of `dplyr` for combining and manipulating data from distinct
   sources
2) Practice use of `ggplot2` to create compelling statistical visualizations. 
   Note that this assignment only requires "basic" `ggplot2` visualizations
   like scatterplots or line graphs; [Mini-Project #03](./mini03.qmd) will 
   require more complex visualization. 
3) Practice summarizing complex analytical pipelines for non-technical audiences.

As with [Mini-Project #01](./mini01.qmd), a major portion of your grade is based
on the _communication and writing_ elements of this mini-project, so make sure
to budget sufficient time for the writing process. Code without context has
little practical value - you are always writing and conveying your analysis to a
specific target audience. 

### Student Responsibilities

Recall our basic analytic workflow and table of student responsibilities:

{{< include _responsibilities.qmd >}}

In this mini-project, you are mainly responsible for data alignment and basic 
statistical analyses. While not the main focus of this mini-project, you are 
also expected to provide basic data visualizations to support your findings; 
the grading rubric, below, emphasizes the `dplyr` tools used in this project, 
but reports without any visualization will be penalized severely. Note that data
visualization will play a larger role in Mini-Project #03.

As before, I will provide code to automatically download and read in the data 
used for this project. Also note that, as compared with Mini-Project #01, I am
providing less 'scaffolding' for tabular display and EDA: students are more 
responsible for directing their own analyses.

### Rubric

{{< var course.short >}} Mini-Projects are evaluated using *peer grading* with 
*meta-review* by the course staff. The following basic rubric will be used for 
all mini-projects:

{{< include _rubric.qmd >}}

At this point, you are responsible for (elementary) data visualization and will
be graded on that aspect of the rubric, as well as the sections on which you were
assessed in [the previous mini-project](./mini01.qmd).

Note that, because I am still providing code to download the data, load it into
`R`, and prepare it for analysis, all reports submitted using my code will 
receive an automatic 10/10 for the 'Data Preparation' element of the rubric. 
Additionally, reports completing all tasks described under [Data Integration and
Exploration](#eda) below should receive a 10/10 for the 'Exploratory Data Analysis'
rubric element. 

Taken together, you are only really responsible for these portions of the
rubric in this assignment: 

- Written Communication
- Project Skeleton
- Tables & Document Presentation
- Data Visualization
- Code Quality
- Analysis and Findings

Reports completing all key steps outlined below essentially start with
20 free points. 

{{< include _submission.qmd >}}

## Mini-Project #{{< meta mp_num >}}: {{< meta mp_title >}}

### Data Acquisition

For this project, we will use the *2003-2024* aggregate microdata files, 
published at <https://www.bls.gov/tus/data/datafiles-0324.htm>. These combine
data in a standard format from the past 21 years of ATUS. 

We will use the following tables: 

- The [ATUS Activity Data File](https://www.bls.gov/tus/datafiles/atusact-0324.zip)
  which contains (self-reported) time spent on various activities from over 
  250,000 Americans over a 20 year period.[^day]
- The [ATUS Respondent File](https://www.bls.gov/tus/datafiles/atusresp-0324.zip)
  which contains demographic information for the survey respondents. 
- The [ATUS Roster File](https://www.bls.gov/tus/datafiles/atusrost-0324.zip) 
  which includes additional demographic information on who was present for
  various activities. *E.g.*, if a mother of two responded to ATUS, the respondent
  file will contain her demographics, while the roster file will also contain
  information on the age of her children if they were present for any activities
  she performed during the reporting period. This is probably the least important
  file and you might not use it often, but I'm making it available if you want it.
- An [instructor-provided CSV](./atus_activity_codes.csv) that can be used to 
  map ATUS activity codes to actual activities.[^atus_activity_codes]
  
[^day]: Note that the survey collects daily activities for a single day 
per respondent; this is not 250,000 Americans all of whom were tracked for 
20 years.
  
[^atus_activity_codes]: This data was extracted from ATUS Lexicon files at
<https://www.bls.gov/tus/lexicons.htm> but the code necessary to convert these
files to a usable format was fragile and well-beyond the scope of this course, 
so I'm providing the cleaned-up data directly.

The following code snippets will download the relevant files into a `data/mp02`
directory in your `{{< var course.repo >}}` folder for use: 

```{r}
library(tidyverse)
library(glue)
library(readxl)
library(httr2)

load_atus_data <- function(file_base = c("resp", "rost", "act")){
    if(!dir.exists(file.path("data", "mp02"))){
        dir.create(file.path("data", "mp02"), showWarnings=FALSE, recursive=TRUE)
    }
    
    file_base <- match.arg(file_base)
    
    file_name_out <- file.path("data", 
                               "mp02", 
                               glue("atus{file_base}_0324.dat"))
        
    if(!file.exists(file_name_out)){
        
        url_end <- glue("atus{file_base}-0324.zip")
        
        temp_zip <- tempfile(fileext=".zip")
        temp_dir <- tools::file_path_sans_ext(temp_zip)
        
        request("https://www.bls.gov") |>
            req_url_path("tus", "datafiles", url_end) |>
            req_headers(`User-Agent` = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0") |>
            req_perform(temp_zip)
    
        unzip(temp_zip, exdir = temp_dir)
        
        temp_file <- file.path(temp_dir, basename(file_name_out))
        
        file.copy(temp_file, file_name_out)
    }
    
    file <- read_csv(file_name_out, 
                     show_col_types = FALSE)
    
    switch(file_base, 
           resp=file |> 
                  rename(survey_weight = TUFNWGTP, 
                         time_alone = TRTALONE, 
                         survey_year = TUYEAR),
           rost=file |>
                  mutate(sex = case_match(TESEX, 1 ~ "M", 2 ~ "F")), 
           act =file |>
                mutate(activity_n_of_day = TUACTIVITY_N, 
                       time_spent_min = TUACTDUR24, 
                       start_time = TUSTARTTIM,
                       stop_time = TUSTOPTIME, 
                       level1_code = paste0(TRTIER1P, "0000"), 
                       level2_code = paste0(TRTIER2P, "00"),
                       level3_code = TRCODEP) 
           ) |> 
                rename(participant_id = TUCASEID) |>
                select(matches("[:lower:]", 
                               ignore.case=FALSE))
}

load_atus_activities <- function(){
    if(!dir.exists(file.path("data", "mp02"))){
        dir.create(file.path("data", "mp02"), showWarnings=FALSE, recursive=TRUE)
    }
    
    dest_file <- file.path("data",
                           "mp02",
                           "atus_activity_codes.csv")
    
    if(!file.exists(dest_file)){
        download.file("https://michael-weylandt.com/STA9750/mini/atus_activity_codes.csv")
    }
    
    read_csv(dest_file, show_col_types = FALSE) 
}
```


::: {.callout-tip title="Task 1: Data Acquisition"}

Using the code above, acquire the relevant ATUS files. Copy the code into
your Quarto document and make sure it runs successfully. 

:::

::: {.callout-caution title="Do Not `git add` Data Files"}

Make sure that `git` is set to ignore data files, such as the one created above.
Check the `git` pane in `RStudio` and make sure that the files created in 
`data/mp02` do not appear. (If you set up your `.gitignore` file correctly in
[MP#00](./mini00.qmd), it should already be ignored.) If it is appearing, you
may need to edit your `.gitignore` file.

Removing a large data file from `git` is possible, but difficult. Don't get
into a bad state!

:::

### Data Cleaning and Preparation 

Use the two functions above to create _four_ data frames suitable for 
your analyses: three of these will be created using the `load_atus_data` 
function with different inputs and one with the `load_atus_activities` function.

After each data frame has been read into `R`, use the baseline type and 
distribution checks described 
[in the previous assignment](./mini01.qmd#exploratory-analysis)
to confirm that you understand the types of all columns in the four tables and
that they will be suitable for your analyses. 

As part of this analysis, review the various tables and begin to map out how
they fit together and will be used for your analysis; see [below]{#ec01} for 
one possible way to visualize and explore data of this format. 

Before proceeding, you will want to consider how you will want to join these
data files together. In this case, there are sometimes *several* possible ways
to join together a pair of tables. This mental checklist will help you maximize
the probability that you're using the correct join for your analysis.[^foolproof]

[^foolproof]: This process isn't fool-proof, but it should cover most use-cases.

1.  What two pieces of information am I looking to connect? *Answer(s)*: two
    columns from different tables. (If the columns are already in the same 
    table, you don't need a join.)
2.  What is the common feature(s) I can use to join together these two tables? 
    *Answer(s)*: one or more columns that have corresponding values in each 
    table. It is not required for the columns to have the same name or the exact
    same set of values in both tables. (If there is not a natural pair of 
    combining columns, you might need to introduce a third table to your 
    analyis.)
3.  Join type: 

    a) Do I want to *guarantee* use all of the rows from Table 1, filling in with
       missing values if no corresponding row is found in Table 2? 
    b) Do I want to *guarantee* use all of the rows from Table 2, filling in with
       missing values if no corresponding row is found in Table 1? 
      
    If "yes/yes", you probably want a `full_join`; if "yes/no", you probably want
    a `left_join`; if "no/yes", you probably want a `right_join`; if "no/no", you
    probably want an `inner_join`.
4.  Construct a potential join: 

    ```{r}
    #| eval: false
    JOIN_TYPE(TABLE_1_NAME, 
              TABLE_2_NAME, 
              join_by(TABLE_1_KEY_COLUMN == TABLE_2_KEY_COLUMN))
    ```
    
    Here, replace all values with the answers from your questions above. (We
    are not considering non-equality joins here.)
5.  Run your join and make sure you get no "many-to-many" or "many-to-one" 
    warnings.
6.  Embed your join into your full analysis
   
For example, if we want to answer the question "Who played guitar in which bands?"
using the `band_members` and `band_instruments` tables in the `dplyr` package,
the join-design flowchart would go something like this: 

1. I want to connect the `plays` column from `band_instruments` to the `band`
   column of `band_members`. 
2. The common feature ("key") connecting the `band_instruments` and `band_members`
   table is the `name` column, which has the same name and consistent values in 
   both tables. 
3. a. I am fine dropping rows from the `band_instruments` table if I am unable
      to determine what band someone was a part of. 
   b. I am fine dropping rows from the `band_members` table if I am unable to
      determine what instrument someone played. 
      
   These suggest I want to use an `inner_join`. 
4. My candidate join is: 
   
    ```{r}
    library(dplyr)
    inner_join(band_members, 
               band_instruments, 
               join_by(name == name))
    ```
5.  This runs without any warnings or other danger signs, so I'm good to proceed.
6.  I can use this join to find one guitarist in my data set with a known band: 

    ```{r}
    library(dplyr)
    inner_join(band_members, 
               band_instruments, 
               join_by(name == name)) |>
        filter(plays == "guitar") 
    ```
    so the one row in our result is John (Lennon) playing guitar in the Beatles.
    Sadly, our datasets were incomplete and we lost track of 
    [Keith Richards](https://en.wikipedia.org/wiki/Keith_Richards), guitarist in
    the Rolling Stones. 

::: {.callout-tip title="Task 2: Practice Joining Data Files"}

Work through the join-design flowchart for the following questions based on the
ATUS data: 

1. How many total hours were recorded on sleeping-type activities? 

   *Hint*: Make sure you identify the correct "level" of task precision to
   construct your join. 
2. How many female participants reported spending no time alone in 2003?

*Note*: You do not need to actually include your answers to these questions in
your submission. You will demonstrate knowledge of proper joining strategies
in the EDA questions and your final deliverable.

:::

When dealing with complex datasets, it is important to be able to interpret
and make use of the documentation to interpret complex data representations. 
For surveys like ATUS, where the respondent gives an interview, the survey
administrator will "code" the qualitative responses into quantitative values: 
*e.g.*, after asking the respondent their gender, which can elicit a wide range
of responses ("male", "man", "guy", *etc.*), the response will be encoded
as a simple `1` or `2` using pre-defined rules. This simplified representation
is easier to work with than free-text responses. When working with data in `R`,
it is typically easier to use a (standardized) set of string values (*e.g.*, 
"male" and "female" or "man" and "woman"), rather than an uninterpretable 
numeric value. 

::: {.callout-tip title="Task 3: Adding Additional Demographic Variables"}

You will need to add a few more demographic variables to the data frames 
generated by `load_atus_data` to complete your analysis. Specifically, modify
`load_atus_data` to additionally report the following information: 

1. How many children under the age of 18 live in the respondent's home?
2. Is the respondent married? 
3. What is the respondent's employment status (employed, unemployed, retired, 
   *etc.*)?
4. Location of activity
5. Is the respondent enrolled in a college or university (not high school)? 

   *Hint*: This will require use of at least two ATUS variables which will need
   to be combined using Boolean operators.

Use the [ATUS Data 
Dictionary](https://www.bls.gov/tus/dictionaries/atusintcodebk0324.pdf) to 
identify appropriate variables and to determine a suitable mapping from 
numeric codes to interpretable values, *e.g.*, recoding the `TESEX` variable
from 1/2 to "M"/"F". You will want to use the `case_match` function to implement
these transformations. 

Note that some of these variables are present in multiple files and can be 
incorporated in the data set in multiple ways.

*Note*: You do not need to have a copy of "my" `load_atus_data` and your 
modified `load_atus_data` function. Just include the final version in your
submission.


:::

### Data Integration and Initial Exploration {#eda}

#### Survey Weighting 

When working with microdata, you rarely want to use 'raw' averages from the
sampled subjects to estimate population averages. Because survey respondent 
distributions rarely match the actual population distributions (*e.g.*, because
younger people are less likely to respond to phone polls than senior citizens),
_weighted_ averages are used to adjust the sample to match the demographics
of the population. 

For example, if a survey has 100 men and 200 women reply, but we want to make
an inference about the US population as a whole (50% male and 50% female), we
will want to upweight the men vs the women so we have "in effect" 
200 men and 200 women, matching the 50/50 split of the population. In particular, 
we would use code like: 

```{r}
data.frame(gender = rep(c("M", "F"), times=c(100, 200)), 
           income = rnorm(300, mean=50000, sd=10000)) |>
    summarize(avg_income = weighted.mean(income, 
                                         w=if_else(gender=="M", 2, 1)))
```


This is not too hard to do with a single simple feature like gender, but these  
calculations can become quite complex when trying to match on a variety of 
demographic attributes (income bracket, gender, race, age, state, urban/rural, 
education level, *etc.*). Thankfully, the Census Bureau provides us with a set 
of pre-computed weights that we can use. These are used to adjust ATUS to broader
estimates of US demographics like the decadal census and the Current Population
Survey (CPS). 

These are represented by the `TUFNWGTP` variable, which I have renamed as 
`survey_weight`. If we want to know how much time the average American spent
alone every year, we might do something like: 

```{r}
#| eval: false
load_atus_data("resp") |> 
    summarize(avg_time_alone = weighted.mean(time_alone, survey_weight))
```

where we would find that the average American spent about 5 hours (298 minutes) 
alone per day over the previous two decades. If we want to break this out 
further by year, we might do something like: 

```{r}
load_atus_data("resp") |> 
    group_by(survey_year) |>
    summarize(avg_time_alone = weighted.mean(time_alone, survey_weight)) |>
    mutate(point_color = if_else(survey_year == 2020, "red", "black")) |>
    ggplot(aes(x=survey_year, 
               y=avg_time_alone, 
               color=point_color)) + 
        theme_bw() + 
        xlab("ATUS Survey Year") + 
        ylab("Average Time Spent Alone [Minutes]") + 
        scale_y_continuous(sec.axis=sec_axis(~ . / 60, 
                                             name="Average Time Spent Alone [Hours]")) + 
        geom_point() + 
        geom_path() + 
        scale_color_identity() + 
        ggtitle("ATUS Population-Weighted Estimates of Average Time Spent Alone", 
                "2020 Estimates (Red) are Unreliable") + 
        labs(caption="Estimates Created Using 2003-2024 Public Use Microdata Samples")
```

We see here an increasing trend in time spent alone, with the post-2020 period
leading to significant increases in time spent alone, perhaps indicative of
broader trends of social isolation.[^covid_weight]

Note that we can use the same weighting variable here and, because the Census
Bureau has done the hard work of developing survey weights for us, the switch
from whole-sample to per-year weights does not require adjustments to our
code beyond the obvious `group_by`. 

[^covid_weight]: ATUS cautions against using 2020 data and the usual weights,
providing an alternative weight variable `TU20FWGT` for 2020 data that also 
weights by date to ensure the "peak social isolation" period of March 18 - May 9
is represented. You can ignore this subtlety, but I'm flagging it here in case
pandemic-effect questions are of interest. 

Appendix J of the [ATUS User's Guide](https://www.bls.gov/tus/atususersguide.pdf)
gives some more worked examples of how these weights can be used, but note that
some of the variable names might differ from what we find in the combined year 
file. The [2003-2024 ATUS
Codebook](https://www.bls.gov/tus/dictionaries/atusintcodebk0324.pdf) is your
best reference for the data used in this analysis. 

#### Exploratory Questions

As in [the previous mini-project](./mini01.qmd), we are going to structure our
EDA around several sets of exploratory questions. This time, I am providing
**three** sets of questions to be answered in different formats: inline values,
tables, plots. 


::: {.callout-tip title="Task 4: Exploratory Questions - Inline Values"}

Answer the following questions: report your answers using 
Quarto's [inline code](https://quarto.org/docs/computations/inline-code.html)
functionality to place the values in a sentence; that is, you should answer in
complete sentences, written as normal text with inline code for computed values.

1. How many different respondents have answered an ATUS survey since 2003?
2. Americans enjoy watching a variety of sports. How many different sports does
   ATUS ask about watching as a "Level 3" task?
3. Approximately what percent of Americans are retired? 

   *Hint*: You will first want to create a new variable for retirement status;
   after creating this variable, use a weighted version of the "mean of Boolean"
   trick to answer this question. 
4. On average, how many hours do Americans sleep per night? 
5. On average, how many hours do Americans with one or more children in the home
   spend taking care of their children (all activities including healthcare and
   involvement in their children's education)? 

See the [previous mini-project](./mini01.qmd#eda) for advice on formatting your
responses. 

:::


::: {.callout-tip title="Task 5: Exploratory Questions - Table Answers"}

Answer the following questions: each of these questions can be answered with a 
table of just a few rows. Use the `gt` package, introduced in the previous MP, 
or a similar package to to present your results in an attractive
'publication-quality' format, not just a "raw" `R` output. 

1. Do retired persons spend more time on lawn-care than people who have 
   full-time employment?
2. How does the amount of time spent on social and recreational activities
   typically compare for married and unmarried people? 
3. Where do high-earning Americans spend the majority of their time? 

   You may define "high-earning" however you see fit. 
4. What are the most common activities reported by Americans when on a bus, 
   train, subway, boat, or ferry? 
5.  How much time per day do Americans spend on *your four favorite activities* 
    and how does this vary by age group? 
   
    *Hint*: The `cut` function can be used to 'bin' a continuous variable.
    This is useful when trying to break a continuous variable into 'buckets' for
    display in a table. *E.g.*,
   
    ```{r}
    #| message: false
    #| warning: false
    library(gt)
    penguins |> 
        mutate(body_mass = cut(body_mass/1000, breaks=6, dig.lab=3)) |> 
        group_by(species, body_mass, .drop=FALSE) |> 
        count() |> 
        ungroup() |> 
        drop_na() |>
        pivot_wider(names_from=body_mass, 
                    values_from=n) |>
        gt() |>
        cols_label(species="Species ðŸ§") |>
        tab_spanner(columns=2:7, 
                    "Body Mass (kg)") |>
        tab_header(title="Distribution of Penguin Body Mass by Species") |>
        tab_source_note(md(
          "Data originally published in by K.B. Gorman, T.D. Williams, and 
          W. R. Fraser in 'Ecological Sexual Dimorphism and Environmental 
          Variability within a Community of Antarctic Penguins (Genus 
          *Pyogscelis*). *PLoS One* 9(3): e90081.
          <https://doi.org/10.1371/journal.pone.0090081>. Later popularized via 
          the `R` package [`palmerpenguins`](https://allisonhorst.github.io/palmerpenguins/)"))
    ```


See the [previous mini-project](./mini01.qmd#eda) for advice on formatting your
responses. 

:::


::: {.callout-tip title="Task 6: Exploratory Questions - Graph Answers"}

Answer the following questions: for each query, produce both i) an appropriate
'publication-quality' plot that answers the question; and ii) a sentence or two
interpreting the plot. Note that the text needs to guide the reader through
the answer: it is not enough to simple 'point to' the plot and expect it to be
self-describing. 

E.g., if the question were "What is the largest species in the `penguins` data?",
you might produce a plot like: 

```{r}
penguins |>
    select(-island, -sex, -year) |>
    mutate(body_mass = body_mass / 1000) |>
    group_by(species) |>
    summarize(across(where(is.numeric), \(x) mean(x, na.rm=TRUE))) |>
    pivot_longer(-species, 
                 names_to="measure",
                 values_to="species_mean") |>
    mutate(measure = case_match(measure, 
        "bill_dep" ~ "Bill Depth (mm)", 
        "bill_len" ~ "Bill Length (mm)", 
        "body_mass" ~ "Body Mass (kg)", 
        "flipper_len" ~ "Flipper Length (mm)")) |>
    mutate(measure = factor(measure), 
           measure = fct_relevel(measure, "Body Mass (kg)")) |> 
    ggplot(aes(x=measure, 
               y=species_mean, 
               fill=species)) + 
    geom_col(position="dodge") + 
    facet_wrap(~measure, scales="free", nrow=1) + 
    theme_minimal() + 
    xlab(NULL) + 
    ylab("Species Mean") + 
    theme(legend.position="bottom", 
          axis.text.x=element_blank(), 
          strip.text=element_text(size=rel(1))) + 
    scale_fill_brewer(type="qual", 
                      palette=2, 
                      name="Species")
```

and include text to the effect of "By mass, Gentoo penguins are the
largest species studied, weighing almost  1.5 kg more than Adelie or
Chinstrap penguins on average; despite this, Gentoo
bills are typically a bit smaller than other species on multiple dimensions, 
and Gentoo flipper lengths are broadly in line. This suggests that
the extra mass of Gentoo penguins may be an artifact of a larger average
torso, but that the other extremities do not scale accordingly." This
sentence directly answers the simplest interpretation of the question
(largest by mass), but it also uses some other data to give a fuller
context. Additionally, while the plot can be used to give precise
figures, the text gives rough figures ("roughly 1.5 kg", "a bit smaller", 
"broadly in line") for the qualitative comparisons.

1. How does the amount of time spent on household activities change
   as Americans age?
2. A major perk of working in education is the freedom to spend summers how
   you wish. What are the most popular activities for Americans employed in
   education during the summer months (June, July, and August)? 
   
   *Hint*: There are several variables that can be used to access the occupation
   of the respondent. Make sure to pick one that gives you enough detail to
   focus your analysis to people on a "school year schedule", but you do not 
   need to be 100% accurate here. 
3. It is often remarked that "education is a privilege wasted on the young". 
   Among respondents who report being enrolled in a higher-education program, 
   how does the amount of time allocated to various educational activities
   (attending class, doing homework, studying, *etc.*) vary by age? Does it seem
   to be true that older students dedicate more time to their education?
4. A ["bump plot"](https://r-graph-gallery.com/web-bump-plot-with-highlights.html)
   is an effective way to display _ranking_ (ordinal) data and to show how it
   changes over time. Use a bump plot to show how the relative interest in
   different sports changes as a function of either age or income (your choice).
   
   *Hint*: Because bump plots require a discretized $x$-axis, the `cut` function
   may again be your friend here. 
   
5. ["Middle-child syndrome"](https://en.wikipedia.org/wiki/Middle_child_syndrome)
   can be explained, at least in part, by the idea that parents have less time
   to spend _per child_ when they have multiple children. 
   
   *E.g.*, a couple with their first child may be able to spend 6 hours per day
   on child care while maintaining their full-time jobs. After their second
   child is born, the parents may be able to find some extra time in their
   schedules and devote 8 hours per day to child care, or approximately four
   hours per child. Consequently, the second child reports getting 33% less 
   parental attention than their holder sibling. 
   
   How does the amount of time parents spend *per child* change as family size
   increases? 
   
As with the other exploratory questions, some of these are intentionally a bit
open-ended and can be answered in several different equally valid ways.

:::

### Final Deliverable: Market Research on Time Use Patterns in Target Demographics

You are the product manager for a start-up hoping to launch a new time-tracking
app. Unlike regular calendar apps, your product will allow users to compare how
they spend their time against other demographically-similar users (*e.g.*, 
"Congratulations! You spent 3 more hours exercising this week than other users
like you!"), with an intent of using gamification to help instill healthy
time-use habits. 

In order to design the user interface of this new app, the graphics design team
has asked you to prepare estimates of 'typical' time use for representative
users in your target demographics. For each of these sample users, you need to
do the following: 

i) Identify typical demographic parameters for your sample users. Depending on 
   the sample user and target demographic, different demographic parameters or
   different ranges may be more appropriate. (*I.e.*, an 18 year-old high-school
   senior and a 25-year new college graduate just settling into their first job
   may only be seven years apart, but they are in very distinct 'life stages'.
   Conversely, a 38 year-old suburban father of two and a 45 year-old suburban
   father of two are pretty comparable.)

   One of your example users may be a woman who is 23 years, 5 months, and 6 
   days old, but you won't have very many respondents in your data set who are 
   the exact same age, so you might define her comparable population to be women
   22 to 26. If you're defining her peer group as "young women fresh out of
   college", you might filter for women age 22 to 26 who have a college degree
   but are not currently enrolled in an educational program. 
   
ii) Identify key activities for your target demographic. These should be 
    activities that are significant time usage and things that your target demo
    is likely to want to maximize. *E.g.*, college students might sleep more
    than middle-aged men, but middle-aged men are likely to care more about 
    getting enough sleep. Similarly, time spent on exercise is likely far less
    than time at work for most employed individuals, but your users are more
    likely to want to prioritize exercise. 
    
iii) Identify a 'target level' and 'typical level' of time spent by your 
     target demographic on each activity. For the typical level, compute 
     'low', 'medium', and 'high' values; for the target level, use your best
     judgement to define a level. 
    
iv)  Determine what fraction of people in your target demographic actually hit
     the target level of time spent on the key activities. 
     

::: {.callout-tip title="Task 7: Time Use Categories for Target Demographics"}

Write up your findings for the product team. Make sure to *define* your target
demographic, the key activities you identified, and give the various time levels.
You can use tables or graphs (or a blend of each) to best communicate your 
finding. 

Your target demographics are: 

i)   "Stay-at-home" dads. 
ii)  College undergraduates. 
iii) A demographic of your choosing. (This can be a demographic group you 
     identify with or one you simple select out of thin air.)
     
:::
    
{{< include _aidisclosure.qmd >}}


{{< include _echeader.qmd >}}


### Extra Credit Opportunity #01: Relationship Diagram {#ec01}

**Possible Extra Credit: Up to 2 Points**

After completing Task 01, examine the structure of your data sets and create a
suitable *data relationship diagram*. An *data relationship diagram* is a visual 
representation of: 

1) The data tables available for an analysis
2) The columns (names and types) present in each table
3) The possible `join` structures that can be used to combine these tables. 

Generate a suitable visualization and include it in your final submission. Tools
like <https://dbdiagram.io> or Canva can be useful for creating this visualization,
but you may ultimately use whatever tool is most convenient for you (even a photo
of an on-paper sketch). In determining the amount of extra credit awarded, peer
evaluators will look at both the aesthetics of the visualization and the accuracy
of the depicted relationships. 

Note that this data is not 'fully normalized' so there may be cross-table
relationships between one or more columns, particularly when connecting a 'wide'
and 'long' data set. Note also that some of the join  keys are a bit more
complex than typical of a standard database set up so you may wish to annotate
when a relationship more complex than simple quality is required. 


### Extra Credit Opportunity #02: Ring Visualizations {#ec02}

**Possible Extra Credit: Up to 4 Points**

After completing Task 07, your engineering team has now started working on the
visual components of the user interface (UI). Using `ggplot2` and extension 
packages, mock up a possible UI based on the Apple Watch 
["Activity Rings"](https://www.apple.com/watch/close-your-rings/). Display your
mock-up showing time use target and typical values for your demographic 
(Target Demographic 3), as well as your own time use over the previous week.
Use this visualization to compare how you "should have" spent your time to how 
you have actually spent your time.[^fake]

[^fake]: If you don't want to reveal your actual time use patterns or if your 
Target Demographic 3 is a group to which you don't belong, feel free to use
made-up values here. No one is actually monitoring how you spend your time 
outside of class.

This must be an easy-to-read visual / graphical display, not a table. Note that
here you are optimizing for a blend of beauty and easy legibility, not 
"statistical clarity" or "informational efficiency", so you can explore
different types of graphics than we have used in class. Make sure to choose a
"modern" color scheme, font choice, *etc.*

Up to four points of extra credit may be awarded for visualizations that are
particularly artful, effective, or otherwise impressive. "Standard" statistical
visualizations (*i.e.*, a generic `ggplot2` bar chart) will not be given extra
credit.

{{< include _footer.qmd >}}

