#!/usr/bin/env python

# Standard Library
from collections import namedtuple
from functools import cache
from datetime import datetime
from glob import glob
import json
import os
import os.path as op
from pathlib import Path
import subprocess
import random
import re
import webbrowser
from copy import deepcopy as dcopy

# PyPI Modules - Nothing here is too exotic, so not worrying about
# environment management / versioning for now
from bs4 import BeautifulSoup as BS
import click
from jinja2 import Environment, FileSystemLoader, select_autoescape
import numpy as np
import pandas as pd
import requests as r
from requests.compat import urljoin, quote, quote_plus
from yaml import safe_load as yaml_load
from pandas.api.types import is_list_like
import math
from math import nan
from statistics import median

Issue = namedtuple(
    "Issue", ["title", "number", "html_url", "body", "state", "opened_by"]
)

Comment = namedtuple("Comment", ["commenter", "text", "ctime", "url"])

# FIXME: This technically isn't sufficient for arbitrary URLs, but it
# seems to cover the whole GitHub API, which is enough for us.
URL_REGEX = "http[A-Za-z0-9:/\\.\\-]+"

SCORES_STR = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"]

def is_nan(x):
    if type(x) == float:
        return math.isnan(x)
    else:
        return False


def str_simplify(x):
    return re.sub("\\s+", " ", x.replace(" +", " ").lower())


def vecho(*args, **kwargs):
    if click.get_current_context().params["verbose"]:
        return click.echo(*args, **kwargs)
    return None


def print_web_to_pdf(url, output_file):
    CHROME = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
    FLAGS = ["--headless", "--disable-gpu"]

    OUTPUT = f"--print-to-pdf={output_file}"
    INPUT = url

    return subprocess.run([CHROME] + FLAGS + [OUTPUT] + [INPUT], check=True)


def open_email(
    *,
    to="michael.weylandt@baruch.cuny.edu",
    subject=None,
    bcc=None,
    body=None,
    flag_auto=True,
):
    ## This function is _incredibly_ brittle, but it seems like,
    ## at a minimum, the "to" field is mandatory.
    URL_BASE = "https://outlook.office.com/mail/deeplink/compose"
    URL_PATH = ""

    if to:
        if not is_list_like(to):
            to = [to]
        URL_PATH += "?mailtouri=mailto:" + quote(",".join(to))

    if subject:
        if body and flag_auto:
            subject = "[AUTOGENERATED: SEE END OF EMAIL FOR CONTENT] " + subject
        URL_PATH += "?subject=" + quote(subject)

    if bcc:
        if not is_list_like(bcc):
            bcc = [bcc]
        URL_PATH += "?bcc=" + quote(",".join(bcc))

    ## Body doesn't play well with signature
    ## See https://stackoverflow.com/q/79482557
    ##
    ## For now, I'll just lump this into the subject and manually copy from there
    if body:
        URL_PATH += "?body=" + quote(body)

    webbrowser.open(URL_BASE + URL_PATH, new=1)


@cache
def git_version():
    "Return the current git hash (for use in auto-posting to student repos"
    # https://stackoverflow.com/a/57683700
    git_describe = subprocess.check_output(["git", "describe", "--always"])
    return git_describe.strip().decode()


@cache
def gh_pat():
    "Read GitHub PAT from relevant file"
    with open(".githubPAT") as f:
        return f.read().strip()


@cache
def _gh(path, get=True, data=None):
    headers = {
        "Accept": "application/vnd.github+json",
        "Authorization": f"Bearer {gh_pat()}",
        "X-GitHub-Api-Version": "2022-11-28",
    }

    fullpath = "https://api.github.com/" + path

    if get:
        return r.get(fullpath, headers=headers).json()
    else:
        return r.post(fullpath, headers=headers, data=data)


@cache
def gh_list_issues(owner=None):
    ## FIXME: Paginate this
    resp = _gh(f"/repos/michaelweylandt/{course_repo()}/issues?state=all&per_page=100")
    if "status" in resp and resp["status"] != "200":
        return dict()

    ISSUES_DICT = {
        # FIXME: Handle duplicate issue titles
        # Common case is students forgetting to fix "<GITHUB_ID>"
        #
        str_simplify(i["title"]): Issue(
            # str_simplify(i["title"].replace("<GITHUB_ID>", i["user"]["login"])): Issue(
            i["title"],
            i["number"],
            i["html_url"],
            i["body"],
            i["state"],
            i["user"]["login"],
        )
        for i in resp
    }

    if owner:
        ISSUES_DICT = {
            k: v
            for k, v in ISSUES_DICT.items()
            if owner.lower() in v.title.lower() or owner.lower() in v.opened_by.lower()
        }

    return ISSUES_DICT


@cache
def gh_list_comments(issue_id):
    resp = _gh(f"/repos/michaelweylandt/{course_repo()}/issues/{issue_id}/comments")
    if "status" in resp and resp["status"] != "200":
        return list()
    return [
        Comment(c["user"]["login"], c["body"], c["created_at"], c["html_url"])
        for c in resp
    ]


def gh_issue_comment(issue_id, text):
    return _gh(
        f"/repos/michaelweylandt/{course_repo()}/issues/{issue_id}/comments",
        get=False,
        data=json.dumps({"body": text}),
    )


def gh_issue_create(title, text):
    return _gh(
        f"/repos/michaelweylandt/{course_repo()}/issues",
        get=False,
        data=json.dumps({"title": title, "body": text}),
    )


@cache
def course_dir():
    click.echo("Reading configuration from _teaching.yml")
    with open("_teaching.yml") as f:
        course_config = yaml_load(f)
        click.echo("Course Configuration: " + str(course_config))

    return op.expanduser(course_config["gradedir"])


@cache
def load_issues():
    issues_dir = op.join(course_dir(), "latest", "issues")

    ALL_ISSUES = []
    for fname in glob(op.join(issues_dir, "*.json")):
        with open(fname) as f:
            RAW_ISSUE = json.loads(f.read())
            ISSUE = {
                "title": RAW_ISSUE["title"],
                "number": RAW_ISSUE["number"],
                "comments": [
                    (c["commenter"], c["text"]) for c in RAW_ISSUE["comments"]
                ],
            }
            ALL_ISSUES.append(ISSUE)

    ALL_ISSUES.sort(key=lambda x: x["number"])

    return ALL_ISSUES


@cache
def course_short():
    with open("_variables.yml") as f:
        course_config = yaml_load(f)

    return course_config["course"]["short"]


@cache
def course_repo():
    with open("_variables.yml") as f:
        course_config = yaml_load(f)

    return course_config["course"]["repo"]


@cache
def roster_file():
    filename = Path(op.join(course_dir(), "roster.json")).resolve(strict=True)
    filename.touch(exist_ok=True)
    return filename


@cache
def grades_file():
    filename = Path(op.join(course_dir(), "grades.json")).resolve()
    filename.touch(exist_ok=True)
    return filename


@cache
def teams_file():
    filename = Path(op.join(course_dir(), "teams.json")).resolve()
    filename.touch(exist_ok=True)
    return filename


@cache
def team_grades_file():
    filename = Path(op.join(course_dir(), "team_grades.json")).resolve()
    filename.touch(exist_ok=True)
    return filename


@cache
def mapping_file():
    filename = Path(op.join(course_dir(), "master_mapping.json")).resolve(strict=True)
    filename.touch(exist_ok=True)
    return filename


@cache
def mapping_index_file():
    filename = Path(op.join(course_dir(), "mapping_indices.json")).resolve(strict=True)
    filename.touch(exist_ok=True)
    return filename


def get_pf_mapping(project_id):
    """Load peer feedback mapping for MP#N.

    Returns a dict of with (REVIEWER_NAME, GITHUB, EMAIL) keys and a list of
    (AUTHOR, URL) pairs as the value."""
    with open(mapping_file(), "r") as f:
        MASTER_MAPPING = json.loads(f.read())

    with open(mapping_index_file(), "r") as f:
        MAPPING_INDEX = json.loads(f.read())
        MAPPING_INDEX = MAPPING_INDEX[f"mp0{project_id}"]

    ROSTER = current_roster()

    MAPPING = {}
    PF_DIR = op.join(course_dir(), f"mp0{project_id}", "assignments")

    for row in ROSTER.itertuples(index=False):
        name = row.name
        gh = row.github
        email = row.email

        key = (name, gh, email)

        value = []
        for ix in MAPPING_INDEX:
            author = MASTER_MAPPING[gh][ix]

            issue = glob(op.join(PF_DIR, f"{author}_*.md"))

            if not issue:
                click.echo(
                    f"No issue assignment found for {author} on MP #0{project_id}. Skpping."
                )
                continue

            if len(issue) > 1:
                click.echo(
                    "ERROR: "
                    + f"Multiple issues found for {author} on MP #0{project_id}"
                )
                raise click.Abort()

            issue_file = op.basename(issue[0])
            issue_num = int(re.search("_issue([0-9]+)\\.md$", issue_file).group(1))

            if not issue_file:
                click.echo(
                    "ERROR: Issue appears to be issue #0. Please confirm all PF properly assigned."
                )
                raise click.Abort()

            issue_url = (
                f"https://github.com/michaelweylandt/{course_repo()}/issues/{issue_num}"
            )
            value.append((author, issue_url))

        MAPPING[key] = value

    return MAPPING


@click.group()
@click.option(
    "-v", "--verbose", count=True, help="Include additional output", default=False
)
@click.option(
    "-q",
    "--quiet",
    is_flag=True,
    help="Suppress default output. If `verbose` is given, it takes precedence over this flag.",
)
def course_manager(verbose, quiet):
    """Course Management Tool (CISSOID) for STA9750

    This tool can be used to perform various administrative things (e.g., roster
    management, peer feedback assignment, etc.) associated with STA 9750."""

    click.echo("Results to be stored in directory: " + course_dir())


@course_manager.group()
def roster():
    """Course Roster Management

    This subcommand manages the course roster. As part of Mini-Project #00, students
    should register their GitHub ID with the instructor. The instructor must log
    these GitHub IDs here. Once the course is underway, the instructor can use
    this command to drop / deactive students, delete them entirely (in rare
    circumstances) or otherwise manage the course. Note that this roster is
    used implicitly by all other commands, so it must be kept up to date."""


def current_roster():
    try:
        return pd.read_json(roster_file())
    except ValueError:
        return pd.DataFrame()


def current_teams():
    try:
        return pd.read_json(teams_file())
    except ValueError:
        return pd.DataFrame()


@roster.command(name="list")
def roster_list():
    """Print current course roster"""
    click.echo(current_roster())


@roster.command(name="add")
@click.option(
    "-g",
    "--github",
    help="Student's GitHub ID",
    prompt="What is the Student's GitHub ID?",
)
@click.option(
    "-e", "--email", help="Student's Email", prompt="What is the Student's Email?"
)
@click.option(
    "-E",
    "--emplid",
    help="Student's CUNY EmplID",
    prompt="What is the Student's CUNY EmplID?",
)
@click.option(
    "-n",
    "--name",
    help="Student's Real Name",
    prompt="What is the Student's Real Name?",
)
def roster_add(github, email, emplid, name):
    """Add a new student to the roster.

    This function is intended for use following MP00. As part of MP00,
    students need to register their GitHub ID and contact info with the
    instructor. This function maintains a roster which is used for all later
    course activities."""
    NEW_STUDENT = pd.DataFrame(
        {
            "github": github,
            "email": email,
            "emplid": emplid,
            "name": name,
            "active": True,
            "project_team": None,
        },
        index=[emplid],
    )

    ROSTER = current_roster()

    if not ROSTER.empty and github in ROSTER.github.values:
        warning("Student with GitHub ID", github, "already in roster.")
        click.confirm("Should I overwrite?", abort=True)

    NEW_ROSTER = pd.concat([ROSTER, NEW_STUDENT], ignore_index=True)

    click.echo("The following student has been added to the roster")
    click.echo(NEW_STUDENT)

    with open(roster_file(), "w+") as f:
        f.write(NEW_ROSTER.to_json(orient="records"))


@roster.command(name="delete")
@click.option(
    "--github",
    help="Student's GitHub ID",
    prompt="What is the Student's GitHub ID?",
    prompt_required=False,
    default=None,
)
def roster_delete(github):
    """Delete a student from the course roster.

    This function should only be used in exceptional circumstances.
    Students who drop the course are better handled by marking them
    as inactive, rather than deleting all records."""
    ROSTER = current_roster()

    click.echo(ROSTER)

    if github is None:
        github = click.prompt(
            "Which student would you like to delete?",
            type=click.Choice(ROSTER.github.values),
        )

    if github not in ROSTER.github.values:
        raise click.Abort("Could not match user to existing roster")

    DROPPER = ROSTER.loc[ROSTER["github"] == github]

    click.echo("Student to be deleted:")
    click.echo(DROPPER)
    click.confirm("Are you sure you want to delete this student?", abort=True)

    NEW_ROSTER = ROSTER.loc[ROSTER["github"] != github]

    with open(roster_file(), "w+") as f:
        f.write(NEW_ROSTER.to_json())


@course_manager.group()
def project():
    """Course Project Management

    This subcommand manages the course project. It includes functionality to create
    groups, add students to them, and to assign grades for group-based work."""


@project.command(name="create")
def project_create():
    "Define a new team. After running this, use 'modify' to add or remove students."

    TEAMS = current_teams()
    n_teams, _ = TEAMS.shape

    new_team_number = n_teams + 1
    NEW_TEAM = pd.DataFrame(
        {
            "number": new_team_number,
            "name": "TBD",
        },
        index=[new_team_number],
    )

    NEW_TEAMS = pd.concat([TEAMS, NEW_TEAM], ignore_index=True)

    click.echo("The following team has been added to the roster")
    click.echo(NEW_TEAM)

    ROSTER = current_roster()

    while True:
        click.echo("The following students have not yet been assigned a team:")
        NO_TEAM = ROSTER[ROSTER["project_team"].isna()]["name"].to_dict()

        for k, v in NO_TEAM.items():
            click.echo(f"- {k}: {v}")

        id_to_add = click.prompt(
            "Enter the number of a student to add to this team. (Leave blank to finish team creation.)",
            default="",
            show_default=False,
        )

        if not id_to_add:
            click.echo("Team creation complete.")
            break

        try:
            id_to_add = int(id_to_add)
        except ValueError:
            click.echo("I was expecting an integer or a blank. Please try again.")
            continue

        if id_to_add not in NO_TEAM:
            click.echo("I don't recognize that key. Please try again.")
            continue

        ROSTER.at[id_to_add, "project_team"] = new_team_number

    click.echo("The following team has been added to the roster")
    click.echo(NEW_TEAM)
    click.echo("The team members are:")
    for nm in ROSTER[ROSTER["project_team"] == new_team_number]["name"]:
        click.echo(f"- {nm}")

    click.confirm("Should I create this team?", abort=True)

    with open(teams_file(), "w+") as f:
        f.write(NEW_TEAMS.to_json(orient="records"))

    with open(roster_file(), "w+") as f:
        f.write(ROSTER.to_json())


@project.command(name="modify")
@click.option("-N", "--number", default=0, help="Number of team to modify")
@click.option(
    "-A",
    "--action",
    type=click.Choice(
        ["Rename", "Add-student", "Remove-Student"], case_sensitive=False
    ),
    help="Which action to take?",
    prompt="Which modification should be performed?",
)
def project_modify(number, action):
    "Add or remove students from a team or change team name"

    TEAMS = current_teams()
    N_TEAMS, _ = TEAMS.shape

    if number > N_TEAMS or number < 0:
        raise click.Abort("Invalid team number provided.")

    if number == 0:
        click.echo("The current teams are:")
        for row in TEAMS.itertuples():
            click.echo(f"Team Number {row.number} - {row.name}")

        number = click.prompt(
            "Which team would you like to modify?",
            type=click.IntRange(min=TEAMS["number"].min(), max=TEAMS["number"].max()),
        )

    ROSTER = current_roster()
    BIG_ROSTER = pd.merge(
        ROSTER,
        TEAMS.rename(columns={"name": "team_name"}),
        how="left",
        left_on="project_team",
        right_on="number",
    )

    TEAM = BIG_ROSTER[BIG_ROSTER["project_team"] == number]

    click.echo("You are about to assign modify the team consisting of:")
    for row in TEAM.itertuples():
        click.echo(f"{row.name} (GH: {row.github}; EMAIL: {row.email})")
    click.echo("\n")

    if action != "Rename":
        raise NotImplementedError

    if action == "Rename":
        current_name = TEAM["team_name"].iloc[0]
        click.echo(f"The current name of this team is {current_name}.")
        new_name = click.prompt("What should the new name of this team be?")

        if new_name in TEAMS["name"]:
            click.echo("WARNING: This team name is already in use. Be careful.")

        click.confirm(
            f"Are you sure you would like to change the name of Team {number} from {current_name} to {new_name}?",
            abort=True,
        )

        TEAMS.loc[TEAMS["number"] == number, "name"] = new_name

        with open(teams_file(), "w+") as f:
            f.write(TEAMS.to_json(orient="records"))


@project.command(name="list")
def project_list():
    "List existing teams (with members) and identify students not yet on a team."
    ROSTER = current_roster()
    TEAMS = current_teams()
    BIG_ROSTER = pd.merge(
        ROSTER,
        TEAMS.rename(columns={"name": "team_name"}),
        how="left",
        left_on="project_team",
        right_on="number",
    )

    for team_num in sorted(BIG_ROSTER["project_team"].unique()):
        if pd.isna(team_num):
            continue

        team_name = TEAMS[TEAMS["number"] == team_num]["name"].item()

        click.echo(
            f"Team Number {int(team_num)} (current name {team_name}) has the following members:"
        )

        SUB_ROSTER = BIG_ROSTER[BIG_ROSTER["number"] == team_num]
        for nm in SUB_ROSTER["name"]:
            click.echo(f"- {nm}")
        click.echo("--------------")

    click.echo("The following students have not yet been assigned a team:")
    SUB_ROSTER = BIG_ROSTER[BIG_ROSTER["project_team"].isna()]

    for nm in SUB_ROSTER["name"]:
        click.echo(f"- {nm}")


@project.command(name="grade")
@click.option(
    "--element",
    type=click.Choice(["Proposal", "Check-In", "Final"], case_sensitive=False),
    help="Which project element to assign grades for?",
    prompt="Which project element should grades be assigned for?",
)
@click.option("-N", "--number", default=0, help="Number of team to grade")
@click.option(
    "--skip-email",
    default=False,
    is_flag=True,
    help="Should email feedback be skipped?",
)
def project_grade(element, number, skip_email):
    "Assign group project grades"

    TEAMS = current_teams()
    N_TEAMS, _ = TEAMS.shape

    if number > N_TEAMS or number < 0:
        raise click.Abort("Invalid team number provided.")

    if number == 0:
        click.echo("The current teams are:")
        for row in TEAMS.itertuples():
            click.echo(f"Team Number {row.number} - {row.name}")

        number = click.prompt(
            "Which team would you like to provide feedback to?",
            type=click.IntRange(min=TEAMS["number"].min(), max=TEAMS["number"].max()),
        )

    ROSTER = current_roster()
    BIG_ROSTER = pd.merge(
        ROSTER,
        TEAMS.rename(columns={"name": "team_name"}),
        how="left",
        left_on="project_team",
        right_on="number",
    )

    TEAM = BIG_ROSTER[BIG_ROSTER["project_team"] == number]
    TEAM_NAME = TEAM["team_name"].iloc[0]

    click.echo("You are about to assign feedback to the team consisting of:")
    for row in TEAM.itertuples():
        click.echo(
            f"{row.name} (GH: {row.github}; EMAIL: {row.email}) - {row.team_name}"
        )

    feedback_dir = op.join(course_dir(), element.lower())
    os.makedirs(feedback_dir, exist_ok=True)

    feedback_file = op.join(feedback_dir, f"team_{number}.txt")

    if op.exists(feedback_file):
        new_feedback = click.confirm(
            f"Feedback already found for Team {number}. Are you sure you want to overwrite?"
        )
    else:
        new_feedback = True

    if new_feedback:
        TEMPLATES = Environment(
            loader=FileSystemLoader("_cissoid/templates"),
            autoescape=select_autoescape(),
        )

        TEMPLATE = TEMPLATES.get_template(f"project_{element.lower()}.md")

        SKELETON = TEMPLATE.render(team_name=TEAM_NAME, members=TEAM.itertuples())

        FEEDBACK = click.edit(text=SKELETON, editor="/usr/bin/nano")

        with open(feedback_file, "w") as f:
            f.write(FEEDBACK)

    with open(feedback_file, "r") as f:
        FEEDBACK_TXT = f.read()

    scores = [int(s) for s in re.findall("points\\): (\\d{,2})", FEEDBACK_TXT)]
    total = sum(scores)

    with open(team_grades_file(), "r") as f:
        file_text = f.read()

        if not file_text:
            ALL_GRADES = dict()
        else:
            ALL_GRADES = json.loads(file_text)

    if element.lower() not in ALL_GRADES:
        ALL_GRADES[element.lower()] = dict()

    ALL_GRADES[element.lower()][number] = total

    with open(team_grades_file(), "w") as f:
        f.write(json.dumps(ALL_GRADES))

    if skip_email:
        return 0

    open_email(
        subject=f"STA 9750 - Team {TEAM_NAME} - Project {element} - Score {total}",
        to=TEAM["email"].to_list(),
        body=FEEDBACK_TXT,
    )


@course_manager.group()
def mini():
    """Mini-Project Management

    This subcommand manages the mini-projects (homework). It includes functionality
    to automatically verify GitHub submissions, to download GitHub submissions,
    to assign peer feedback, to download peer feedback once complete, and to assign
    meta-reviews."""


@mini.command(name="archive")
@click.option(
    "-N",
    "--number",
    "project_id",
    prompt="Which mini-project to archive?",
    type=click.IntRange(min=0, max=4),
)
@click.option(
    "-g",
    "--github",
    multiple=True,
    help="GitHub ID to archive. Can be given multiple times. If blank, all active students in roster.",
)
@click.option("--skip-pdf", is_flag=True, default=False, help="Skip PDF archiving")
def mini_archive(project_id, github, skip_pdf):
    """Archive Student Mini-Project Submissions

    This function will archive all elements of a student's mini-project
    submission. It is designed to be used after deadlines pass, but it can
    be run repeatedly if needed. Specifically, this function will:

    1) Sync the student's Git repository (cloning if needed)
    2) Download all issue comment threads
    3) Export a PDF of their rendered site.

    PDF export depends on a headless use of Chrome/Chromium and
    may be a bit tricky to get working."""
    return _mini_archive(project_id, github, skip_pdf)


def _mini_archive(project_id, github, skip_pdf=False):
    "Actual implementation of mini_archive. Can be called directly"
    if not github:
        github = current_roster().github.values

    latest_dir = op.join(course_dir(), "latest")
    os.makedirs(latest_dir, exist_ok=True)
    now = datetime.now().astimezone().strftime("%Y-%m-%dT%H:%M:%S_%Z")

    for gh in github:
        click.echo(f"Archiving Mini-Project #0{project_id} for User {gh}")

        click.echo("Syncronizing Git Repository")

        student_dir = op.join(course_dir(), "students", gh)
        student_git_dir = op.join(student_dir, "git")
        # Don't worry too much about race conditions / file system issues
        os.makedirs(student_git_dir, exist_ok=True)

        if op.exists(op.join(student_git_dir, ".git")):
            subprocess.run(["git", "-C", student_git_dir, "pull"])
        else:
            student_repo_url = f"https://github.com/{gh}/{course_repo()}"
            subprocess.run(["git", "clone", student_repo_url, student_git_dir])

        if skip_pdf:
            continue

        click.echo("Exporting Web Page as PDF")

        student_web_dir = op.join(student_dir, "pdfs", now)
        # Don't worry too much about race conditions / file system issues
        os.makedirs(student_web_dir, exist_ok=True)
        if project_id > 0:
            student_mp_url = f"https://{gh}.github.io/{course_repo()}/mp0{project_id}"
            student_mp_pdf = op.join(student_web_dir, f"mp0{project_id}.pdf")
        else:
            student_mp_url = f"https://{gh}.github.io/{course_repo()}/"
            student_mp_pdf = op.join(student_web_dir, "index.pdf")

        rr = print_web_to_pdf(student_mp_url, student_mp_pdf)

        if rr.returncode:
            click.echo(f"Failed to export {student_mp_url} to PDF.")

        latest_pdf_dir = op.join(latest_dir, "pdfs")
        os.makedirs(latest_pdf_dir, exist_ok=True)

        pdf_symlink = op.join(latest_pdf_dir, f"{gh}_mp{project_id}.pdf")
        try:
            os.remove(pdf_symlink)
        except FileNotFoundError:
            pass
        os.symlink(student_mp_pdf, pdf_symlink, target_is_directory=True)

    # Now export all comments from my course issues repository
    issues_dir = op.join(course_dir(), "issues", now)
    # Don't worry too much about race conditions / file system issues
    os.makedirs(issues_dir, exist_ok=True)

    click.echo(f"Exporting GitHub Issues to {issues_dir}")

    ISSUES = [issue._asdict() for issue in gh_list_issues().values()]

    for issue in ISSUES:
        click.echo(f" - Exporting Issue #{issue['number']}")
        issue["comments"] = [c._asdict() for c in gh_list_comments(issue["number"])]
        issue_file = op.join(issues_dir, f"issue{issue['number']:03}.json")

        with open(issue_file, "w") as f:
            f.write(json.dumps(issue, indent=4))

    issue_symlink = op.join(latest_dir, "issues")
    try:
        os.remove(issue_symlink)
    except FileNotFoundError:
        pass
    os.symlink(issues_dir, issue_symlink, target_is_directory=True)


@mini.command(name="verify")
@click.option(
    "-N",
    "--number",
    "project_id",
    prompt="Mini-Project submission to verify",
    type=click.IntRange(min=0, max=4),
)
@click.option(
    "-g",
    "--github",
    multiple=True,
    help="GitHub ID to verify. Can be given multiple times. If blank, all active students in roster. ",
)
def mini_verify(project_id, github):
    """Verify that a mini-project was submitted and formatted correctly

    This script can be used to verify correct submission of mini-projects and,
    if necessary, to highlight possible issues. This function does _not_ provide
    feedback for students and is primarily intended for instructor use.

    To process submissions after the deadline passes, use the `process` command
    instead."""
    return _mini_verify(project_id, github)


def _get_submission_urls(project_id, github, check_master=True):
    MP_URL = f"https://michael-weylandt.com/STA9750/miniprojects/mini0{project_id}.html"

    MP_PAGE = r.get(MP_URL)
    MP_TEXT = (
        BS(MP_PAGE.text, features="lxml").find(id="submission-text").get_text().strip()
    )
    BODY = MP_TEXT.replace("<GITHUB_ID>", github)
    EXPECTED_URL = re.search(URL_REGEX, BODY).group()

    if project_id:
        RAW_URL = f"https://raw.githubusercontent.com/{github}/{course_repo()}/refs/heads/main/mp0{project_id}.qmd"
    else:
        RAW_URL = f"https://raw.githubusercontent.com/{github}/{course_repo()}/refs/heads/main/index.qmd"

    if check_master and not r.get(RAW_URL).ok:
        RAW_URL_MASTER = RAW_URL.replace("main", "master")
        if r.get(RAW_URL_MASTER).ok:
            return (EXPECTED_URL, RAW_URL_MASTER)

    return (EXPECTED_URL, RAW_URL)


def _mini_verify(project_id, github):
    "Implementation of mini_verify. Can be called directly by other functions"
    MP_URL = f"https://michael-weylandt.com/STA9750/miniprojects/mini0{project_id}.html"

    MP_PAGE = r.get(MP_URL)
    MP_TEXT = (
        BS(MP_PAGE.text, features="lxml").find(id="submission-text").get_text().strip()
    )

    if not github:
        github = current_roster().github.values

    RESULTS = {g: None for g in github}

    for gh in github:
        click.echo("-----------------------------------")
        click.echo(f"Attempting to verify for user {gh}")
        TITLE = f"{course_short()} {gh} MiniProject #0{project_id}"
        BODY = MP_TEXT.replace("<GITHUB_ID>", gh)

        ISSUES = gh_list_issues(gh)

        if not ISSUES:
            click.echo(f"No issues found for user {gh} in repo {course_repo()}")
            RESULTS[gh] = (False, gh, "NO ISSUES FOUND", None)
            continue

        if str_simplify(TITLE) not in ISSUES.keys():
            click.echo("I could not find an issue with the desired title:")
            click.echo(f" - {TITLE}")
            click.echo("I found issues with the following titles instead:")

            for t in ISSUES.values():
                click.echo(f" - {t.title}")

            RESULTS[gh] = (False, gh, "NO TITLE MATCH", None)
            continue

        ISSUE = ISSUES[str_simplify(TITLE)]

        if ISSUE.state != "open":
            click.echo(f"Issue {TITLE} is not in 'open' state")
            RESULTS[gh] = (False, gh, "ISSUE CLOSED", ISSUE.number)

        SUB_URL = re.search(URL_REGEX, ISSUE.body, re.IGNORECASE)
        EXPECTED_URL = re.search(URL_REGEX, BODY, re.IGNORECASE)

        if EXPECTED_URL is None:
            raise click.Abort("Could not find expected URL in MP Instructions")

        if SUB_URL is None:
            click.echo("No URL found in submission")
            RESULTS[gh] = (False, gh, "NO URL", ISSUE.number)
            continue

        SUB_URL = SUB_URL.group()
        EXPECTED_URL = EXPECTED_URL.group()

        if SUB_URL.lower() != EXPECTED_URL.lower():
            click.echo("Submitted URL does not match MP Instructions")
            click.echo("Expected: " + EXPECTED_URL)
            click.echo("Submitted: " + SUB_URL)

            RESULTS[gh] = (False, gh, "INCORRECT URL", ISSUE.number)
            continue

        SUB_RESP = r.get(SUB_URL)

        if not SUB_RESP.ok:
            click.echo("Submitted URL did not resolve properly")
            click.echo(" - Submitted: " + SUB_URL)

            RESULTS[gh] = (False, gh, "INVALID URL", ISSUE.number)
            continue

        if project_id:
            RAW_URL = f"https://raw.githubusercontent.com/{gh}/{course_repo()}/refs/heads/main/mp0{project_id}.qmd"
        else:
            RAW_URL = f"https://raw.githubusercontent.com/{gh}/{course_repo()}/refs/heads/main/index.qmd"

        RAW_RESP = r.get(RAW_URL)

        if not RAW_RESP.ok:
            # Some students use 'master' instead of 'main' branch
            # so try this as well
            RAW_URL = RAW_URL.replace("main", "master")
            RAW_RESP = r.get(RAW_URL)

        if not RAW_RESP.ok:
            click.echo("Could not identify qmd source")
            click.echo(" - Expected: " + RAW_URL)

            RESULTS[gh] = (False, gh, "NO SOURCE", ISSUE.number)
            continue

        RESULTS[gh] = (True, gh, "SUCCESS", ISSUE.number)

    DF = pd.DataFrame(RESULTS).T
    DF.columns = ["ok", "github", "message", "issue_num"]

    click.echo("-----------------------------------")
    click.echo("----------ANALYSIS COMPLETE--------")

    if DF.ok.any():
        click.echo("-----------------------------------")
        click.echo(f"MP {project_id} successfully verified for: ")
        click.echo(", ".join(DF[DF.ok].github.values))

    if not DF.ok.all():
        click.echo("-----------------------------------")
        click.echo("Problems were identified for the following users.")

        for _, row in DF[DF.ok != True].iterrows():
            if row.issue_num:
                click.echo(
                    f"- GitHub user {row.github} failed with message {row.message}. See https://github.com/michaelweylandt/{course_repo()}/issues/{row.issue_num} for details"
                )
            else:
                click.echo(
                    f"- GitHub user {row.github} failed with message {row.message}. No suitable issue found. See above for details."
                )

    return DF


@mini.group(name="peer")
def peer():
    """Assign, collect, and evaluate peer feedback

    These functions can be used to administer the peer feedback cycle for each
    mini-project. Roughly,

    - Assign: Review all submission issues"""


def _peer_assignment_master():
    """Take all registered students and create a balanced Latin Square

    This function implements a _Balanced Latin Square_ using the algorithm
    of J.V. Bradley JASA 53, pp.525-528(1958). The code is modified from
    https://medium.com/@graycoding/balanced-latin-squares-in-python-2c3aa6ec95b9.
    Once this Latin Square is created, it can be used to create peer assignments
    for the mini-projects."""

    MAPPING_FILE = mapping_file()

    if not op.exists(MAPPING_FILE):
        NAMES = current_roster()["github"]
        n = len(NAMES)

        l = [
            [((j // 2 + 1 if j % 2 else n - j // 2) + i) % n + 1 for j in range(n)]
            for i in range(n)
        ]
        if n % 2:  # Repeat reversed for odd n
            l += [seq[::-1] for seq in l]
        MAPPING_MATRIX = np.asarray(l[:n]) - 1

        MAPPING = {}

        for ix, nm in enumerate(NAMES):
            MAPPING[nm] = NAMES[MAPPING_MATRIX[ix, 1:]].to_list()

        with open(MAPPING_FILE, "w") as f:
            f.write(json.dumps(MAPPING, indent=4))

    with open(MAPPING_FILE, "r") as f:
        MAPPING = json.loads(f.read())

    return MAPPING


def _peer_assignment_mapping(project_id, n_peers=None):
    MASTER_MAPPING = _peer_assignment_master()
    project_id_s = f"mp0{project_id}"

    # Check for previous mappings
    INDEX_TO_PF_FILE = op.join(course_dir(), "mapping_indices.json")

    if not op.exists(INDEX_TO_PF_FILE):
        Path(INDEX_TO_PF_FILE).touch(exist_ok=True)

    with open(INDEX_TO_PF_FILE, "r") as f:
        try:
            INDEX_DICT = json.loads(f.read())

        except json.JSONDecodeError:
            INDEX_DICT = {}

    if project_id_s not in INDEX_DICT:
        if not n_peers:
            raise click.Abort("Cannot assign peer feedback with 0 peers")
        ALL_PREVIOUS = [ix for mp in INDEX_DICT.values() for ix in mp]
        ALL_POSSIBLE = range(len(MASTER_MAPPING))
        AVAILABLE = set(ALL_POSSIBLE) - set(ALL_PREVIOUS)

        NEW_IX = [AVAILABLE.pop() for _ in range(n_peers)]
        INDEX_DICT[project_id_s] = NEW_IX

        with open(INDEX_TO_PF_FILE, "w") as f:
            f.write(json.dumps(INDEX_DICT))

    with open(INDEX_TO_PF_FILE, "r") as f:
        INDEX_DICT = json.loads(f.read())

    INDEX_SET = INDEX_DICT[project_id_s]

    PROJECT_MAPPING = {}

    for nm, pf in MASTER_MAPPING.items():
        PROJECT_MAPPING[nm] = [pf[n] for n in INDEX_SET]

    return PROJECT_MAPPING


@peer.command(name="assign")
@click.option(
    "-N",
    "--number",
    "project_id",
    prompt="Which mini-project to archive?",
    type=click.IntRange(min=0, max=4),
)
@click.option(
    "--dry-run", "dry_run", help="Skip posting to GitHub", default=False, is_flag=True
)
@click.option(
    "--skip-archive",
    "skip_archive",
    help="Skip archiving student submissions",
    default=False,
    is_flag=True,
)
@click.option(
    "-P",
    "--peer-count",
    help="How many peers to assign per submission",
    prompt="How many peers should be assigned for each submission?",
    type=click.IntRange(min=1, max=5),
)
@click.option(
    "-s",
    "--github-skip",
    multiple=True,
    help="GitHub ID to skip submission. This is used when students have a pre-approved extension.",
)
def peer_assign(project_id, peer_count, dry_run, skip_archive, github_skip):
    """Assign Mini-Project Peer Feedback

    This command does several things:

    0) It archives all submissions for that MP
    1) It creates a mapping from submitters to peer reviewers
    2) It automatically creates assignment comments for each student,
       noting any automatically identified issues
    3) It posts comments to GitHub.

    Note that this command should only be used once per assignment.
    """
    if not skip_archive:
        _mini_archive(project_id, [])

    # We begin by loading the peer feedback
    # mapping for this assignment
    ROSTER = current_roster()
    PF_MAPPING = _peer_assignment_mapping(project_id, peer_count)

    PF_DIR = op.join(course_dir(), f"mp0{project_id}")
    os.makedirs(PF_DIR, exist_ok=True)
    os.makedirs(op.join(PF_DIR, "assignments"), exist_ok=True)

    PF_TEMPLATES = Environment(
        loader=FileSystemLoader("_cissoid/templates"), autoescape=select_autoescape()
    )

    STUDENTS = set(ROSTER.github.values)
    STUDENTS -= set(github_skip)

    for gh in STUDENTS:
        post_glob = glob(op.join(PF_DIR, "assignments", f"{gh}_issue*.md"))

        if post_glob:
            continue

        EVALUATORS = PF_MAPPING[gh]
        ISSUE_STATUS = _mini_verify(project_id, [gh])

        SUB_URL, RAW_URL = _get_submission_urls(project_id, gh)

        if ISSUE_STATUS.ok.item():
            gh_issue_num = ISSUE_STATUS.issue_num.item()
            template = PF_TEMPLATES.get_template("success.md")
        elif ISSUE_STATUS.issue_num.item():
            gh_issue_num = ISSUE_STATUS.issue_num.item()
            template = PF_TEMPLATES.get_template("fail_issue_ok.md")
        else:
            try:
                ALL_ISSUES = gh_list_issues(gh)
            except TypeError:
                import pdb

                pdb.set_trace()

            ALL_ISSUES = list(ALL_ISSUES.values())
            ALL_ISSUES.sort(key=lambda i: i.number)

            if ALL_ISSUES:
                click.echo(
                    f"The following GitHub issues opened by {gh} were found on the repository."
                )
                for issue in ALL_ISSUES:
                    if issue.opened_by == gh:
                        click.echo(
                            f" - Issue Number {issue.number}. Title '{issue.title}"
                        )
                click.echo("Where would you like to assign peer feedback?")

                gh_issue_num = input(
                    "Enter the number of the issue above or 0 for opening a new issue: >> "
                )
                gh_issue_num = int(gh_issue_num)
            else:
                click.echo("No issues found. Creating new issue")
                gh_issue_num = 0

            if gh_issue_num:
                template = PF_TEMPLATES.get_template("fail_issue_selected.md")
            else:
                template = PF_TEMPLATES.get_template("fail_issue_created.md")

        post_md = template.render(
            project_id=project_id,
            gh=gh,
            peers=EVALUATORS,
            sub_url=SUB_URL,
            raw_url=RAW_URL,
            check_message=ISSUE_STATUS.message.item(),
        )

        post_file = op.join(PF_DIR, "assignments", f"{gh}_issue{gh_issue_num}.md")

        with open(post_file, "w") as f:
            f.write(post_md)

    if dry_run:
        return 0

    for gh in STUDENTS:
        click.echo(f"Assigning Peer Feedback for {gh}'s MP#0{project_id}")
        post_glob = glob(op.join(PF_DIR, "assignments", f"{gh}_issue*.md"))

        if len(post_glob) != 1:
            raise click.Abort(f"Cannot find unique PF assignment script for {gh}")

        post_file = post_glob[0]

        with open(post_file, "r") as f:
            post_md = f.read()

        post_file_short = op.basename(post_file)

        gh_issue_num = int(re.search("_issue([0-9]+)\\.md$", post_file_short).group(1))

        # Directory with files indicating a successful post
        confirm_dir = op.join(PF_DIR, "confirms")
        os.makedirs(confirm_dir, exist_ok=True)

        # Check to see if issue has already been posted
        confirm_file = op.join(confirm_dir, f"{gh}_mp{project_id}_GITHUBSUCCESS")
        if op.exists(confirm_file):
            click.echo(f"Peer Feedback already assigned for {gh}'s MP#0{project_id}")
            continue  # If we find something that matches, move to next student

        # Else make a post
        if gh_issue_num:
            resp = gh_issue_comment(gh_issue_num, post_md)
            if resp.ok:
                with open(confirm_file, "w") as f:
                    f.write(
                        f"Successfully posted as {datetime.now().astimezone().strftime('%Y-%m-%dT%H:%M:%S_%Z')}"
                    )
                click.echo(
                    f"Peer Feedback successfully assigned for {gh}'s MP#0{project_id}"
                )
            else:
                raise click.Abort("Could not post issue for {gh} at #{gh_issue_num}")
        else:
            title = f"MP #0{project_id} Peer Feedback for {gh} [INSTRUCTOR OPENED]"
            resp = gh_issue_create(title, post_md)
            if resp.ok:
                with open(confirm_file, "w") as f:
                    f.write(
                        f"Successfully posted as {datetime.now().astimezone().strftime('%Y-%m-%dT%H:%M:%S_%Z')}"
                    )
                click.echo(
                    f"Peer Feedback successfully assigned for {gh}'s MP#0{project_id}"
                )
                new_issue_num = int(resp.json()["number"])
                new_file = post_file.replace("issue0", f"issue{new_issue_num}")
                os.rename(post_file, new_file)
            else:
                raise click.Abort("Could not post issue for {gh} at #{gh_issue_num}")


@cache
def _peer_feedback_collect_file(project_id):
    fname = op.join(course_dir(), f"mp0{project_id}", "COLLECTED_PEER_FEEDBACK.json")

    if not op.exists(fname):
        Path(fname).touch(exist_ok=True)

    return fname


@cache
def _peer_feedback_meta_file(project_id):
    fname = op.join(course_dir(), f"mp0{project_id}", "COLLECTED_META_REVIEW.json")

    if not op.exists(fname):
        Path(fname).touch(exist_ok=True)

    return fname

@cache
def grade_dir(project_id):
    dirname = op.join(course_dir(), f"mp0{project_id}", "grades")

    os.makedirs(dirname, exist_ok=True)

    return dirname

@cache
def meta_dir(project_id):
    dirname = op.join(course_dir(), f"mp0{project_id}", "meta")

    os.makedirs(dirname, exist_ok=True)

    return dirname

@peer.command(name="collect")
@click.option(
    "-N",
    "--number",
    "project_id",
    help="Mini-project to process peer feedback",
    prompt="Which mini-project to collect peer feedback for?",
    type=click.IntRange(min=0, max=4),
)
@click.option(
    "--use-existing",
    "rearchive",
    is_flag=True,
    default=True,
    help="Skip archiving and use cached issue comments",
)
@click.option("--skip-pdf", is_flag=True, default=False, help="Skip PDF archiving")
@click.option(
    "-F",
    "--force",
    is_flag=True,
    default=False,
    help="Force re-evaluation of peer feedback.",
)
@click.option(
    "-g",
    "--github",
    multiple=True,
    help="GitHub ID to archive. Can be given multiple times. If blank, all active students in roster.",
)
def peer_collect(project_id, rearchive, skip_pdf, force, github):
    """Collect all peer feedback in a standardized format

    After the peer feedback window ends, this function will parse all GH
    issue comments to collect students' peer feedback. In theory, this is
    fully automatable, but students are bad at following instruction so this
    function will prompt for clarification repeatedly as it runs. This function
    saves its output as it goes so it is safe to terminate mid-execution and
    resume later. Run with '--force' to force regrading from the beginning.

    TODO: Add support for the -g flag to only process certain students."""
    if rearchive:
        _mini_archive(project_id, [], skip_pdf)

    ROSTER = current_roster()
    ALL_STUDENTS = set(ROSTER["github"])
    PF_MAPPING = _peer_assignment_mapping(project_id)

    PEER_FEEDBACK_SKELETON = {
        "scores": {
            "Written Communication": nan,
            "Project Skeleton": nan,
            "Formatting & Display": nan,
            "Code Quality": nan,
            "Data Preparation": nan,
            "Extra Credit": nan,
        },
        "comments": {
            "Written Communication": nan,
            "Project Skeleton": nan,
            "Formatting & Display": nan,
            "Code Quality": nan,
            "Data Preparation": nan,
            "Extra Credit": nan,
        },
        "posts": nan,
        "formatted_properly": nan,
    }

    pf_file = _peer_feedback_collect_file(project_id)

    with open(pf_file, "r") as f:
        try:
            ALL_PEER_FEEDBACK = json.loads(f.read())
            
            if not ALL_PEER_FEEDBACK:
                raise ValueError # Bail to make new feedback file
        except ValueError:
            ALL_PEER_FEEDBACK = {
                s: {vv: dcopy(PEER_FEEDBACK_SKELETON) for vv in v}
                for s, v in PF_MAPPING.items()
            }

    ALL_ISSUES_CONTENTS = load_issues()
    
    if not github: 
        STUDENT_LIST = ALL_PEER_FEEDBACK.keys()
    else: 
        STUDENT_LIST = github

    for student in STUDENT_LIST:
        if "issue_num" in ALL_PEER_FEEDBACK[student]:
            if not force:
                continue

        STUDENT_ISSUES = [
            c for c in ALL_ISSUES_CONTENTS if student.lower() in c["title"].lower()
        ]
        STUDENT_ISSUES.sort(key=lambda c: c["number"])

        num_str = f"#0{project_id}"

        RELEVANT_ISSUES = [s for s in STUDENT_ISSUES if num_str in s["title"]]

        if not RELEVANT_ISSUES:
            click.echo(
                f"I could not find an issue for {student} with '{num_str}' in the title."
            )
            click.echo("Please select the relevant issue from the choices below.")
            click.echo("If you want to see all issues, press 0.")

            for s in STUDENT_ISSUES:
                click.echo(f"- {s['number']}. Title: {s['title']}")

            issue_num = click.prompt(
                "Select the issue to use or press 0.",
                type=click.Choice([str(s["number"]) for s in STUDENT_ISSUES] + ["0"]),
            )

            try:
                issue_num = int(issue_num)
            except ValueError:
                click.echo(
                    "I could not interpret your input as an issue number. Interpreting as 0."
                )
                issue_num = 0

            if issue_num:
                KEY_ISSUE = [r for r in STUDENT_ISSUES if r["number"] == issue_num][0]
            else:
                click.echo(
                    "Please select from all issues. If no issue is appropriate, press 0."
                )
                for i in ALL_ISSUES_CONTENTS:
                    click.echo(f"- {i['number']}. Title: {i['title']}")

                issue_num = click.prompt(
                    "Select the issue to use or press 0.",
                    type=click.Choice(
                        [str(i["number"]) for i in ALL_ISSUES_CONTENTS] + ["0"]
                    ),
                )

                try:
                    issue_num = int(issue_num)
                except ValueError:
                    click.echo(
                        "I could not interpret your input as an issue number. Interpreting as 0."
                    )
                    issue_num = 0

            if not issue_num:
                continue  # Go to next student

            KEY_ISSUE = [i for i in ALL_ISSUES_CONTENTS if i["number"] == issue_num][0]

        elif len(RELEVANT_ISSUES) > 1:
            click.echo(
                f"I found multiple issue for {student} with '{num_str}' in the title."
            )
            click.echo("Please select the relevant issue from the choices below.")

            for r in RELEVANT_ISSUES:
                click.echo(f"- {r['number']}. Title: {r['title']}")

            issue_num = click.prompt(
                "Select the issue to use.",
                type=click.Choice([str(r["number"]) for r in RELEVANT_ISSUES]),
            )

            try:
                issue_num = int(issue_num)
            except ValueError:
                raise click.Abort("I could not interpret your input as an issue number")

            KEY_ISSUE = [r for r in RELEVANT_ISSUES if r["number"] == issue_num][0]
        else:
            KEY_ISSUE = RELEVANT_ISSUES[0]
            
        EVALUATORS = ALL_STUDENTS.intersection(ALL_PEER_FEEDBACK[student].keys())

        for evaluator in EVALUATORS:
            current_posts = ALL_PEER_FEEDBACK[student][evaluator]["posts"]
            has_posts = current_posts and not is_nan(current_posts)
            
            if has_posts and not force:
                continue

            POSTS = "\n----NEW POST----\n".join(
                [
                    text
                    for author, text in KEY_ISSUE["comments"]
                    if author.lower() == evaluator.lower()
                ]
            )

            if not POSTS:
                ALL_PEER_FEEDBACK[student][evaluator]["posts"] = POSTS
                continue

            ALL_FORMATTING = True
            for k in ALL_PEER_FEEDBACK[student][evaluator]["scores"].keys():
                pattern = k + "[:][ ]+(\\d{1,2})"
                match = re.search(pattern, POSTS)

                if match is not None:
                    try:
                        score = int(match.group(1))
                    except ValueError:
                        ALL_FORMATTING = False
                        click.clear()
                        click.echo(f"I was unable to locate a score for '{k}'")
                        click.echo(
                            f"What score should be given to {student}, based on these comments:\n----"
                        )
                        click.echo(POSTS)
                        click.echo("----\n")
                        score = click.prompt(
                            f"Enter a score 0-10 for '{k}' or leave blank to keep NaN",
                            type=int,
                            default=-1,
                        )

                        if score < 0:
                            score = nan

                else:
                    ALL_FORMATTING = False
                    click.clear()
                    click.echo(f"I was unable to locate a score for '{k}'")
                    click.echo(
                        f"What score should be given to {student}, based on these comments:\n----"
                    )
                    click.echo(POSTS)
                    click.echo("----\n")
                    score = click.prompt(
                        f"Enter a score 0-10 for '{k}' or leave blank to keep NaN",
                        type=int,
                        default=nan,
                    )

                ALL_PEER_FEEDBACK[student][evaluator]["scores"][k] = score

            for k in ALL_PEER_FEEDBACK[student][evaluator]["comments"].keys():
                pattern = k + "[ \n]+(.*)[\n]?"
                match = re.search(pattern, POSTS)

                if match is not None:
                    comment = match.group(1)
                else:
                    ALL_FORMATTING = False
                    HEADER = f"""
                    I was unable to extract comments for '{k}'. Please edit
                    this file and leave only the comments on that topic.\n------\n"""
                    SOURCE = HEADER + POSTS
                    comment = click.edit(text=SOURCE, editor="/usr/bin/nano")

                    if not comment:
                        comment = ""

                ALL_PEER_FEEDBACK[student][evaluator]["comments"][k] = comment.strip()

            ALL_PEER_FEEDBACK[student][evaluator]["formatted_properly"] = ALL_FORMATTING
            ALL_PEER_FEEDBACK[student][evaluator]["posts"] = POSTS

            with open(pf_file, "w") as f:
                f.write(json.dumps(ALL_PEER_FEEDBACK))

        # Save the issue number for later just in case
        ALL_PEER_FEEDBACK[student]["issue_num"] = KEY_ISSUE["number"]
        ALL_PEER_FEEDBACK[student]["submission_penalty"] = (
            "INSTRUCTOR" in KEY_ISSUE["title"]
        )

    click.echo(f"All peer feedback processed for MP#0{project_id}")
    click.echo("You may now run 'peer grade' to compute actual grades")


@peer.command(name="grade")
@click.option(
    "-N",
    "--number",
    "project_id",
    help="Mini-Project to grade",
    prompt="Which mini-project to grade using peer feedback?",
    type=click.IntRange(min=0, max=4),
)
def peer_grade(project_id):
    """Consolidate collected peer feedback to assign grades

    After running 'collect', this function will parse the peer feedback
    comments and create text files summarizing feedback for each student.
    The instructor must then upload these to Brightspace to return them
    to the students. This function is very fast and can be safely re-run."""
    pf_file = _peer_feedback_collect_file(project_id)

    ROSTER = current_roster()

    with open(pf_file, "r") as f:
        try:
            ALL_PEER_FEEDBACK = json.loads(f.read() or '{}')
        except ValueError:
            click.echo(
                "ERROR: Feedback file appears to be empty. Run 'peer collect' first."
            )
            raise click.Abort()

    template = Environment(
        loader=FileSystemLoader("_cissoid/templates"), autoescape=select_autoescape()
    ).get_template("pf_grade_for_brightspace.md")

    with click.progressbar(ALL_PEER_FEEDBACK.items()) as bar:
        for student, feedback in bar:
            real_name = ROSTER[ROSTER["github"] == student]["name"].item()
            safe_name = real_name.lower().replace(" ", "_")
            evaluators = [e for e in feedback.keys()]

            try:
                penalty = feedback["submission_penalty"]
            except KeyError:
                penalty = click.confirm(
                    f"Should a submission penalty be applied to {student} on MP#0{project_id}?"
                )

            try:
                issue_num = feedback["issue_num"]
            except KeyError:
                click.echo(
                    f"I somehow lost the issue number for {student} on MP#0{project_id}."
                )
                issue_num = click.prompt("What issue number should I use?", type=int)

            if "submission_penalty" in evaluators:
                evaluators.remove("submission_penalty")
            if "issue_num" in evaluators:
                evaluators.remove("issue_num")

            CATEGORIES = [
                "Written Communication",
                "Project Skeleton",
                "Formatting & Display",
                "Code Quality",
                "Data Preparation",
                "Extra Credit",
            ]

            SCORE_DICT = {c: [] for c in CATEGORIES}
            COMMENTS_DICT = {c: [] for c in CATEGORIES}

            for evaluator in evaluators:
                e_feedback = feedback[evaluator]

                for c in CATEGORIES:
                    try:
                        e_feedback["scores"][c]
                    except TypeError:
                        import pdb

                        pdb.set_trace()

                    if is_nan(e_feedback["scores"][c]):
                        continue

                    SCORE_DICT[c].append(e_feedback["scores"][c])
                    COMMENTS_DICT[c].append((evaluator, e_feedback["comments"][c]))

            try:
                SCORES = {k: median(v) if v else nan for k, v in SCORE_DICT.items()}
                TOTAL = sum(SCORES.values())
            except:
                import pdb

                pdb.set_trace()

            if penalty:
                TOTAL = TOTAL - 5

            if TOTAL < 0:
                TOTAL = 0

            grade_text = template.render(
                project_id=project_id,
                categories=CATEGORIES,
                scores=SCORES,
                scores_dict=SCORE_DICT,
                comments=COMMENTS_DICT,
                name=real_name,
                total=TOTAL,
                penalty=penalty,
                course_repo = course_repo(),
                course_dir=course_dir(),
                issue_num=issue_num,
            )

            grade_file = op.join(grade_dir(project_id), f"{safe_name}.txt")

            with open(grade_file, "w") as f:
                f.write(grade_text)


@peer.command(name="meta")
@click.option(
    "-N",
    "--number",
    "project_id",
    prompt="Which mini-project to meta-review peer feedback for?",
    type=click.IntRange(min=0, max=4),
)
@click.option(
    "--use-existing",
    "rearchive",
    is_flag=True,
    default=True,
    help="Skip archiving and use cached issue comments",
)
@click.option(
    "--skip-pdf", 
    is_flag=True, 
    default=False, 
    help="Skip PDF archiving"
)
@click.option(
    "-F",
    "--force",
    is_flag=True,
    default=False,
    help="Force re-evaluation of peer feedback."
)
def peer_meta(project_id, rearchive, skip_pdf, force):
    """Provide a Meta-Review of Peer-Feedback

    After collecting peer feedback, the course staff (instructor or TAs)
    need to assign a 'meta-review' score (0-10) on the quality of the peer
    feedback given. This function is designed to be used repeatedly and saves
    scores throughout evaluation. Run with '--force' to force regrading from 
    the beginning.

    TODO: Add support for the -g flag to only process certain students."""
    if rearchive:
        _mini_archive(project_id, [], skip_pdf)
        
    mp_str  = f"MP#0{project_id}"
    pf_file = _peer_feedback_collect_file(project_id)
    mg_file = _peer_feedback_meta_file(project_id)

    if not op.exists(pf_file):
        raise click.Abort("Peer feedback file not found: run `peer collect -N {project_id}` to create feedback file at {pf_file}")

    with open(pf_file, "r") as f:
        try:
            ALL_PEER_FEEDBACK = json.loads(f.read())
        except ValueError:
            raise click.Abort("Peer feedback appears to be empty. Run `peer collect -N {project_id}` to create feedback file at {pf_file}")
        
    NUM_PEER_FEEDBACK = sum([isinstance(v, dict) for f in ALL_PEER_FEEDBACK.values() for v in f.values()])

    ROSTER   = current_roster()
    STUDENTS = set(ROSTER["github"])
    
    with open(mg_file, "r") as f:
        try: 
            META_GRADES = json.loads(f.read())
        except json.JSONDecodeError:
            META_GRADES = {s: {} for s in STUDENTS}
    
    for submittor in STUDENTS: 
        FEEDBACK   = ALL_PEER_FEEDBACK[submittor]
        EVALUATORS = STUDENTS.intersection(FEEDBACK.keys())
        RUBRIC = None
        
        for evaluator in EVALUATORS:
            has_feedback = submittor in META_GRADES[evaluator]

            if has_feedback and not force:
                continue
                
            peer_comments = FEEDBACK[evaluator]["posts"]

            if not is_nan(peer_comments) and peer_comments:
                peer_comments = peer_comments.strip()
                SCORES_ASSIGNED = sum([len(v.keys()) for v in META_GRADES.values()])
                click.clear()
                click.echo(f"Meta {SCORES_ASSIGNED}/{NUM_PEER_FEEDBACK} ({SCORES_ASSIGNED / NUM_PEER_FEEDBACK:.1%}): {evaluator} left the following comments on {submittor}'s {mp_str}")
                click.echo("---===---")
                click.echo(peer_comments)
                click.echo("---===---")
                click.echo(f"Based on the above, what meta-review grade should be assigned to {evaluator}?")
            
                while True: 
                    score = click.prompt("Enter a score 0-10, hit r to print the rubric, or s to show the relevant submission:",
                                        type=click.Choice(SCORES_STR + ['r', 's']))
                                        
                    if score == "r":
                        if not RUBRIC:
                            RUBRIC = pd.read_html("https://michael-weylandt.com/STA9750/miniprojects.html")[0]
                        click.echo(RUBRIC)
                    elif score == "s":
                        latest_pdf = op.join(course_dir(), "latest", "pdfs", f"{submittor}_mp{project_id}.pdf")
                        if op.exists(latest_pdf):
                            subprocess.call(["open", latest_pdf])
                        else: 
                            import pdb; pdb.set_trace()
                    else: 
                        score = int(score)
                        STANDARD_COMMENTS = [
                            "NO COMMENTS ON THIS TOPIC FOUND", 
                            "Missing submission - no penalty to evaluator.",
                            "This is really above and beyond - thank you for making the effort.",
                            "Great suggestions - really actionable and will certainly make a difference.",
                            "Comments are directionally correct, but would be better if there was more specificity.",
                            "Solid comments - but there's always some room for constructive cricitism even on the strongest projects, so think about if there's anything you would recommend improving upon.",
                            "Really useful comments. I think your scores are well-calibrated here, but try to be a bit more clear about _why_ you took off a few points in certain sections.",
                            "Good comments, focusing on just a few actionable items - thank you"
                        ]
                        
                        click.echo("Pre-canned comments:")
                        for n, txt in enumerate(STANDARD_COMMENTS):
                            click.echo(f"- {n}: {txt}")
                            
                        comment = click.prompt(f"Any comments for {evaluator}?", type=str)
                        
                        try: 
                            comment = STANDARD_COMMENTS[int(comment)]
                        except ValueError:
                            pass
                        
                        format_check = click.confirm("Was the submission appropriately formatted?", default=True)
                        
                        if not format_check:
                            comment += "\nPlease not that your submission was not formatted according to the provided template: https://michael-weylandt.com/STA9750/miniprojects.html#peer-feedback-template Please attempt to do so in the future or a penalty may be assessed."
                        
                        break
            else:
                score = 0
                comment = "NO PEER FEEDBACK IDENTIFIED. CONTACT INSTRUCTOR IF YOU BELIEVE PEER FEEDBACK SUBMISSION WAS MISSED."
            
            if 'issue_num' in FEEDBACK: 
                url = f"https://github.com/michaelweylandt/{course_repo()}/issues/{FEEDBACK['issue_num']}"
            else: 
                url = None

            META_GRADES[evaluator][submittor] = {
                "score": score, 
                "comment": comment, 
                "url": url
            }
            
            with open(mg_file, "w") as f:
                f.write(json.dumps(META_GRADES, indent=2))
            
    template = Environment(
        loader=FileSystemLoader("_cissoid/templates"), 
        autoescape=select_autoescape()
    ).get_template("meta_review.md")
    
    # Now consolidate feedback and write to file
    
    for student in STUDENTS: 
        feedback = META_GRADES[student]
        scores = [v['score'] for v in feedback.values()]
        real_name = ROSTER[ROSTER["github"] == student]["name"].item()
        safe_name = real_name.lower().replace(" ", "_")
        overall = sum(scores) / len(scores)
        
        grade_text = template.render(
            name = real_name,
            github = student, 
            project_id = project_id, 
            feedback = feedback, 
            overall = overall
            )
        grade_file = op.join(meta_dir(project_id), f"{safe_name}.txt")

        with open(grade_file, "w") as f:
            f.write(grade_text)

    click.echo(f"Meta review files written to {meta_dir(project_id)} can now be loaded to Brightspace.")



@course_manager.group()
def notify():
    """Helper functions to create reminder emails

    These work by creating a mailto URL that Outlook webmail can (supposedly)
    handle. This functionality is pretty sensitive and all emails should be
    reviewed before (manually) being sent."""


@notify.command(name="no-team")
def notify_no_team():
    "Email students who have not formed a STA 9750 course project team"

    ROSTER = current_roster()
    TEAMS = current_teams()
    BIG_ROSTER = pd.merge(
        ROSTER,
        TEAMS.rename(columns={"name": "team_name"}),
        how="left",
        left_on="project_team",
        right_on="number",
    )

    BIG_ROSTER = BIG_ROSTER[BIG_ROSTER["team_name"].isna()]

    BCC = BIG_ROSTER["email"].to_list()

    open_email(
        subject="STA 9750 - No Team Registered",
        bcc=BCC,
        body="Hello,\n\nIf you are receiving this email, it means I have not yet registered you on a team for your STA 9750 course project.\n\nPlease register a team with the instructor as soon as possible. See https://michael-weylandt.com/STA9750/project.html for details.",
    )


@notify.command(name="pf-assigned")
@click.option(
    "-N",
    "--number",
    "project_id",
    prompt="Which mini-project to notify peer feedback for?",
    type=click.IntRange(min=0, max=4),
)
def notify_pf_assigned(project_id):
    PF_MAPPING = get_pf_mapping(project_id)

    template = Environment(
        loader=FileSystemLoader("_cissoid/templates"), autoescape=select_autoescape()
    ).get_template("pf_assignment_email.md")

    for k, assignments in PF_MAPPING.items():
        name, github, email = k

        body = template.render(
            name=name, gh=github, project_id=project_id, assignments=assignments
        )

        open_email(
            to=email,
            body=body,
            flag_auto=False,
            subject=f"STA9750 MP#0{project_id} Peer Feedback Assignments for {github}",
        )


if __name__ == "__main__":
    course_manager()
