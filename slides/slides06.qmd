---
session: "`r session <- 6; session`"
class_date:
  "`r library(tidyverse); 
    tuesday_date <- read_csv('key_dates_tuesday.csv', name_repair='universal') |>
    filter(Course.Element == 'Class Session', 
           Item.Number == session) |>
    pull(Date) |> format('%a %Y-%m-%d'); 
    thursday_date <- read_csv('key_dates_thursday.csv', name_repair='universal') |>
    filter(Course.Element == 'Class Session', 
           Item.Number == session) |>
    pull(Date) |> format('%a %Y-%m-%d'); 
    paste(c(tuesday_date, thursday_date), collapse=' <br>')`"
---

{{< include _setup.qmd >}}

```{r}
#| include: false
#| warning: false
#| message: false
library(tidyverse)
DATES   <- read_csv("key_dates.csv")
TODAY   <- DATES |> 
    filter(`Course Element` == "Class Session", 
           `Item Number` == session)

TODAY_TUESDAY  <- TODAY |> 
    filter(Section == "Tuesday")  |> 
    pull(Date)
TODAY_THURSDAY <- TODAY |> 
    filter(Section == "Thursday") |> 
    pull(Date)

TODAY_TOPIC <- TODAY |>
    pull(Details) |>
    unique()

ROSTERS <- DATES |> 
    filter(Details == "Team Roster Submission") |>
    pull(Date)

PROPOSALS <- DATES |> 
    filter(str_detect(Details, "Project Proposal")) |> 
    pull(Date) |> 
    strftime("%A %b %d") |> 
    paste(collapse=" and ")

library(jsonlite)
library(yaml)

N_TEAMS <- read_yaml("_teaching.yml")$gradedir |>
    file.path("roster.json") |> 
    read_json(simplify=TRUE) |> 
    pull(project_team) |> 
    max(na.rm=TRUE)

N_ON_TEAM <- read_yaml("_teaching.yml")$gradedir |>
    file.path("roster.json") |> 
    read_json(simplify=TRUE) |> 
    filter(!is.na(project_team)) |> 
    NROW()

N_NO_TEAM <- read_yaml("_teaching.yml")$gradedir |>
    file.path("roster.json") |> 
    read_json(simplify=TRUE) |> 
    filter(is.na(project_team)) |> 
    NROW()
```
    
## {{< var course.short >}} Week {{< meta session >}} 

Today: 

- Tuesday Section: `r TODAY_TUESDAY`
- Thursday Section: `r TODAY_THURSDAY`

. . . 

`r TODAY_TOPIC`

# Today

## Today

- Course Administration
- Mini-Project #01 Reflection
- Project Proposal Presentations
- Wrap-Up
  - Life Tip of the Day
- *Time Allowing*: `dplyr` vs `SQL`

## MP#01 - Peer Feedback

Assigned on GitHub - due on `r get_mp_pf_deadline(1)`

- $\approx 4$ feedbacks each 
- Take this seriously: around 20\% of this assignment is "meta-review"
- Goal: _rigorous_ _constructive_ critique

Use [helper functions](https://michael-weylandt.com/STA9750/tips.html#find-peer-feedback-assigned-to-me) 
to find submissions assigned to you. Ask on Piazza if still having
trouble.

## MP#01 - Peer Feedback

Submissions may not map perfectly to rubric - use your best judgement

. . . 

Be _generous_ but serious: 

- Goal is improvement, so "everything is great, no comments" is unhelpful
  - Nothing is completely right nor completely wrong
- Remember, *meta-review* (TA scores of your feedback) to follow
- Extra Credit is limited per instructions; only award for exceptional work

. . .

Learn from this! What can _you_ adapt for MP#02? 

## MP#01 - Peer Feedback {.scrollable}

Example of poor feedback: 

::: {.smallest}

```
Scores
Written Communication: 10
Project Skeleton: 10
Formatting & Display: 10
Code Quality: 10
Data Preparation: 10
Extra Credit: 10

Comments
Website looks really good, nice and clean!

Written Communication
Excellent and straight to the point.

Project Skeleton
Solid skeleton, well-organized .

Formatting & Display
Nicely formatted and balanced across the website.

Code Quality
Code runs like Forrest Gump in a Slump.

Data Preparation
Very great. 

Extra Credit
Added graphes for even better understanding.
```

:::

- Improper formatting
- Superficial comments / no sign of actually reading work

Red Flag: Repeated verbatim on several posts

Reminder: Poor feedback $\neq$ poor work.

## MP#01 - Peer Feedback {.scrollable}

Example of medium feedback: 

::: {.smallest}

```
## Scores 

- Written Communication: 8
- Project Skeleton: 10
- Formatting & Display: 9
- Code Quality: 10
- Data Preparation: 10
- Extra Credit: 1

## Comments

Love the visuals for Press Releases.

### Written Communication

A short summary or a description of this project would be great to add.

### Project Skeleton

Code completes all instructor-provided tasks correctly.

### Formatting & Display

Tables have well-formatted column names; caption would be great.

### Code Quality

Code is clear and well written.

### Data Preparation

Automatic (10/10). Out of scope for this mini-project.

### Extra Credit

I found all the Press Releases very interesting. The visuals were a great touch.
```

:::

- Proper formatting
- Gave actionable suggestions
- Directionally correct, but a bit vague

## MP#01 - Peer Feedback {.scrollable}

Example of great feedback: 

::: {.smallest}

```

## Scores

- Written Communication: 9
- Project Skeleton: 10
- Formatting & Display: 9
- Code Quality: 9
- Data Preparation: 10
- Extra Credit: 2

## Comments

### Written Communication

Overall, your writing is clear and easy to follow. I noticed a few small typos, but nothing major ‚Äî using the built-in spell check in RStudio should catch those quickly. Everything else looks solid, and I didn‚Äôt have any major concerns based on my review.

### Project Skeleton

All tasks were completed satisfactorily. 

### Formatting & Display

Overall, your tables and figures are well-organized and clear. There is one table in the "Data" section with column titles that could be formatted a little more cleanly for easier reading. I also noticed a few small typos in some of the table captions, but they should be easy to fix.

### Code Quality

The code quality is generally good, but there are a few minor linter issues. The comments could be more frequent and clearer in some places. For example, the comment `#checking dupolicate so that github will not block it` should be corrected to "duplicate," and the explanation could be clarified to avoid confusion‚Äìwhat duplicate are you referring to, and how does it block GitHub? It might confuse someone else reading the code.

Also, I would recommend moving all library imports to the top of the script, as that is typically considered standard practice.

### Data Preparation

The data preparation looks solid overall. I like how you handle missing files and JSON parsing failures‚Äìit demonstrates strong defensive programming.

### Extra Credit

Additional two points for using quarto's video support!
```


:::

- Positive tone
- Detailed comments
- Noted issues & gave suggestions on how to fix
- Noted unclear sections for improvement


## MP#01 - Impressions

üëè Thank you for excellent work here! üëè

. . .

I've started reading some submissions - really impressive! 

- Plots (exciting!)
- Sophisticated and Insightful Analyses
- Lots of *K-Pop Demon Hunters*
- Fun for me to get to learn your interests

. . . 

Hopefully automated checks helpful. If you have ideas for
other useful automated 'supports' let me know

## MP#01 - Peer Feedback

Lack of prior experience _is not a hinderance_ here: 

- If something is unclear to you, that's a problem!
- Nothing required super-complex code, so anything overly complex probably could
  have been a simpler way (except for some above-and-beyond stuff)
- You don't have to be definitive in comments - impressions and questions are
  just as helpful.

## MP#01 - Common Questions

Q: Why doesn't my site look the same on GitHub as it does on my laptop?

. . . 

A: Missing `css` and `js` files. Need to _upload_ everything in `docs`. 

. . . 

New helper function: 

```{r}
#| eval: false
#| echo: true
source("https://michael-weylandt.com/STA9750/load_helpers.R")
mp_submission_ready()
```

Tries to make sure all 'secondary' files are present. 

## MP#01 - Common Questions

Q: Why doesn't my submission have the right URL? 

. . . 

A: _Most_ computers have case-sensitive file names, Mac doesn't, and Windows is iffy. 

. . . 

New helper function: 

```{r}
#| eval: false
#| echo: true
source("https://michael-weylandt.com/STA9750/load_helpers.R")
mp_start()
```

Generates a very basic `qmd` file with the correct file name and some relevant
context. 


## Mini-Project #02

[MP#02 released](../miniprojcts/mini02.html) - Housing Affordability

. . .

**Due`r get_mp_deadline(2)`** üéÉ [Note change!]

- GitHub post (used for peer feedback) AND Brightspace
- $\approx$ One Month: don't wait until the very end

. . .

Pay attention to [the rubric](../miniprojects/mini02.html#rubric)

-   Writing and presentation are about 50% of your grade
-   Evaluated on *rigor* and *thoughtfulness*, not necessarily correctness

::: {.smallest}

Thanks to HI and SK for help with Windows-compatibility!

:::

## Upcoming Mini-Projects

Tentative Topics

-   MP#03: NYC Trees - `r get_mp_deadline(3)`
-   MP#04: US Employment Surveys - `r get_mp_deadline(4)`

Likely will push back 1 week each - TBD

## Pre-Assignments

Brightspace - day before class

- Reading, typically on course website
- Brightspace auto-grades
  - I have to manually change to completion grading

Next pre-assignment is **`r get_pa_deadline(next_session)`**

. . . 

I missed a few comments in the previous cycle (sorry!) - trying to catch up 
in the next few days

## Course Support

- Synchronous
  - Virtual Office Hours 2x / week on Class Days
- Asynchronous
  - Piazza ($<20$ minute average response time)

## Upcoming Week

Due Wednesday at 11:45pm:

- Pre-Assignment #07 (Brightspace)
  - **Introduction to plotting with `ggplot2`**

Expect back:

-   MP#01 grades
-   Project proposal instructor feedback

## Project Proposals

[Official Description](https://michael-weylandt.com/STA9750/project.html#project-proposal-presentations)

- **6 minute presentation**
- Key topics:
  - *Animating Question*
  - *Team Roster*
- Also discuss: Possible specific questions, data sources, 
  analytical plan, anticipated challenges

. . . 

Most important: team names! 

Previous: Rat Pack, Subway Surfers, Going for Gold, *etc.*

## After Proposals

100% optional discussion of `dplyr` vs `SQL`

- `SQL` is a very common Data Scientist interview topic so if you're not taking
  a database course, might be useful

# On to the Show!

## Tuesday Presentations {.scrollable}

```{r}
#| echo: false
library(jsonlite)
library(yaml)

ROSTER <- read_yaml("_teaching.yml")$gradedir |> 
    file.path("roster.json") |> 
    read_json(simplify=TRUE)

TEAMS <- read_yaml("_teaching.yml")$gradedir |> 
    file.path("teams.json") |> 
    read_json(simplify=TRUE)

library(DT)

PRESENTATIONS <- ROSTER |>
    select(github, active, project_team) |> 
    filter(active) |>
    left_join(TEAMS, join_by(project_team == number)) |>
    mutate(team_shortname = str_extract(name, "(.*) \\(([TR])\\)", group=1), 
           team_day = str_extract(name, "(.*) \\(([TR])\\)", group=2)) |>
    group_by(team_shortname) |>
    summarize(team_day = first(team_day), 
              members = paste(github, collapse=", "))
```

::: {.smaller}

```{r}
#| echo: false
set.seed(9750)
PRESENTATIONS |>
    filter(team_day == "T") |>
    slice_sample(n=100) |>
    select(-team_day) |> 
    rename(`Team Name` = team_shortname, 
           Members=members) |> 
    mutate(`Presentation Order` = row_number()) |>
    select(`Presentation Order`, `Team Name`, Members) |>
    datatable(options=list(info=FALSE,
                           paging=FALSE,
                           searching=FALSE,
                           ordering=FALSE), 
              rownames=FALSE)
```

:::

## Thursday Presentations {.scrollable}

First: `lencon279, BH2xj, aratikalor26, rashika-auti, rohansamavedam`

Then: 

::: {.smaller}

```{r}
#| echo: false
set.seed(9750)
PRESENTATIONS |>
    filter(team_day == "R") |>
    slice_sample(n=100) |>
    select(-team_day) |> 
    rename(`Team Name` = team_shortname, 
           Members=members) |> 
    mutate(`Presentation Order` = row_number()) |>
    select(`Presentation Order`, `Team Name`, Members) |>
    datatable(options=list(info=FALSE,
                           paging=FALSE,
                           searching=FALSE,
                           ordering=FALSE), 
              rownames=FALSE)
```

:::

# Wrap-Up

## Orientation

- Communicating Results (`quarto`)  ‚úÖ
- `R` Basics  ‚úÖ
- Data Manipulation in `R`  ‚úÖ
- Data Visualization in `R`  ‚¨ÖÔ∏è
- Getting Data into `R`
- Statistical Modeling in `R`

## Upcoming Work

Upcoming work from [course calendar](../index.qmd#calendar)

::: {.incremental}

- [Mini-Project #01 Peer Feedback](../miniprojects.html) due on `r get_mp_pf_deadline(1)`
- [Pre-Assignment #0`r next_session`](../preassigns/pa07.html) due `r get_pa_deadline(next_session)`
- [Mini-Project #02](../miniprojects/mini02.html) due on `r get_mp_deadline(2)` [Note change!]

:::


{{< include ../advice/register_to_vote.qmd >}}

## Musical Treat

</br>

{{< video https://www.youtube.com/watch?v=LBIo5QuEeaA width="80%" height="400px">}}

# Optional Material (Time Allowing)

## SQL

*S*tructured *Q*uery *L*anguage (ISO 9075)

::: {.incremental}

- Standard language for interacting with _Relational Database 
  Management Systems_ (RDMS)
- Divided into three main sub-languages: 
  - Data Manipulation Language (DML)
  - Data Declaration Language (DDL)
  - Data Control Language (DCL)
- *Declarative*, not *Imperative* Programming

:::

## SQL Languages

Data Definition Language (DDL)

::: {.small}

- "Design" language for DB
- What tables are present? What types are the columns? What relationships (keys)
  are enforced? 
  
:::

. . . 

Data Manipulation Language (DML)

::: {.small}

- "User" interaction with DB: queries, adding, changing, deleting data
- `SELECT`, `GROUP BY`, `WHERE`, `HAVING`, `JOIN`, *etc.*
- Roughly coextensive with `dplyr`

:::



. . . 

Data Control Language (DCL)

::: {.small}

- "Security" language for DB
- What permissions do different users have? 

:::

## RDMS

Relational Database Management System (RDMS)

::: {.incremental}

- Data management server for complex environments
- _Relational_ tabular data structures
- Manipulated using SQL
  - Common SQL Standard + _Many_ RDMS-specific extensions
- ACID Guarantees

:::

## DDL 

RDMS has a fixed number of _tables_ (roughly `data.frame`s). DDL specifies

- Column names and type
- Can values be left blank (nullability)?
- Identifiers (Primary Keys)
- Constraints (non-negative, `a < b`, *etc.*)
- Relationships to other tables (Foreign Keys)

## DDL 

Basic command is the `CREATE TABLE` statement.

```{sql}
#| eval: false
CREATE TABLE table_name (
  column_name1 TYPE1 PRIMARY KEY -- <- Guaranteed Unique and not blank
  column_name2 TYPE2 NOT NULL -- <- Can't be left blank
  column_name3 TYPE3 -- <- Can be blank
  column_name4 TYPE4 DEFAULT 0 -- <- Has default value
);
```

`TYPE` can be `INTEGER`, `DECIMAL`, `CHAR`, `VARCHAR`, `DATE`, 
`TIMESTAMP` and more

## DDL 

Our beloved `penguins`: 

```{sql}
#| eval: false
CREATE TABLE penguins (
  species VARCHAR(9) NOT NULL, -- Name of up to 9 characters
  island  VARCHAR(9) NOT NULL, -- Also can't be left blank
  bill_len NUMERIC,
  bill_dep NUMERIC,
  flipper_len INTEGER,
  body_mass INTEGER, 
  sex VARCHAR(6), -- Name of 6 (female) but also allow missing
  year INTEGER NOT NULL
);
```

Compare: 

```{r}
glimpse(penguins)
```

## DDL 

We can add a _constraint_ so the year is 2007-2009: 

```{sql}
#| eval: false
CREATE TABLE penguins (
  species VARCHAR(9) NOT NULL, -- Name of up to 9 characters
  island  VARCHAR(9) NOT NULL, -- Also can't be left blank
  bill_len NUMERIC,
  bill_dep NUMERIC,
  flipper_len INTEGER,
  body_mass INTEGER, 
  sex VARCHAR(6), -- Name of 6 (female) but also allow missing
  year INTEGER NOT NULL CHECK (year > 2006 AND year < 2010)
);
```

## DDL 

Constraints for categorical variables (`R` `factor`s) can be tricky. 
Create a _normalized_ set of tables: 

::: {.smaller}

| Sex ID | Name   | 
|--------|--------|
| 1      | Female |
| 2      | Male   |

| Penguin ID | Sex ID | Weight |
|------------|--------|--------|
|       1    |     1  |   4000 |
|       2    |     1  |   3500 |
|       3    |     2  |   5000 |

:::

. . . 

We want to make sure each `Sex ID` in table 2 matches a row of the first table

## DDL 

Constraints for categorical variables (`R` `factor`s) can be tricky. 
Create a _normalized_ set of tables: 

```{sql}
#| eval: false
CREATE TABLE sex (
  sex_id INTEGER PRIMARY KEY, 
  sex_name VARCHAR(6) NOT NULL
)

CREATE TABLE penguins (
  species VARCHAR(9) NOT NULL, -- Name of up to 9 characters
  island  VARCHAR(9) NOT NULL, -- Also can't be left blank
  bill_len NUMERIC,
  bill_dep NUMERIC,
  flipper_len INTEGER,
  body_mass INTEGER, 
  sex_id INTEGER, 
  year INTEGER NOT NULL CHECK (year > 2006 AND year < 2010), 
  FOREIGN KEY (sex_id) REFERENCES sex(sex_id)
);
```

## DDL Normalization

Normalization: 

1) Increases integrity via constraints
2) Reduces redundancy (need to change value in 1 place only)
3) Slight performance benefits

. . . 

More `JOIN` work in `SQL` than in `dplyr`

. . . 

Proper table design is a science and an art

## DML 

**Data Manipulation Language** is for DB users. 

- Close parallels to `dplyr`
- Compare: 

```{r}
#| eval: false
penguins
```

*vs.*

```{sql}
#| eval: false
SELECT * FROM penguins; 
```

`SELECT *` means _all columns_ which matches `R`'s default

## DML 

**Data Manipulation Language** is for DB users. 

- Close parallels to `dplyr`
- Compare: 

```{r}
#| eval: false
penguins |> select(species, island, bill_dep)
```

*vs.*

```{sql}
#| eval: false
SELECT species, island, bill_dep FROM penguins; 
```

## DML 

**Data Manipulation Language** is for DB users. 

- Close parallels to `dplyr`
- Compare: 

```{r}
#| eval: false
penguins |> select(species, island, bill_dep) |> filter(bill_dep > 20)
```

*vs.*

```{sql}
#| eval: false
SELECT species, island, bill_dep FROM penguins WHERE bill_dep > 20; 
```

## DML 

Note that

```{r}
#| eval: false
penguins |> select(species, island, bill_dep) |> filter(bill_dep > 20)
```

and 

```{r}
#| eval: false
penguins |> select(species, island, bill_dep) |> filter(bill_dep > 20)
```

are both 

```{sql}
#| eval: false
SELECT species, island, bill_dep FROM penguins WHERE bill_dep > 20; 
```

. . . 

Why? 

## DML 

- `R` is an *imperative* paradigm - you say _what to do_
- `SQL` is a *declarative* paradigm - you say _what you want_ 

. . . 

Declarative paradigms let the server figure out the _best_ way to compute, but
are more restrictive

. . . 

For a well-defined paradigm like SQL, _much_ work has gone into query optimization

## DML 

- Compare: 

```{r}
#| eval: false
penguins |> group_by(species) |> summarize(mean_mass = mean(body_mass, na.rm=TRUE))
```

*vs.*

```{sql}
#| eval: false
SELECT species, AVERAGE(body_mass) FROM penguins GROUP BY species;
```

`summarize()` is implicit in `SELECT` clause when we have a `GROUP BY`

## DML 

But:

```{r}
#| eval: false
penguins |> 
  group_by(species) |> 
  summarize(mean_mass = mean(body_mass, na.rm=TRUE)) |>
  filter(mean_mass > 4000)
```

is 

```{sql}
#| eval: false
SELECT species, AVERAGE(body_mass) AS avg_mass FROM penguins 
  GROUP BY species HAVING avg_mass > 4000;
```

. . . 

Why not `WHERE` again? 

## DML 

In `SQL`:
  - `WHERE` happens before a `GROUP BY`; and 
  - `HAVING` happens after a `GROUP BY`
  
In `R`: 
  - Ordering of `summarize()` and `filter` controls execution order
  
## ACID Guarantees

Most RDMSs provide a set of four `ACID` guarantees: 

::: {.incremental}

- **Atomicity**: Changes occur all-or-nothing
  - *E.g.*: Add charge to customer's account + schedule item for shipping
- **Consistency**: DDL Constraints are always valid
  - *E.g.*: Can't create a user without having an email on file
- **Isolation**: Two different users don't "step on each other's toes"
  - *E.g.*: If I'm updating prices while you are checking out, you get your original price
- **Durability**: When a change is complete, it is guaranteed to be saved to disk

:::

## Getting Started with RDMS

Easiest RDMS to use is `SQLite`

- Most ubiquitous DB in the world (by a long-shot)
- Already installed on your computer (many times over!)
- Access directly through `sqlite3` or `sqlite3.exe` 
- Access from `R` with `RSQLite` package

. . . 

[Excellent documentation](https://sqlite.org/lang.html)

## dbplyr

Can use `dbplyr` (extra `b`!) to put `dplyr`-UX on DB

*E.g.*

```{r}
library(dbplyr)
my_db <- DBI::dbConnect(RSQLite::SQLite(), ":memory:")
copy_to(my_db, penguins) # Copy penguins data into a temporary SQLite DB
```


## dbplyr

```{r}
penguins_db <- tbl(my_db, "penguins") # Connect to my_db and load penguins table
penguins_db
```


## dbplyr

```{r}
penguins_db |> 
  group_by(species) |> 
  summarize(mean_mass = mean(body_mass, na.rm=TRUE)) |> 
  show_query()
```

Extra step - `show_query` to see SQL or `collect()` to run in DB:

```{r}
penguins_db |> 
  group_by(species) |> 
  summarize(mean_mass = mean(body_mass, na.rm=TRUE)) |> 
  collect()
```

## dbplyr

::: {.smaller}

```{r}
penguins_db |> 
  filter(body_mass > 4000) |>
  group_by(species) |> 
  summarize(mean_mass = mean(body_mass, na.rm=TRUE)) |> 
  show_query()
```
Compared to: 

```{r}
penguins_db |> 
  group_by(species) |> 
  summarize(mean_mass = mean(body_mass, na.rm=TRUE)) |> 
  filter(mean_mass > 4000) |>
  show_query()
```

Not the best SQL, but RDMS can make quick work of it

:::

## dbplyr {.scrollable}

A more substantial example: 

::: {.smaller}
```{r}
library(dbplyr)
baseball_db <- lahman_sqlite()
```
:::

## dbplyr {.scrollable}

A more substantial example: 

::: {.smaller}

```{r}
#| eval: false
library(dbplyr)
baseball_db <- lahman_sqlite()
```

```{r}
#| echo: false
#| eval: true
cat(system2("sqlite3", c(baseball_db@dbname, ".schema"), stdout=TRUE), sep="\n")
```
:::



## dbplyr {.scrollable}

List the 5 modern-era managers with the highest winning percentages. (Define
modern-era as managed over 1000 games post 1920.)

::: {.smaller}

```{r}
managers <- tbl(baseball_db, "Managers")
people   <- tbl(baseball_db, "People")

managers |> 
    filter(yearID >= 1920) |> 
    group_by(playerID) |> 
    summarize(W = sum(W, na.rm=TRUE),
              L = sum(L, na.rm=TRUE)) |> 
    mutate(G = W + L, 
           record = W / (G + 0.0)) |>  # + 0.0 forces to work with decimals, not ints
    filter(G > 1000) |> 
    inner_join(people, 
               join_by(playerID == playerID)) |> 
    slice_max(record, n=5) |> 
    mutate(name = paste(nameGiven, nameLast)) |> 
    select(name, W, L, G, record) |>
    show_query()
```

:::

## dbplyr {.scrollable}

Add `|> explain()` to see the DB internal _query plan_:

::: {.smaller}

```{r}
managers <- tbl(baseball_db, "Managers")
people   <- tbl(baseball_db, "People")

managers |> 
    filter(yearID >= 1920) |> 
    group_by(playerID) |> 
    summarize(W = sum(W, na.rm=TRUE),
              L = sum(L, na.rm=TRUE)) |> 
    mutate(G = W + L, 
           record = W / (G + 0.0)) |>  # + 0.0 forces to work with decimals, not ints
    filter(G > 1000) |> 
    inner_join(people, 
               join_by(playerID == playerID)) |> 
    slice_max(record, n=5) |> 
    mutate(name = paste(nameGiven, nameLast)) |> 
    select(name, W, L, G, record) |>
    explain() # 
```

:::

Beyond this class to interpret, but in general use of `INDEX` yields 
good performance. Take DB classes for _much more_


## dbplyr {.scrollable}

List the 5 modern-era managers with the highest winning percentages. (Define
modern-era as managed over 1000 games post 1920.)

::: {.smaller}

```{r}
managers <- tbl(baseball_db, "Managers")
people   <- tbl(baseball_db, "People")

managers |> 
    filter(yearID >= 1920) |> 
    group_by(playerID) |> 
    summarize(W = sum(W, na.rm=TRUE),
              L = sum(L, na.rm=TRUE)) |> 
    mutate(G = W + L, 
           record = W / (G + 0.0)) |>  # + 0.0 forces to work with decimals, not ints
    filter(G > 1000) |> 
    inner_join(people, 
               join_by(playerID == playerID)) |> 
    slice_max(record, n=5) |> 
    mutate(name = paste(nameGiven, nameLast)) |> 
    select(name, W, L, G, record) |>
    collect()
```

:::
